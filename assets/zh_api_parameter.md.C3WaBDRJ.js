import{_ as e,c as o,o as a,ah as r}from"./chunks/framework.DJtYh1Hc.js";const t=JSON.parse('{"title":"tensorplay.nn.parameter","description":"","frontmatter":{},"headers":[],"relativePath":"zh/api/parameter.md","filePath":"zh/api/parameter.md"}');const s=e({name:"zh/api/parameter.md"},[["render",function(e,t,s,n,l,i){return a(),o("div",null,[...t[0]||(t[0]=[r('<div class="warning custom-block"><p class="custom-block-title">WARNING</p><p>该页面尚未翻译。 以下内容为英文原版。</p></div><h1 id="tensorplay-nn-parameter" tabindex="-1">tensorplay.nn.parameter <a class="header-anchor" href="#tensorplay-nn-parameter" aria-label="Permalink to &quot;tensorplay.nn.parameter&quot;">​</a></h1><h2 id="classes" tabindex="-1">Classes <a class="header-anchor" href="#classes" aria-label="Permalink to &quot;Classes&quot;">​</a></h2><h3 id="class-buffer-source" tabindex="-1"><code>class Buffer</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L158" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-buffer-source" aria-label="Permalink to &quot;`class Buffer` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L158)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Buffer(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">data</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">persistent</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>TensorBase</code></p><p>A kind of Tensor that should not be considered a model parameter. For example, BatchNorm&#39;s <code>running_mean</code> is not a parameter, but is part of the module&#39;s state.</p><p>Buffers are <code>~tensorplay.Tensor</code> subclasses, that have a very special property when used with <code>Module</code> s -- when they&#39;re assigned as Module attributes they are automatically added to the list of its buffers, and will appear e.g. in <code>~tensorplay.nn.Module.buffers</code> iterator. Assigning a Tensor doesn&#39;t have such effect. One can still assign a Tensor as explicitly by using the <code>~tensorplay.nn.Module.register_buffer</code> function.</p><h4 id="args" tabindex="-1">Args <a class="header-anchor" href="#args" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>data</strong> (<code>Tensor</code>): buffer tensor.</li><li><strong>persistent</strong> (<code>bool, optional</code>): whether the buffer is part of the module&#39;s <code>state_dict</code>. Default: <code>True</code></li></ul><details><summary>Methods</summary><h4 id="init-self-data-none-persistent-true-source" tabindex="-1"><code>__init__(self, data=None, persistent=True)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L174" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-data-none-persistent-true-source" aria-label="Permalink to &quot;`__init__(self, data=None, persistent=True)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L174)&quot;">​</a></h4><p><strong>init</strong>(self) -&gt; None <strong>init</strong>(self, data: object, dtype: tensorplay.DType | None = None, device: tensorplay.Device | None = None, requires_grad: bool = False) -&gt; None</p><hr><h4 id="cpu-self-source" tabindex="-1"><code>cpu(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L123" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cpu-self-source" aria-label="Permalink to &quot;`cpu(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L123)&quot;">​</a></h4><p>Returns a copy of this object in CPU memory. If this object is already in CPU memory, then no copy is performed and the original object is returned.</p><hr><h4 id="cuda-self-device-none-non-blocking-false-source" tabindex="-1"><code>cuda(self, device=None, non_blocking=False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L107" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cuda-self-device-none-non-blocking-false-source" aria-label="Permalink to &quot;`cuda(self, device=None, non_blocking=False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L107)&quot;">​</a></h4><p>Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.</p><hr><h4 id="double-self-source" tabindex="-1"><code>double(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L104" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#double-self-source" aria-label="Permalink to &quot;`double(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L104)&quot;">​</a></h4><hr><h4 id="flatten-self-start-dim-0-end-dim-1-source" tabindex="-1"><code>flatten(self, start_dim=0, end_dim=-1)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L19" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#flatten-self-start-dim-0-end-dim-1-source" aria-label="Permalink to &quot;`flatten(self, start_dim=0, end_dim=-1)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L19)&quot;">​</a></h4><p>Flattens a contiguous range of dims.</p><hr><h4 id="float-self-source" tabindex="-1"><code>float(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L98" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#float-self-source" aria-label="Permalink to &quot;`float(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L98)&quot;">​</a></h4><hr><h4 id="int-self-source" tabindex="-1"><code>int(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L101" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#int-self-source" aria-label="Permalink to &quot;`int(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L101)&quot;">​</a></h4><hr><h4 id="is-float-self-bool-source" tabindex="-1"><code>is_float(self) -&gt; bool</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L7" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#is-float-self-bool-source" aria-label="Permalink to &quot;`is_float(self) -&gt; bool` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L7)&quot;">​</a></h4><p>Check if tensor is floating point.</p><hr><h4 id="long-self-source" tabindex="-1"><code>long(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L95" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#long-self-source" aria-label="Permalink to &quot;`long(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L95)&quot;">​</a></h4><hr><h4 id="ndimension-self-int-source" tabindex="-1"><code>ndimension(self) -&gt; int</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L13" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#ndimension-self-int-source" aria-label="Permalink to &quot;`ndimension(self) -&gt; int` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L13)&quot;">​</a></h4><p>Alias for dim()</p><hr><h4 id="t-self-source" tabindex="-1"><code>t(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L130" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#t-self-source" aria-label="Permalink to &quot;`t(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L130)&quot;">​</a></h4><p>Returns the transpose of the tensor. Aliased to transpose(0, 1) to ensure correct autograd behavior (TransposeBackward).</p><hr><h4 id="unflatten-self-dim-sizes-source" tabindex="-1"><code>unflatten(self, dim, sizes)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L51" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#unflatten-self-dim-sizes-source" aria-label="Permalink to &quot;`unflatten(self, dim, sizes)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L51)&quot;">​</a></h4><p>Expands a dimension of the input tensor over multiple dimensions.</p><hr></details><h3 id="class-parameter-source" tabindex="-1"><code>class Parameter</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L15" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-parameter-source" aria-label="Permalink to &quot;`class Parameter` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L15)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">data</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>TensorBase</code></p><p>A kind of Tensor that is to be considered a module parameter.</p><p>Parameters are <code>~tensorplay.Tensor</code> subclasses, that have a very special property when used with <code>Module</code> s - when they&#39;re assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in <code>~Module.parameters</code> iterator. Assigning a Tensor doesn&#39;t have such effect. This is because one might want to cache some temporary state, like last hidden state of the RNN, in the model. If there was no such class as <code>Parameter</code>, these temporaries would get registered too.</p><h4 id="args-1" tabindex="-1">Args <a class="header-anchor" href="#args-1" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>data</strong> (<code>Tensor</code>): parameter tensor.</li><li><strong>requires_grad</strong> (<code>bool, optional</code>): if the parameter requires gradient. Note that the tensorplay.no_grad() context does NOT affect the default behavior of Parameter creation--the Parameter will always have <code>requires_grad=True</code> unless given explicitly. Default: <code>True</code></li></ul><details><summary>Methods</summary><h4 id="init-self-data-none-requires-grad-true-source" tabindex="-1"><code>__init__(self, data=None, requires_grad=True)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L34" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-data-none-requires-grad-true-source" aria-label="Permalink to &quot;`__init__(self, data=None, requires_grad=True)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L34)&quot;">​</a></h4><p><strong>init</strong>(self) -&gt; None <strong>init</strong>(self, data: object, dtype: tensorplay.DType | None = None, device: tensorplay.Device | None = None, requires_grad: bool = False) -&gt; None</p><hr><h4 id="cpu-self-source-1" tabindex="-1"><code>cpu(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L123" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cpu-self-source-1" aria-label="Permalink to &quot;`cpu(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L123)&quot;">​</a></h4><p>Returns a copy of this object in CPU memory. If this object is already in CPU memory, then no copy is performed and the original object is returned.</p><hr><h4 id="cuda-self-device-none-non-blocking-false-source-1" tabindex="-1"><code>cuda(self, device=None, non_blocking=False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L107" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cuda-self-device-none-non-blocking-false-source-1" aria-label="Permalink to &quot;`cuda(self, device=None, non_blocking=False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L107)&quot;">​</a></h4><p>Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.</p><hr><h4 id="double-self-source-1" tabindex="-1"><code>double(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L104" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#double-self-source-1" aria-label="Permalink to &quot;`double(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L104)&quot;">​</a></h4><hr><h4 id="flatten-self-start-dim-0-end-dim-1-source-1" tabindex="-1"><code>flatten(self, start_dim=0, end_dim=-1)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L19" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#flatten-self-start-dim-0-end-dim-1-source-1" aria-label="Permalink to &quot;`flatten(self, start_dim=0, end_dim=-1)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L19)&quot;">​</a></h4><p>Flattens a contiguous range of dims.</p><hr><h4 id="float-self-source-1" tabindex="-1"><code>float(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L98" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#float-self-source-1" aria-label="Permalink to &quot;`float(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L98)&quot;">​</a></h4><hr><h4 id="int-self-source-1" tabindex="-1"><code>int(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L101" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#int-self-source-1" aria-label="Permalink to &quot;`int(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L101)&quot;">​</a></h4><hr><h4 id="is-float-self-bool-source-1" tabindex="-1"><code>is_float(self) -&gt; bool</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L7" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#is-float-self-bool-source-1" aria-label="Permalink to &quot;`is_float(self) -&gt; bool` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L7)&quot;">​</a></h4><p>Check if tensor is floating point.</p><hr><h4 id="long-self-source-1" tabindex="-1"><code>long(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L95" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#long-self-source-1" aria-label="Permalink to &quot;`long(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L95)&quot;">​</a></h4><hr><h4 id="ndimension-self-int-source-1" tabindex="-1"><code>ndimension(self) -&gt; int</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L13" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#ndimension-self-int-source-1" aria-label="Permalink to &quot;`ndimension(self) -&gt; int` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L13)&quot;">​</a></h4><p>Alias for dim()</p><hr><h4 id="t-self-source-1" tabindex="-1"><code>t(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L130" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#t-self-source-1" aria-label="Permalink to &quot;`t(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L130)&quot;">​</a></h4><p>Returns the transpose of the tensor. Aliased to transpose(0, 1) to ensure correct autograd behavior (TransposeBackward).</p><hr><h4 id="unflatten-self-dim-sizes-source-1" tabindex="-1"><code>unflatten(self, dim, sizes)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L51" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#unflatten-self-dim-sizes-source-1" aria-label="Permalink to &quot;`unflatten(self, dim, sizes)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L51)&quot;">​</a></h4><p>Expands a dimension of the input tensor over multiple dimensions.</p><hr></details><h3 id="class-uninitializedbuffer-source" tabindex="-1"><code>class UninitializedBuffer</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L195" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-uninitializedbuffer-source" aria-label="Permalink to &quot;`class UninitializedBuffer` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L195)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">UninitializedBuffer(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">persistent</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>TensorBase</code></p><p>A buffer that is not initialized.</p><p>Uninitialized Buffer is a a special case of <code>tensorplay.Tensor</code> where the shape of the data is still unknown.</p><p>Unlike a <code>tensorplay.Tensor</code>, uninitialized parameters hold no data and attempting to access some properties, like their shape, will throw a runtime error. The only operations that can be performed on a uninitialized parameter are changing its datatype, moving it to a different device and converting it to a regular <code>tensorplay.Tensor</code>.</p><p>The default device or dtype to use when the buffer is materialized can be set during construction using e.g. <code>device=&#39;cuda&#39;</code>.</p><details><summary>Methods</summary><h4 id="init-self-requires-grad-false-device-none-dtype-none-persistent-true-source" tabindex="-1"><code>__init__(self, requires_grad=False, device=None, dtype=None, persistent=True)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L212" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-requires-grad-false-device-none-dtype-none-persistent-true-source" aria-label="Permalink to &quot;`__init__(self, requires_grad=False, device=None, dtype=None, persistent=True)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L212)&quot;">​</a></h4><p><strong>init</strong>(self) -&gt; None <strong>init</strong>(self, data: object, dtype: tensorplay.DType | None = None, device: tensorplay.Device | None = None, requires_grad: bool = False) -&gt; None</p><hr><h4 id="cpu-self-source-2" tabindex="-1"><code>cpu(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L123" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cpu-self-source-2" aria-label="Permalink to &quot;`cpu(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L123)&quot;">​</a></h4><p>Returns a copy of this object in CPU memory. If this object is already in CPU memory, then no copy is performed and the original object is returned.</p><hr><h4 id="cuda-self-device-none-non-blocking-false-source-2" tabindex="-1"><code>cuda(self, device=None, non_blocking=False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L107" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cuda-self-device-none-non-blocking-false-source-2" aria-label="Permalink to &quot;`cuda(self, device=None, non_blocking=False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L107)&quot;">​</a></h4><p>Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.</p><hr><h4 id="double-self-source-2" tabindex="-1"><code>double(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L104" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#double-self-source-2" aria-label="Permalink to &quot;`double(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L104)&quot;">​</a></h4><hr><h4 id="flatten-self-start-dim-0-end-dim-1-source-2" tabindex="-1"><code>flatten(self, start_dim=0, end_dim=-1)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L19" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#flatten-self-start-dim-0-end-dim-1-source-2" aria-label="Permalink to &quot;`flatten(self, start_dim=0, end_dim=-1)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L19)&quot;">​</a></h4><p>Flattens a contiguous range of dims.</p><hr><h4 id="float-self-source-2" tabindex="-1"><code>float(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L98" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#float-self-source-2" aria-label="Permalink to &quot;`float(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L98)&quot;">​</a></h4><hr><h4 id="int-self-source-2" tabindex="-1"><code>int(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L101" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#int-self-source-2" aria-label="Permalink to &quot;`int(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L101)&quot;">​</a></h4><hr><h4 id="is-float-self-bool-source-2" tabindex="-1"><code>is_float(self) -&gt; bool</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L7" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#is-float-self-bool-source-2" aria-label="Permalink to &quot;`is_float(self) -&gt; bool` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L7)&quot;">​</a></h4><p>Check if tensor is floating point.</p><hr><h4 id="long-self-source-2" tabindex="-1"><code>long(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L95" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#long-self-source-2" aria-label="Permalink to &quot;`long(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L95)&quot;">​</a></h4><hr><h4 id="materialize-self-shape-device-none-dtype-none-source" tabindex="-1"><code>materialize(self, shape, device=None, dtype=None)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L224" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#materialize-self-shape-device-none-dtype-none-source" aria-label="Permalink to &quot;`materialize(self, shape, device=None, dtype=None)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L224)&quot;">​</a></h4><hr><h4 id="ndimension-self-int-source-2" tabindex="-1"><code>ndimension(self) -&gt; int</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L13" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#ndimension-self-int-source-2" aria-label="Permalink to &quot;`ndimension(self) -&gt; int` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L13)&quot;">​</a></h4><p>Alias for dim()</p><hr><h4 id="share-memory-self-source" tabindex="-1"><code>share_memory_(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L231" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#share-memory-self-source" aria-label="Permalink to &quot;`share_memory_(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L231)&quot;">​</a></h4><p>share_memory_(self) -&gt; object</p><hr><h4 id="t-self-source-2" tabindex="-1"><code>t(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L130" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#t-self-source-2" aria-label="Permalink to &quot;`t(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L130)&quot;">​</a></h4><p>Returns the transpose of the tensor. Aliased to transpose(0, 1) to ensure correct autograd behavior (TransposeBackward).</p><hr><h4 id="unflatten-self-dim-sizes-source-2" tabindex="-1"><code>unflatten(self, dim, sizes)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L51" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#unflatten-self-dim-sizes-source-2" aria-label="Permalink to &quot;`unflatten(self, dim, sizes)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L51)&quot;">​</a></h4><p>Expands a dimension of the input tensor over multiple dimensions.</p><hr></details><h3 id="class-uninitializedparameter-source" tabindex="-1"><code>class UninitializedParameter</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L112" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-uninitializedparameter-source" aria-label="Permalink to &quot;`class UninitializedParameter` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L112)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">UninitializedParameter(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>Parameter</code></p><p>A parameter that is not initialized.</p><p>Uninitialized Parameters are a special case of <code>tensorplay.nn.Parameter</code> where the shape of the data is still unknown.</p><p>Unlike a <code>tensorplay.nn.Parameter</code>, uninitialized parameters hold no data and attempting to access some properties, like their shape, will throw a runtime error. The only operations that can be performed on a uninitialized parameter are changing its datatype, moving it to a different device and converting it to a regular <code>tensorplay.nn.Parameter</code>.</p><p>The default device or dtype to use when the parameter is materialized can be set during construction using e.g. <code>device=&#39;cuda&#39;</code>.</p><details><summary>Methods</summary><h4 id="init-self-requires-grad-true-device-none-dtype-none-source" tabindex="-1"><code>__init__(self, requires_grad=True, device=None, dtype=None)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L129" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-requires-grad-true-device-none-dtype-none-source" aria-label="Permalink to &quot;`__init__(self, requires_grad=True, device=None, dtype=None)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L129)&quot;">​</a></h4><p><strong>init</strong>(self) -&gt; None <strong>init</strong>(self, data: object, dtype: tensorplay.DType | None = None, device: tensorplay.Device | None = None, requires_grad: bool = False) -&gt; None</p><hr><h4 id="cpu-self-source-3" tabindex="-1"><code>cpu(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L123" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cpu-self-source-3" aria-label="Permalink to &quot;`cpu(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L123)&quot;">​</a></h4><p>Returns a copy of this object in CPU memory. If this object is already in CPU memory, then no copy is performed and the original object is returned.</p><hr><h4 id="cuda-self-device-none-non-blocking-false-source-3" tabindex="-1"><code>cuda(self, device=None, non_blocking=False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L107" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cuda-self-device-none-non-blocking-false-source-3" aria-label="Permalink to &quot;`cuda(self, device=None, non_blocking=False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L107)&quot;">​</a></h4><p>Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.</p><hr><h4 id="double-self-source-3" tabindex="-1"><code>double(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L104" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#double-self-source-3" aria-label="Permalink to &quot;`double(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L104)&quot;">​</a></h4><hr><h4 id="flatten-self-start-dim-0-end-dim-1-source-3" tabindex="-1"><code>flatten(self, start_dim=0, end_dim=-1)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L19" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#flatten-self-start-dim-0-end-dim-1-source-3" aria-label="Permalink to &quot;`flatten(self, start_dim=0, end_dim=-1)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L19)&quot;">​</a></h4><p>Flattens a contiguous range of dims.</p><hr><h4 id="float-self-source-3" tabindex="-1"><code>float(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L98" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#float-self-source-3" aria-label="Permalink to &quot;`float(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L98)&quot;">​</a></h4><hr><h4 id="int-self-source-3" tabindex="-1"><code>int(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L101" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#int-self-source-3" aria-label="Permalink to &quot;`int(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L101)&quot;">​</a></h4><hr><h4 id="is-float-self-bool-source-3" tabindex="-1"><code>is_float(self) -&gt; bool</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L7" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#is-float-self-bool-source-3" aria-label="Permalink to &quot;`is_float(self) -&gt; bool` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L7)&quot;">​</a></h4><p>Check if tensor is floating point.</p><hr><h4 id="long-self-source-3" tabindex="-1"><code>long(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L95" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#long-self-source-3" aria-label="Permalink to &quot;`long(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L95)&quot;">​</a></h4><hr><h4 id="materialize-self-shape-device-none-dtype-none-source-1" tabindex="-1"><code>materialize(self, shape, device=None, dtype=None)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L138" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#materialize-self-shape-device-none-dtype-none-source-1" aria-label="Permalink to &quot;`materialize(self, shape, device=None, dtype=None)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L138)&quot;">​</a></h4><hr><h4 id="ndimension-self-int-source-3" tabindex="-1"><code>ndimension(self) -&gt; int</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L13" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#ndimension-self-int-source-3" aria-label="Permalink to &quot;`ndimension(self) -&gt; int` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L13)&quot;">​</a></h4><p>Alias for dim()</p><hr><h4 id="share-memory-self-source-1" tabindex="-1"><code>share_memory_(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L145" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#share-memory-self-source-1" aria-label="Permalink to &quot;`share_memory_(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L145)&quot;">​</a></h4><p>share_memory_(self) -&gt; object</p><hr><h4 id="t-self-source-3" tabindex="-1"><code>t(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L130" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#t-self-source-3" aria-label="Permalink to &quot;`t(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L130)&quot;">​</a></h4><p>Returns the transpose of the tensor. Aliased to transpose(0, 1) to ensure correct autograd behavior (TransposeBackward).</p><hr><h4 id="unflatten-self-dim-sizes-source-3" tabindex="-1"><code>unflatten(self, dim, sizes)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L51" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#unflatten-self-dim-sizes-source-3" aria-label="Permalink to &quot;`unflatten(self, dim, sizes)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L51)&quot;">​</a></h4><p>Expands a dimension of the input tensor over multiple dimensions.</p><hr></details><h3 id="class-uninitializedtensormixin-source" tabindex="-1"><code>class UninitializedTensorMixin</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L52" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-uninitializedtensormixin-source" aria-label="Permalink to &quot;`class UninitializedTensorMixin` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L52)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">UninitializedTensorMixin()</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><details><summary>Methods</summary><h4 id="materialize-self-shape-device-none-dtype-none-source-2" tabindex="-1"><code>materialize(self, shape, device=None, dtype=None)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L53" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#materialize-self-shape-device-none-dtype-none-source-2" aria-label="Permalink to &quot;`materialize(self, shape, device=None, dtype=None)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L53)&quot;">​</a></h4><p>Create a Parameter or Tensor with the same properties of the uninitialized one.</p><p>Given a shape, it materializes a parameter in the same device and with the same <code>dtype</code> as the current one or the specified ones in the arguments.</p><h4 id="args-2" tabindex="-1">Args <a class="header-anchor" href="#args-2" aria-label="Permalink to &quot;Args&quot;">​</a></h4><pre><code>shape : (tuple): the shape for the materialized tensor.\n</code></pre><ul><li><strong>device</strong> (<code>tensorplay.device</code>): the desired device of the parameters and buffers in this module. Optional.</li><li><strong>dtype</strong> (<code>tensorplay.dtype</code>): the desired floating point type of the floating point parameters and buffers in this module. Optional.</li></ul><hr><h4 id="share-memory-self-source-2" tabindex="-1"><code>share_memory_(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L88" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#share-memory-self-source-2" aria-label="Permalink to &quot;`share_memory_(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L88)&quot;">​</a></h4><hr></details><h2 id="functions" tabindex="-1">Functions <a class="header-anchor" href="#functions" aria-label="Permalink to &quot;Functions&quot;">​</a></h2><h3 id="is-lazy-source" tabindex="-1"><code>is_lazy()</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L102" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#is-lazy-source" aria-label="Permalink to &quot;`is_lazy()` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L102)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">is_lazy(param: Any) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> bool</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>Returns whether <code>param</code> is an <code>UninitializedParameter</code> or <code>UninitializedBuffer</code>.</p><h4 id="args-3" tabindex="-1">Args <a class="header-anchor" href="#args-3" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>param</strong> (<code>Any</code>): the input to check.</li></ul>',44)])])}]]);export{t as __pageData,s as default};
