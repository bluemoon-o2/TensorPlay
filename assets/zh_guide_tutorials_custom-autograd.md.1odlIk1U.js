import{_ as a,c as i,o as n,a4 as l}from"./chunks/framework.DWXU4yX9.js";const o=JSON.parse('{"title":"自定义 Autograd 函数","description":"","frontmatter":{},"headers":[],"relativePath":"zh/guide/tutorials/custom-autograd.md","filePath":"zh/guide/tutorials/custom-autograd.md"}'),e={name:"zh/guide/tutorials/custom-autograd.md"};function p(t,s,r,h,k,d){return n(),i("div",null,[...s[0]||(s[0]=[l(`<h1 id="自定义-autograd-函数" tabindex="-1">自定义 Autograd 函数 <a class="header-anchor" href="#自定义-autograd-函数" aria-label="Permalink to &quot;自定义 Autograd 函数&quot;">​</a></h1><p>TensorPlay 允许你通过定义自己的前向和反向逻辑来扩展其自动微分引擎。这对于实现带有自定义梯度的不可微操作或优化特定内核非常有用。</p><h2 id="定义自定义函数" tabindex="-1">定义自定义函数 <a class="header-anchor" href="#定义自定义函数" aria-label="Permalink to &quot;定义自定义函数&quot;">​</a></h2><p>要创建自定义 autograd 函数，请继承 <code>tpx.autograd.Function</code> 并实现静态方法 <code>forward</code> 和 <code>backward</code>。</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tp</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.autograd </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Function</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">class</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> MyExp</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">Function</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">    @</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">staticmethod</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> forward</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(ctx, i):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        result </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> i.exp()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        ctx.save_for_backward(result)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> result</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">    @</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">staticmethod</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> backward</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(ctx, grad_output):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        result, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> ctx.saved_tensors</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> grad_output </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> result</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 使用方法</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> my_exp</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(x):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> MyExp.apply(x)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tp.randn(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> my_exp(x)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y.sum().backward()</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(x.grad)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br></div></div><h2 id="何时使用自定义函数" tabindex="-1">何时使用自定义函数？ <a class="header-anchor" href="#何时使用自定义函数" aria-label="Permalink to &quot;何时使用自定义函数？&quot;">​</a></h2><ol><li><strong>效率</strong>：如果你能比标准操作的组合更有效地计算组合梯度。</li><li><strong>稳定性</strong>：实现数值稳定版本的函数（例如 LogSumExp）。</li><li><strong>自定义硬件</strong>：对接具有自己微分逻辑的硬件。</li><li><strong>不可微操作</strong>：为 Step 函数或量化等操作提供“代理梯度”。</li></ol><h2 id="上下文对象-ctx" tabindex="-1">上下文对象 (<code>ctx</code>) <a class="header-anchor" href="#上下文对象-ctx" aria-label="Permalink to &quot;上下文对象 (\`ctx\`)&quot;">​</a></h2><p><code>ctx</code> 对象用于在正向传播和反向传播之间传递信息：</p><ul><li><code>ctx.save_for_backward(*tensors)</code>：使用它来保存反向传播所需的张量。</li><li><code>ctx.saved_tensors</code>：在 <code>backward</code> 中访问保存的张量。</li><li><code>ctx.mark_dirty(*tensors)</code>：标记被就地修改的张量。</li><li><code>ctx.mark_non_differentiable(*tensors)</code>：标记不需要梯度的输出。</li></ul>`,10)])])}const E=a(e,[["render",p]]);export{o as __pageData,E as default};
