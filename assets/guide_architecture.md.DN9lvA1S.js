import{_ as a,c as n,o as i,a4 as r}from"./chunks/framework.t-GQSDtj.js";const u=JSON.parse('{"title":"Architecture Overview","description":"","frontmatter":{},"headers":[],"relativePath":"guide/architecture.md","filePath":"guide/architecture.md"}'),t={name:"guide/architecture.md"};function s(o,e,l,c,d,h){return i(),n("div",null,[...e[0]||(e[0]=[r(`<h1 id="architecture-overview" tabindex="-1">Architecture Overview <a class="header-anchor" href="#architecture-overview" aria-label="Permalink to &quot;Architecture Overview&quot;">​</a></h1><p>TensorPlay is designed with a strictly decoupled, layered architecture. It consists of four core libraries with clear boundaries and unidirectional dependencies.</p><h2 id="the-4-core-libraries" tabindex="-1">The 4 Core Libraries <a class="header-anchor" href="#the-4-core-libraries" aria-label="Permalink to &quot;The 4 Core Libraries&quot;">​</a></h2><h3 id="_1-p10-tensor-computation-engine" tabindex="-1">1. P10 (Tensor Computation Engine) <a class="header-anchor" href="#_1-p10-tensor-computation-engine" aria-label="Permalink to &quot;1. P10 (Tensor Computation Engine)&quot;">​</a></h3><ul><li><strong>Role</strong>: The foundational &quot;Calculation Engine&quot;.</li><li><strong>Design Philosophy</strong>: <ul><li><strong>Hardware Abstraction</strong>: Uses a <code>Tensor</code> interface and <code>TensorImpl</code> polymorphism to support multiple hardware backends (CPU, CUDA, Custom Edge Chips) without changing the frontend API.</li><li><strong>Zero Differential Logic</strong>: Focused purely on efficient tensor kernels and memory management, serving as the stable bedrock for all other layers.</li><li><strong>Dispatcher Pattern</strong>: Decouples operator definitions from device-specific implementations, allowing for easy integration of libraries like MKL or cuDNN.</li></ul></li></ul><h3 id="_2-tpx-autograd-engine" tabindex="-1">2. TPX (Autograd Engine) <a class="header-anchor" href="#_2-tpx-autograd-engine" aria-label="Permalink to &quot;2. TPX (Autograd Engine)&quot;">​</a></h3><ul><li><strong>Role</strong>: The &quot;Differentiation Layer&quot;.</li><li><strong>Design Philosophy</strong>: <ul><li><strong>Decoupled Autograd</strong>: Implemented as a lightweight extension layer rather than being baked into the tensor core. It only tracks operations when <code>requires_grad=True</code>.</li><li><strong>Explicit Graph Building</strong>: Designed for educational clarity, allowing users to inspect how the computational graph is constructed and traversed during the backward pass.</li><li><strong>Pluggable Engine</strong>: Can be replaced or extended with different differentiation modes (e.g., higher-order derivatives) without affecting the underlying P10 engine.</li></ul></li></ul><h3 id="_3-stax-static-graph-accelerator" tabindex="-1">3. Stax (Static Graph Accelerator) <a class="header-anchor" href="#_3-stax-static-graph-accelerator" aria-label="Permalink to &quot;3. Stax (Static Graph Accelerator)&quot;">​</a></h3><ul><li><strong>Role</strong>: The &quot;Optimization Layer&quot;.</li><li><strong>Design Philosophy</strong>: <ul><li><strong>Optimization-First</strong>: Focuses purely on static graph capture, operator fusion, and just-in-time (JIT) compilation to minimize Python overhead.</li><li><strong>Independent Path</strong>: Operates on a separate dependency chain from TPX/NN, making it a modular component that can be added or removed based on performance needs.</li><li><strong>Compiler Integration</strong>: Designed to interface with advanced compiler backends like MLIR or TVM in the future.</li></ul></li></ul><h3 id="_4-nn-neural-network-library" tabindex="-1">4. NN (Neural Network Library) <a class="header-anchor" href="#_4-nn-neural-network-library" aria-label="Permalink to &quot;4. NN (Neural Network Library)&quot;">​</a></h3><ul><li><strong>Role</strong>: The &quot;Business Layer&quot;.</li><li><strong>Design Philosophy</strong>: <ul><li><strong>User-Friendly Abstraction</strong>: Provides a familiar, PyTorch-compatible interface for high-level components like <code>Linear</code>, <code>Conv2d</code>, and <code>Optimizers</code>.</li><li><strong>Blueprint Approach</strong>: Every layer is designed to be a clear, readable blueprint, demonstrating how complex neural network components are built from basic tensor operations.</li><li><strong>Pure Dependency</strong>: Relies strictly on the public APIs of P10 and TPX, ensuring it remains an optional, non-intrusive layer for high-level modeling.</li></ul></li></ul><hr><h2 id="dependency-graph" tabindex="-1">Dependency Graph <a class="header-anchor" href="#dependency-graph" aria-label="Permalink to &quot;Dependency Graph&quot;">​</a></h2><div class="language-mermaid vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">mermaid</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">graph TD</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    NN[NN: Neural Networks] --&gt; TPX[TPX: Autograd]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    NN --&gt; P10[P10: Computation]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    TPX --&gt; P10</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    Stax[Stax: Static Graph] --&gt; P10</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    </span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    style P10 fill:#f9f,stroke:#333,stroke-width:2px</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    style TPX fill:#bbf,stroke:#333,stroke-width:1px</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    style Stax fill:#bfb,stroke:#333,stroke-width:1px</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    style NN fill:#fbb,stroke:#333,stroke-width:1px</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><h2 id="why-this-architecture" tabindex="-1">Why This Architecture? <a class="header-anchor" href="#why-this-architecture" aria-label="Permalink to &quot;Why This Architecture?&quot;">​</a></h2><ol><li><strong>Decoupling</strong>: Each library can be developed, tested, and optimized independently.</li><li><strong>Extensibility</strong>: Adding a new hardware backend only requires changes to P10. Adding a new differentiation mode only affects TPX.</li><li><strong>Performance</strong>: Users only pay for what they use. Pure computation tasks don&#39;t load the autograd engine or the static graph compiler.</li><li><strong>Educational Value</strong>: By separating these concerns, TensorPlay makes it easy for developers to understand the internals of a modern deep learning framework.</li></ol>`,16)])])}const g=a(t,[["render",s]]);export{u as __pageData,g as default};
