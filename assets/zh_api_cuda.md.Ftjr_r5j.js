import{_ as e,c as a,o as s,ah as i}from"./chunks/framework.DJtYh1Hc.js";const t=JSON.parse('{"title":"tensorplay.cuda","description":"","frontmatter":{},"headers":[],"relativePath":"zh/api/cuda.md","filePath":"zh/api/cuda.md"}');const n=e({name:"zh/api/cuda.md"},[["render",function(e,t,n,r,o,l){return s(),a("div",null,[...t[0]||(t[0]=[i('<div class="warning custom-block"><p class="custom-block-title">WARNING</p><p>该页面尚未翻译。 以下内容为英文原版。</p></div><h1 id="tensorplay-cuda" tabindex="-1">tensorplay.cuda <a class="header-anchor" href="#tensorplay-cuda" aria-label="Permalink to &quot;tensorplay.cuda&quot;">​</a></h1><h2 id="classes" tabindex="-1">Classes <a class="header-anchor" href="#classes" aria-label="Permalink to &quot;Classes&quot;">​</a></h2><h3 id="class-event-source" tabindex="-1"><code>class Event</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L114" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-event-source" aria-label="Permalink to &quot;`class Event` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L114)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Event(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">enable_timing</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">blocking</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">interprocess</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>Wrapper around a CUDA event.</p><details><summary>Methods</summary><h4 id="init-self-enable-timing-false-blocking-false-interprocess-false-source" tabindex="-1"><code>__init__(self, enable_timing=False, blocking=False, interprocess=False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L116" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-enable-timing-false-blocking-false-interprocess-false-source" aria-label="Permalink to &quot;`__init__(self, enable_timing=False, blocking=False, interprocess=False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L116)&quot;">​</a></h4><p>Initialize self. See help(type(self)) for accurate signature.</p><hr><h4 id="elapsed-time-self-end-event-source" tabindex="-1"><code>elapsed_time(self, end_event)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L130" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#elapsed-time-self-end-event-source" aria-label="Permalink to &quot;`elapsed_time(self, end_event)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L130)&quot;">​</a></h4><hr><h4 id="query-self-source" tabindex="-1"><code>query(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L127" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#query-self-source" aria-label="Permalink to &quot;`query(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L127)&quot;">​</a></h4><hr><h4 id="record-self-stream-none-source" tabindex="-1"><code>record(self, stream=None)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L121" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#record-self-stream-none-source" aria-label="Permalink to &quot;`record(self, stream=None)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L121)&quot;">​</a></h4><hr><h4 id="synchronize-self-source" tabindex="-1"><code>synchronize(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L133" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#synchronize-self-source" aria-label="Permalink to &quot;`synchronize(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L133)&quot;">​</a></h4><hr><h4 id="wait-self-stream-none-source" tabindex="-1"><code>wait(self, stream=None)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L124" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#wait-self-stream-none-source" aria-label="Permalink to &quot;`wait(self, stream=None)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L124)&quot;">​</a></h4><hr></details><h3 id="class-stream-source" tabindex="-1"><code>class Stream</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L136" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-stream-source" aria-label="Permalink to &quot;`class Stream` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L136)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Stream(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">priority</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">**</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">kwargs)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>Wrapper around a CUDA stream.</p><details><summary>Methods</summary><h4 id="init-self-device-none-priority-0-kwargs-source" tabindex="-1"><code>__init__(self, device=None, priority=0, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L138" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-device-none-priority-0-kwargs-source" aria-label="Permalink to &quot;`__init__(self, device=None, priority=0, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L138)&quot;">​</a></h4><p>Initialize self. See help(type(self)) for accurate signature.</p><hr><h4 id="record-event-self-event-none-source" tabindex="-1"><code>record_event(self, event=None)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L148" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#record-event-self-event-none-source" aria-label="Permalink to &quot;`record_event(self, event=None)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L148)&quot;">​</a></h4><hr><h4 id="synchronize-self-source-1" tabindex="-1"><code>synchronize(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L151" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#synchronize-self-source-1" aria-label="Permalink to &quot;`synchronize(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L151)&quot;">​</a></h4><hr><h4 id="wait-event-self-event-source" tabindex="-1"><code>wait_event(self, event)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L142" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#wait-event-self-event-source" aria-label="Permalink to &quot;`wait_event(self, event)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L142)&quot;">​</a></h4><hr><h4 id="wait-stream-self-stream-source" tabindex="-1"><code>wait_stream(self, stream)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L145" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#wait-stream-self-stream-source" aria-label="Permalink to &quot;`wait_stream(self, stream)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L145)&quot;">​</a></h4><hr></details><h2 id="functions" tabindex="-1">Functions <a class="header-anchor" href="#functions" aria-label="Permalink to &quot;Functions&quot;">​</a></h2><h3 id="cudart-source" tabindex="-1"><code>cudart()</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L168" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cudart-source" aria-label="Permalink to &quot;`cudart()` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L168)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cudart()</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>Returns the ctypes wrapper around the CUDA runtime DLL.</p><h3 id="current-device-source" tabindex="-1"><code>current_device()</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L35" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#current-device-source" aria-label="Permalink to &quot;`current_device()` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L35)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">current_device() </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> int</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>Return the index of a currently selected device.</p><h3 id="get-device-capability-source" tabindex="-1"><code>get_device_capability()</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L53" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-device-capability-source" aria-label="Permalink to &quot;`get_device_capability()` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L53)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">get_device_capability(device: Union[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Any, NoneType] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> tuple</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>Gets the cuda capability of a device.</p><h3 id="get-device-name-source" tabindex="-1"><code>get_device_name()</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L45" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-device-name-source" aria-label="Permalink to &quot;`get_device_name()` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L45)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">get_device_name(device: Union[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Any, NoneType] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> str</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>Gets the name of a device.</p><h3 id="get-device-properties-source" tabindex="-1"><code>get_device_properties()</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L61" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-device-properties-source" aria-label="Permalink to &quot;`get_device_properties()` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L61)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">get_device_properties(device: Union[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Any])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>Gets the properties of a device.</p><h3 id="init-source" tabindex="-1"><code>init()</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L22" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-source" aria-label="Permalink to &quot;`init()` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L22)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">init()</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>Initialize CUDA state. This is lazy initialized so normally not needed.</p><h3 id="is-initialized-source" tabindex="-1"><code>is_initialized()</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L31" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#is-initialized-source" aria-label="Permalink to &quot;`is_initialized()` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L31)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">is_initialized()</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>Check if CUDA has been initialized.</p><h3 id="manual-seed-source" tabindex="-1"><code>manual_seed()</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L160" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#manual-seed-source" aria-label="Permalink to &quot;`manual_seed()` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L160)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">manual_seed(seed: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>Sets the seed for generating random numbers for the current GPU.</p><h3 id="manual-seed-all-source" tabindex="-1"><code>manual_seed_all()</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L164" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#manual-seed-all-source" aria-label="Permalink to &quot;`manual_seed_all()` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L164)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">manual_seed_all(seed: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>Sets the seed for generating random numbers on all GPUs.</p><h3 id="max-memory-allocated-source" tabindex="-1"><code>max_memory_allocated()</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L84" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#max-memory-allocated-source" aria-label="Permalink to &quot;`max_memory_allocated()` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L84)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">max_memory_allocated(device: Union[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Any, NoneType] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> int</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>Returns the maximum GPU memory usage by tensors in bytes for a given device.</p><h3 id="max-memory-reserved-source" tabindex="-1"><code>max_memory_reserved()</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L105" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#max-memory-reserved-source" aria-label="Permalink to &quot;`max_memory_reserved()` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L105)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">max_memory_reserved(device: Union[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Any, NoneType] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> int</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.</p><h3 id="memory-allocated-source" tabindex="-1"><code>memory_allocated()</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L76" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#memory-allocated-source" aria-label="Permalink to &quot;`memory_allocated()` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L76)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">memory_allocated(device: Union[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Any, NoneType] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> int</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>Returns the current GPU memory usage by tensors in bytes for a given device.</p><h3 id="memory-reserved-source" tabindex="-1"><code>memory_reserved()</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L100" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#memory-reserved-source" aria-label="Permalink to &quot;`memory_reserved()` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L100)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">memory_reserved(device: Union[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Any, NoneType] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> int</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>Returns the current GPU memory managed by the caching allocator in bytes for a given device.</p><h3 id="reset-max-memory-allocated-source" tabindex="-1"><code>reset_max_memory_allocated()</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L92" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#reset-max-memory-allocated-source" aria-label="Permalink to &quot;`reset_max_memory_allocated()` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L92)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">reset_max_memory_allocated(device: Union[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Any, NoneType] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>Resets the starting point for tracking maximum GPU memory usage.</p><h3 id="reset-max-memory-reserved-source" tabindex="-1"><code>reset_max_memory_reserved()</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L109" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#reset-max-memory-reserved-source" aria-label="Permalink to &quot;`reset_max_memory_reserved()` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L109)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">reset_max_memory_reserved(device: Union[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Any, NoneType] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>Resets the starting point for tracking maximum GPU memory managed by the caching allocator.</p><h3 id="set-device-source" tabindex="-1"><code>set_device()</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L39" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-device-source" aria-label="Permalink to &quot;`set_device()` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L39)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">set_device(device: Union[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Any])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>Sets the current device.</p><h3 id="synchronize-source" tabindex="-1"><code>synchronize()</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L67" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#synchronize-source" aria-label="Permalink to &quot;`synchronize()` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/cuda.py#L67)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">synchronize(device: Union[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Any, NoneType] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>Waits for all kernels in all streams on a CUDA device to complete.</p>',63)])])}]]);export{t as __pageData,n as default};
