import{_ as a,c as e,o as i,a4 as n}from"./chunks/framework.DWXU4yX9.js";const k=JSON.parse('{"title":"tensorplay.autograd","description":"","frontmatter":{},"headers":[],"relativePath":"api/autograd.md","filePath":"api/autograd.md"}'),l={name:"api/autograd.md"};function t(r,s,o,p,h,d){return i(),e("div",null,[...s[0]||(s[0]=[n(`<h1 id="tensorplay-autograd" tabindex="-1">tensorplay.autograd <a class="header-anchor" href="#tensorplay-autograd" aria-label="Permalink to &quot;tensorplay.autograd&quot;">​</a></h1><p><code>tensorplay.autograd</code> provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions.</p><p>It requires minimal changes to the existing code - you only need to declare <code>Tensor</code> s for which gradients should be computed with the <code>requires_grad=True</code> keyword. As of now, we only support autograd for floating point <code>Tensor</code> types ( half, float, double and bfloat16) and complex <code>Tensor</code> types (cfloat, cdouble).</p><h2 id="classes" tabindex="-1">Classes <a class="header-anchor" href="#classes" aria-label="Permalink to &quot;Classes&quot;">​</a></h2><h3 id="class-function-source" tabindex="-1"><code>class Function</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/function.py#L18" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-function-source" aria-label="Permalink to &quot;\`class Function\` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/function.py#L18)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Function()</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>Records operation history and defines formulas for differentiating ops.</p><details><summary>Methods</summary><h4 id="apply-args-kwargs-source" tabindex="-1"><code>apply(*args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/function.py#L37" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#apply-args-kwargs-source" aria-label="Permalink to &quot;\`apply(*args, **kwargs)\` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/function.py#L37)&quot;">​</a></h4><hr><h4 id="backward-ctx-grad-outputs-source" tabindex="-1"><code>backward(ctx, *grad_outputs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/function.py#L30" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#backward-ctx-grad-outputs-source" aria-label="Permalink to &quot;\`backward(ctx, *grad_outputs)\` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/function.py#L30)&quot;">​</a></h4><p>Defines a formula for differentiating the operation.</p><hr><h4 id="forward-ctx-args-kwargs-source" tabindex="-1"><code>forward(ctx, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/function.py#L23" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#forward-ctx-args-kwargs-source" aria-label="Permalink to &quot;\`forward(ctx, *args, **kwargs)\` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/function.py#L23)&quot;">​</a></h4><p>Performs the operation.</p><hr></details><h3 id="class-enable-grad-source" tabindex="-1"><code>class enable_grad</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/grad_mode.py#L86" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-enable-grad-source" aria-label="Permalink to &quot;\`class enable_grad\` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/grad_mode.py#L86)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">enable_grad(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">orig_func</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>_NoParamDecoratorContextManager</code></p><p>Context-manager that enables gradient calculation.</p><p>Enables gradient calculation, if it has been disabled via <code>~no_grad</code> or <code>~set_grad_enabled</code>.</p><p>This context manager is thread local; it will not affect computation in other threads.</p><p>Also functions as a decorator.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>enable_grad is one of several mechanisms that can enable or disable gradients locally see <code>locally-disable-grad-doc</code> for more information on how they compare.</p></div><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This API does not apply to <code>forward-mode AD &lt;forward-mode-ad&gt;</code>.</p></div><h4 id="example" tabindex="-1">Example <a class="header-anchor" href="#example" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.tensor([</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">with</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.no_grad():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    with</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.enable_grad():</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y.requires_grad</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y.backward()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">x.grad</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.])</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@tensorplay.enable_grad</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> doubler</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(x):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">with</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.no_grad():</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    z </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> doubler(x)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">z.requires_grad</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@tensorplay.enable_grad</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> tripler</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(x):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 3</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">with</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.no_grad():</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    z </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tripler(x)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">z.requires_grad</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br></div></div><details><summary>Methods</summary><h4 id="clone-self-source" tabindex="-1"><code>clone(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/utils/_contextlib.py#L147" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#clone-self-source" aria-label="Permalink to &quot;\`clone(self)\` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/utils/_contextlib.py#L147)&quot;">​</a></h4><hr></details><h3 id="class-no-grad-source" tabindex="-1"><code>class no_grad</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/grad_mode.py#L21" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-no-grad-source" aria-label="Permalink to &quot;\`class no_grad\` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/grad_mode.py#L21)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">no_grad() </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>_NoParamDecoratorContextManager</code></p><p>Context-manager that disables gradient calculation.</p><p>Disabling gradient calculation is useful for inference, when you are sure that you will not call <code>Tensor.backward()</code>. It will reduce memory consumption for computations that would otherwise have <code>requires_grad=True</code>.</p><p>In this mode, the result of every computation will have <code>requires_grad=False</code>, even when the inputs have <code>requires_grad=True</code>. There is an exception! All factory functions, or functions that create a new Tensor and take a requires_grad kwarg, will NOT be affected by this mode.</p><p>This context manager is thread local; it will not affect computation in other threads.</p><p>Also functions as a decorator.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>No-grad is one of several mechanisms that can enable or disable gradients locally see <code>locally-disable-grad-doc</code> for more information on how they compare.</p></div><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This API does not apply to <code>forward-mode AD &lt;forward-mode-ad&gt;</code>. If you want to disable forward AD for a computation, you can unpack your dual tensors.</p></div><h4 id="example-1" tabindex="-1">Example <a class="header-anchor" href="#example-1" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.tensor([</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">with</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.no_grad():</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y.requires_grad</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">False</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@tensorplay.no_grad</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> doubler</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(x):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">z </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> doubler(x)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">z.requires_grad</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">False</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@tensorplay.no_grad</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> tripler</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(x):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 3</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">z </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tripler(x)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">z.requires_grad</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">False</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># factory function exception</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">with</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.no_grad():</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    a </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.nn.Parameter(tensorplay.rand(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">a.requires_grad</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br></div></div><details><summary>Methods</summary><h4 id="init-self-none-source" tabindex="-1"><code>__init__(self) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/grad_mode.py#L74" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-none-source" aria-label="Permalink to &quot;\`__init__(self) -&gt; None\` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/grad_mode.py#L74)&quot;">​</a></h4><p>Initialize self. See help(type(self)) for accurate signature.</p><hr><h4 id="clone-self-source-1" tabindex="-1"><code>clone(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/utils/_contextlib.py#L147" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#clone-self-source-1" aria-label="Permalink to &quot;\`clone(self)\` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/utils/_contextlib.py#L147)&quot;">​</a></h4><hr></details><h3 id="class-set-grad-enabled-source" tabindex="-1"><code>class set_grad_enabled</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/grad_mode.py#L145" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-set-grad-enabled-source" aria-label="Permalink to &quot;\`class set_grad_enabled\` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/grad_mode.py#L145)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">set_grad_enabled(mode: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">bool</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>_DecoratorContextManager</code></p><p>Context-manager that sets gradient calculation on or off.</p><p><code>set_grad_enabled</code> will enable or disable grads based on its argument <code>mode</code>. It can be used as a context-manager or as a function.</p><p>This context manager is thread local; it will not affect computation in other threads.</p><h4 id="args" tabindex="-1">Args <a class="header-anchor" href="#args" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>mode</strong> (<code>bool</code>): Flag whether to enable grad (<code>True</code>), or disable (<code>False</code>). This can be used to conditionally enable gradients.</li></ul><div class="info custom-block"><p class="custom-block-title">INFO</p><p>set_grad_enabled is one of several mechanisms that can enable or disable gradients locally see <code>locally-disable-grad-doc</code> for more information on how they compare.</p></div><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This API does not apply to <code>forward-mode AD &lt;forward-mode-ad&gt;</code>.</p></div><h4 id="example-2" tabindex="-1">Example <a class="header-anchor" href="#example-2" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.tensor([</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">is_train </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> False</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">with</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.set_grad_enabled(is_train):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y.requires_grad</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">False</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">_ </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.set_grad_enabled(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y.requires_grad</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">_ </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.set_grad_enabled(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y.requires_grad</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">False</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><details><summary>Methods</summary><h4 id="init-self-mode-bool-none-source" tabindex="-1"><code>__init__(self, mode: bool) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/grad_mode.py#L186" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-mode-bool-none-source" aria-label="Permalink to &quot;\`__init__(self, mode: bool) -&gt; None\` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/grad_mode.py#L186)&quot;">​</a></h4><p>Initialize self. See help(type(self)) for accurate signature.</p><hr><h4 id="clone-self-set-grad-enabled-source" tabindex="-1"><code>clone(self) -&gt; &#39;set_grad_enabled&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/grad_mode.py#L208" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#clone-self-set-grad-enabled-source" aria-label="Permalink to &quot;\`clone(self) -&gt; &#39;set_grad_enabled&#39;\` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/grad_mode.py#L208)&quot;">​</a></h4><p>Create a copy of this class</p><hr></details><h2 id="functions" tabindex="-1">Functions <a class="header-anchor" href="#functions" aria-label="Permalink to &quot;Functions&quot;">​</a></h2><h3 id="grad-source" tabindex="-1"><code>grad()</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/__init__.py#L43" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#grad-source" aria-label="Permalink to &quot;\`grad()\` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/__init__.py#L43)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">grad(outputs: Union[tensorplay._C.TensorBase, collections.abc.Sequence[tensorplay._C.TensorBase]], inputs: Union[tensorplay._C.TensorBase, collections.abc.Sequence[tensorplay._C.TensorBase]], grad_outputs: Union[tensorplay._C.TensorBase, collections.abc.Sequence[tensorplay._C.TensorBase], NoneType] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, retain_graph: Optional[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">bool</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, create_graph: </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bool</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, allow_unused: Optional[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">bool</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tuple[tensorplay._C.TensorBase, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">...</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>Compute and return the sum of gradients of outputs with respect to the inputs.</p><p><code>grad_outputs</code> should be a sequence of length matching <code>output</code> containing the &quot;vector&quot; in vector-Jacobian product, usually the pre-computed gradients w.r.t. each of the outputs. If an output doesn&#39;t require_grad, then the gradient can be <code>None</code>).</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>If you run any forward ops, create <code>grad_outputs</code>, and/or call <code>grad</code> in a user-specified CUDA stream context, see <code>Stream semantics of backward passes&lt;bwd-cuda-stream-semantics&gt;</code>.</p></div><h4 id="args-1" tabindex="-1">Args <a class="header-anchor" href="#args-1" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>outputs</strong> (<code>sequence of Tensor or GradientEdge</code>): outputs of the differentiated function.</li><li><strong>inputs</strong> (<code>sequence of Tensor or GradientEdge</code>): Inputs w.r.t. which the gradient will be returned (and not accumulated into <code>.grad</code>).</li><li><strong>grad_outputs</strong> (<code>sequence of Tensor</code>): The &quot;vector&quot; in the vector-Jacobian product. Usually gradients w.r.t. each output. None values can be specified for scalar Tensors or ones that don&#39;t require grad. If a None value would be acceptable for all grad_tensors, then this argument is optional. Default: None.</li><li><strong>retain_graph</strong> (<code>bool, optional</code>): If <code>False</code>, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to <code>True</code> is not needed and often can be worked around in a much more efficient way. Defaults to the value of <code>create_graph</code>.</li><li><strong>create_graph</strong> (<code>bool, optional</code>): If <code>True</code>, graph of the derivative will be constructed, allowing to compute higher order derivative products.</li><li><strong>Default</strong>: <code>False</code>.</li><li><strong>allow_unused</strong> (<code>Optional[bool], optional</code>): If <code>False</code>, specifying inputs that were not used when computing outputs (and therefore their grad is always zero) is an error. Defaults to the value of <code>materialize_grads</code>.</li></ul><h3 id="is-grad-enabled-source" tabindex="-1"><code>is_grad_enabled()</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/grad_mode.py#L141" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#is-grad-enabled-source" aria-label="Permalink to &quot;\`is_grad_enabled()\` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/grad_mode.py#L141)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">is_grad_enabled()</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div>`,56)])])}const u=a(l,[["render",t]]);export{k as __pageData,u as default};
