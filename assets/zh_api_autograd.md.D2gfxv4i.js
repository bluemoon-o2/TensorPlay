import{_ as s,c as a,o as i,ah as e}from"./chunks/framework.DJtYh1Hc.js";const n=JSON.parse('{"title":"tensorplay.autograd","description":"","frontmatter":{},"headers":[],"relativePath":"zh/api/autograd.md","filePath":"zh/api/autograd.md"}');const l=s({name:"zh/api/autograd.md"},[["render",function(s,n,l,t,r,p){return i(),a("div",null,[...n[0]||(n[0]=[e('<h1 id="tensorplay-autograd" tabindex="-1">tensorplay.autograd <a class="header-anchor" href="#tensorplay-autograd" aria-label="Permalink to &quot;tensorplay.autograd&quot;">​</a></h1><p><code>tensorplay.autograd</code> 提供了实现任意标量值函数自动微分的类和函数。</p><p>它只需要对现有代码进行极少的更改 - 你只需要使用 <code>requires_grad=True</code> 关键字声明应计算梯度的 <code>Tensor</code>。 目前，我们仅支持浮点 <code>Tensor</code> 类型（half, float, double 和 bfloat16）和复数 <code>Tensor</code> 类型（cfloat, cdouble）的自动微分。</p><h2 id="classes" tabindex="-1">Classes <a class="header-anchor" href="#classes" aria-label="Permalink to &quot;Classes&quot;">​</a></h2><h3 id="class-function-source" tabindex="-1"><code>class Function</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/function.py#L18" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-function-source" aria-label="Permalink to &quot;`class Function` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/function.py#L18)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Function()</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>Records operation history and defines formulas for differentiating ops.</p><details><summary>Methods</summary><h4 id="apply-args-kwargs-source" tabindex="-1"><code>apply(*args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/function.py#L37" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#apply-args-kwargs-source" aria-label="Permalink to &quot;`apply(*args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/function.py#L37)&quot;">​</a></h4><hr><h4 id="backward-ctx-grad-outputs-source" tabindex="-1"><code>backward(ctx, *grad_outputs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/function.py#L30" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#backward-ctx-grad-outputs-source" aria-label="Permalink to &quot;`backward(ctx, *grad_outputs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/function.py#L30)&quot;">​</a></h4><p>定义微分操作的公式。</p><hr><h4 id="forward-ctx-args-kwargs-source" tabindex="-1"><code>forward(ctx, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/function.py#L23" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#forward-ctx-args-kwargs-source" aria-label="Permalink to &quot;`forward(ctx, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/function.py#L23)&quot;">​</a></h4><p>执行操作。</p><hr></details><h3 id="class-enable-grad-source" tabindex="-1"><code>class enable_grad</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/grad_mode.py#L86" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-enable-grad-source" aria-label="Permalink to &quot;`class enable_grad` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/grad_mode.py#L86)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">enable_grad(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">orig_func</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>_NoParamDecoratorContextManager</code></p><p>启用梯度计算的上下文管理器。</p><p>如果通过 <code>~no_grad</code> 或 <code>~set_grad_enabled</code> 禁用了梯度计算，则启用它。</p><p>此上下文管理器是线程局部的；它不会影响其他线程中的计算。</p><p>也可以作为装饰器使用。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>enable_grad 是几种可以在局部启用或禁用梯度的机制之一，有关它们的比较，请参阅 <code>locally-disable-grad-doc</code>。</p></div><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此 API 不适用于 <code>forward-mode AD &lt;forward-mode-ad&gt;</code>。</p></div><h4 id="example" tabindex="-1">Example <a class="header-anchor" href="#example" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.tensor([</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>\n<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">with</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.no_grad():</span></span>\n<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    with</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.enable_grad():</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y.requires_grad</span></span>\n<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y.backward()</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">x.grad</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.])</span></span>\n<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@tensorplay.enable_grad</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>\n<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> doubler</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(x):</span></span>\n<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span></span>\n<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">with</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.no_grad():</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    z </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> doubler(x)</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">z.requires_grad</span></span>\n<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span></span>\n<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@tensorplay.enable_grad</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>\n<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> tripler</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(x):</span></span>\n<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 3</span></span>\n<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">with</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.no_grad():</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    z </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tripler(x)</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">z.requires_grad</span></span>\n<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br></div></div><details><summary>Methods</summary><h4 id="clone-self-source" tabindex="-1"><code>clone(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/utils/_contextlib.py#L147" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#clone-self-source" aria-label="Permalink to &quot;`clone(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/utils/_contextlib.py#L147)&quot;">​</a></h4><hr></details><h3 id="class-no-grad-source" tabindex="-1"><code>class no_grad</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/grad_mode.py#L21" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-no-grad-source" aria-label="Permalink to &quot;`class no_grad` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/grad_mode.py#L21)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">no_grad() </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>_NoParamDecoratorContextManager</code></p><p>Context-manager that disables gradient calculation.</p><p>Disabling gradient calculation is useful for inference, when you are sure that you will not call <code>Tensor.backward()</code>. It will reduce memory consumption for computations that would otherwise have <code>requires_grad=True</code>.</p><p>In this mode, the result of every computation will have <code>requires_grad=False</code>, even when the inputs have <code>requires_grad=True</code>. There is an exception! All factory functions, or functions that create a new Tensor and take a requires_grad kwarg, will NOT be affected by this mode.</p><p>This context manager is thread local; it will not affect computation in other threads.</p><p>Also functions as a decorator.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>No-grad is one of several mechanisms that can enable or disable gradients locally see <code>locally-disable-grad-doc</code> for more information on how they compare.</p></div><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This API does not apply to <code>forward-mode AD &lt;forward-mode-ad&gt;</code>. If you want to disable forward AD for a computation, you can unpack your dual tensors.</p></div><h4 id="example-1" tabindex="-1">Example <a class="header-anchor" href="#example-1" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.tensor([</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>\n<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">with</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.no_grad():</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y.requires_grad</span></span>\n<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">False</span></span>\n<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@tensorplay.no_grad</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>\n<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> doubler</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(x):</span></span>\n<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">z </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> doubler(x)</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">z.requires_grad</span></span>\n<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">False</span></span>\n<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@tensorplay.no_grad</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>\n<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> tripler</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(x):</span></span>\n<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 3</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">z </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tripler(x)</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">z.requires_grad</span></span>\n<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">False</span></span>\n<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># factory function exception</span></span>\n<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">with</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.no_grad():</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    a </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.nn.Parameter(tensorplay.rand(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">a.requires_grad</span></span>\n<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br></div></div><details><summary>Methods</summary><h4 id="init-self-none-source" tabindex="-1"><code>__init__(self) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/grad_mode.py#L74" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-none-source" aria-label="Permalink to &quot;`__init__(self) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/grad_mode.py#L74)&quot;">​</a></h4><p>Initialize self. See help(type(self)) for accurate signature.</p><hr><h4 id="clone-self-source-1" tabindex="-1"><code>clone(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/utils/_contextlib.py#L147" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#clone-self-source-1" aria-label="Permalink to &quot;`clone(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/utils/_contextlib.py#L147)&quot;">​</a></h4><hr></details><h3 id="class-set-grad-enabled-source" tabindex="-1"><code>class set_grad_enabled</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/grad_mode.py#L145" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-set-grad-enabled-source" aria-label="Permalink to &quot;`class set_grad_enabled` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/grad_mode.py#L145)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">set_grad_enabled(mode: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">bool</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>_DecoratorContextManager</code></p><p>Context-manager that sets gradient calculation on or off.</p><p><code>set_grad_enabled</code> will enable or disable grads based on its argument <code>mode</code>. It can be used as a context-manager or as a function.</p><p>This context manager is thread local; it will not affect computation in other threads.</p><h4 id="args" tabindex="-1">Args <a class="header-anchor" href="#args" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>mode</strong> (<code>bool</code>): Flag whether to enable grad (<code>True</code>), or disable (<code>False</code>). This can be used to conditionally enable gradients.</li></ul><div class="info custom-block"><p class="custom-block-title">INFO</p><p>set_grad_enabled is one of several mechanisms that can enable or disable gradients locally see <code>locally-disable-grad-doc</code> for more information on how they compare.</p></div><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This API does not apply to <code>forward-mode AD &lt;forward-mode-ad&gt;</code>.</p></div><h4 id="example-2" tabindex="-1">Example <a class="header-anchor" href="#example-2" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.tensor([</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">is_train </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> False</span></span>\n<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">with</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.set_grad_enabled(is_train):</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y.requires_grad</span></span>\n<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">False</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">_ </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.set_grad_enabled(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y.requires_grad</span></span>\n<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">_ </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.set_grad_enabled(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span></span>\n<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y.requires_grad</span></span>\n<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">False</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><details><summary>Methods</summary><h4 id="init-self-mode-bool-none-source" tabindex="-1"><code>__init__(self, mode: bool) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/grad_mode.py#L186" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-mode-bool-none-source" aria-label="Permalink to &quot;`__init__(self, mode: bool) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/grad_mode.py#L186)&quot;">​</a></h4><p>Initialize self. See help(type(self)) for accurate signature.</p><hr><h4 id="clone-self-set-grad-enabled-source" tabindex="-1"><code>clone(self) -&gt; &#39;set_grad_enabled&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/grad_mode.py#L208" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#clone-self-set-grad-enabled-source" aria-label="Permalink to &quot;`clone(self) -&gt; &#39;set_grad_enabled&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/grad_mode.py#L208)&quot;">​</a></h4><p>Create a copy of this class</p><hr></details><h2 id="functions" tabindex="-1">Functions <a class="header-anchor" href="#functions" aria-label="Permalink to &quot;Functions&quot;">​</a></h2><h3 id="grad-source" tabindex="-1"><code>grad()</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/__init__.py#L43" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#grad-source" aria-label="Permalink to &quot;`grad()` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/__init__.py#L43)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">grad(outputs: Union[tensorplay._C.TensorBase, collections.abc.Sequence[tensorplay._C.TensorBase]], inputs: Union[tensorplay._C.TensorBase, collections.abc.Sequence[tensorplay._C.TensorBase]], grad_outputs: Union[tensorplay._C.TensorBase, collections.abc.Sequence[tensorplay._C.TensorBase], NoneType] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, retain_graph: Optional[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">bool</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, create_graph: </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bool</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, allow_unused: Optional[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">bool</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tuple[tensorplay._C.TensorBase, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">...</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>计算并返回输出相对于输入的梯度之和。</p><p><code>grad_outputs</code> 应该是长度与 <code>output</code> 匹配的序列，包含向量-雅可比积中的“向量”，通常是相对于每个输出的预计算梯度。如果输出不需要梯度 (require_grad)，则梯度可以是 <code>None</code>。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>如果你在用户指定的 CUDA 流上下文中运行任何前向操作、创建 <code>grad_outputs</code> 和/或调用 <code>grad</code>，请参阅 <code>Stream semantics of backward passes&lt;bwd-cuda-stream-semantics&gt;</code>。</p></div><h4 id="args-1" tabindex="-1">Args <a class="header-anchor" href="#args-1" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>outputs</strong> (<code>Tensor 或 GradientEdge 的序列</code>): 微分函数的输出。</li><li><strong>inputs</strong> (<code>Tensor 或 GradientEdge 的序列</code>): 将返回其梯度的输入（并且不会累积到 <code>.grad</code> 中）。</li><li><strong>grad_outputs</strong> (<code>Tensor 的序列</code>): 向量-雅可比积中的“向量”。通常是相对于每个输出的梯度。对于标量张量或不需要梯度的张量，可以指定 None 值。如果所有 grad_tensors 都可以接受 None 值，则此参数是可选的。默认值: None。</li><li><strong>retain_graph</strong> (<code>bool, optional</code>): 如果为 <code>False</code>，用于计算梯度的图将被释放。请注意，在几乎所有情况下，都不需要将此选项设置为 <code>True</code>，通常可以以更有效的方式解决。默认为 <code>create_graph</code> 的值。</li><li><strong>create_graph</strong> (<code>bool, optional</code>): 如果为 <code>True</code>，将构建导数图，允许计算高阶导数乘积。</li><li><strong>Default</strong>: <code>False</code>。</li><li><strong>allow_unused</strong> (<code>Optional[bool], optional</code>): 如果为 <code>False</code>，指定在计算输出时未使用的输入（因此它们的梯度始终为零）是一个错误。默认为 <code>materialize_grads</code> 的值。</li></ul><h3 id="is-grad-enabled-source" tabindex="-1"><code>is_grad_enabled()</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/grad_mode.py#L141" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#is-grad-enabled-source" aria-label="Permalink to &quot;`is_grad_enabled()` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/autograd/grad_mode.py#L141)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">is_grad_enabled()</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div>',56)])])}]]);export{n as __pageData,l as default};
