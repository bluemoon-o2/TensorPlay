<!DOCTYPE html>
<html lang="zh" dir="ltr">
  <head>
    <meta charset="utf-8">
    
    <title>tensorplay.nn.parameter | TensorPlay</title>
    <meta name="description" content="一个适合学习者、兼容 PyTorch 的深度学习框架。">
    <meta name="generator" content="VitePress v1.6.4">
    <link rel="preload stylesheet" href="/assets/style.Co3iBdXt.css" as="style">
    <link rel="preload stylesheet" href="/vp-icons.css" as="style">
    
    <script type="module" src="/assets/app.C8Rhfwht.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/theme.BOBjxezA.js">
    <link rel="modulepreload" href="/assets/chunks/framework.DWXU4yX9.js">
    <link rel="modulepreload" href="/assets/zh_api_parameter.md.XiiS9CGF.lean.js">
    <link rel="icon" type="image/png" href="/images/logo-0.png">
    <link rel="apple-touch-icon" href="/images/logo-0.png">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <meta name="keywords" content="deep learning, machine learning, ai framework, pytorch compatible, educational, c++, python, tensorplay, 深度学习, 机器学习, 自动微分">
    <meta name="author" content="TensorPlay Team">
    <meta property="og:type" content="website">
    <meta property="og:title" content="TensorPlay | Transparent &amp; Compatible AI Framework">
    <meta property="og:description" content="A transparent, educational, and PyTorch-compatible deep learning framework for the modern AI era.">
    <meta property="og:site_name" content="TensorPlay">
    <meta property="og:url" content="https://www.tensorplay.cn">
    <meta property="og:image" content="https://www.tensorplay.cn/images/logo-1.png">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="TensorPlay | Transparent &amp; Compatible AI Framework">
    <meta name="twitter:description" content="A transparent, educational, and PyTorch-compatible deep learning framework for the modern AI era.">
    <meta name="twitter:image" content="https://www.tensorplay.cn/images/logo-1.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@300;400;500;600;700&amp;display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-5d98c3a5><!--[--><canvas id="cyberpunk-particles" data-v-36680640></canvas><!----><!--]--><!--[--><span tabindex="-1" data-v-0b0ada53></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-0b0ada53>Skip to content</a><!--]--><!----><header class="VPNav" data-v-5d98c3a5 data-v-ae24b3ad><div class="VPNavBar" data-v-ae24b3ad data-v-6aa21345><div class="wrapper" data-v-6aa21345><div class="container" data-v-6aa21345><div class="title" data-v-6aa21345><div class="VPNavBarTitle has-sidebar" data-v-6aa21345 data-v-1168a8e4><a class="title" href="/zh/" data-v-1168a8e4><!--[--><!--]--><!--[--><img class="VPImage logo" src="/images/logo-0.png" alt data-v-8426fc1a><!--]--><span data-v-1168a8e4>TensorPlay</span><!--[--><!--]--></a></div></div><div class="content" data-v-6aa21345><div class="content-body" data-v-6aa21345><!--[--><!--]--><div class="VPNavBarSearch search" data-v-6aa21345><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="搜索文档"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">搜索文档</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-6aa21345 data-v-dc692963><span id="main-nav-aria-label" class="visually-hidden" data-v-dc692963> Main Navigation </span><!--[--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-dc692963 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-cf11d7a2><span class="text" data-v-cf11d7a2><!----><span data-v-cf11d7a2>学习</span><span class="vpi-chevron-down text-icon" data-v-cf11d7a2></span></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><div class="items" data-v-b98bc113><!--[--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/zh/guide/getting-started.html" data-v-35975db6><!--[--><span data-v-35975db6>快速开始</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/zh/guide/tutorials.html" data-v-35975db6><!--[--><span data-v-35975db6>教程</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/community/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>社区</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/ecosystem/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>项目</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/api/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>文档</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/blog/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>博客与新闻</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/about/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>关于</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/join/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>加入我们</span><!--]--></a><!--]--><!--]--></nav><div class="VPFlyout VPNavBarTranslations translations" data-v-6aa21345 data-v-88af2de4 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="Change language" data-v-cf11d7a2><span class="text" data-v-cf11d7a2><span class="vpi-languages option-icon" data-v-cf11d7a2></span><!----><span class="vpi-chevron-down text-icon" data-v-cf11d7a2></span></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><!----><!--[--><!--[--><div class="items" data-v-88af2de4><p class="title" data-v-88af2de4>简体中文</p><!--[--><div class="VPMenuLink" data-v-88af2de4 data-v-35975db6><a class="VPLink link" href="/api/parameter.html" data-v-35975db6><!--[--><span data-v-35975db6>English</span><!--]--></a></div><!--]--></div><!--]--><!--]--></div></div></div><div class="VPNavBarAppearance appearance" data-v-6aa21345 data-v-6c893767><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-6c893767 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-6aa21345 data-v-0394ad82 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/bluemoon-o2/tensorplay" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-6aa21345 data-v-bb2aa2f0 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-cf11d7a2><span class="vpi-more-horizontal icon" data-v-cf11d7a2></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><!----><!--[--><!--[--><div class="group translations" data-v-bb2aa2f0><p class="trans-title" data-v-bb2aa2f0>简体中文</p><!--[--><div class="VPMenuLink" data-v-bb2aa2f0 data-v-35975db6><a class="VPLink link" href="/api/parameter.html" data-v-35975db6><!--[--><span data-v-35975db6>English</span><!--]--></a></div><!--]--></div><div class="group" data-v-bb2aa2f0><div class="item appearance" data-v-bb2aa2f0><p class="label" data-v-bb2aa2f0>Appearance</p><div class="appearance-action" data-v-bb2aa2f0><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-bb2aa2f0 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div></div></div><div class="group" data-v-bb2aa2f0><div class="item social-links" data-v-bb2aa2f0><div class="VPSocialLinks social-links-list" data-v-bb2aa2f0 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/bluemoon-o2/tensorplay" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-6aa21345 data-v-e5dd9c1c><span class="container" data-v-e5dd9c1c><span class="top" data-v-e5dd9c1c></span><span class="middle" data-v-e5dd9c1c></span><span class="bottom" data-v-e5dd9c1c></span></span></button></div></div></div></div><div class="divider" data-v-6aa21345><div class="divider-line" data-v-6aa21345></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-5d98c3a5 data-v-a6f0e41e><div class="container" data-v-a6f0e41e><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-a6f0e41e><span class="vpi-align-left menu-icon" data-v-a6f0e41e></span><span class="menu-text" data-v-a6f0e41e>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-a6f0e41e data-v-8a42e2b4><button data-v-8a42e2b4>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-5d98c3a5 data-v-319d5ca6><div class="curtain" data-v-319d5ca6></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-319d5ca6><span class="visually-hidden" id="sidebar-aria-label" data-v-319d5ca6> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-c40bc020><section class="VPSidebarItem level-0" data-v-c40bc020 data-v-b3fd67f8><div class="item" role="button" tabindex="0" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><h2 class="text" data-v-b3fd67f8>核心 API</h2><!----></div><div class="items" data-v-b3fd67f8><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>概览</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/tensorplay.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>tensorplay</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/autograd.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>autograd</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/functional.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>functional</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/optim.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>optim</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/cuda.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>cuda</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-c40bc020><section class="VPSidebarItem level-0 collapsible" data-v-c40bc020 data-v-b3fd67f8><div class="item" role="button" tabindex="0" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><h2 class="text" data-v-b3fd67f8>神经网络 (nn)</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-b3fd67f8><span class="vpi-chevron-right caret-icon" data-v-b3fd67f8></span></div></div><div class="items" data-v-b3fd67f8><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/nn.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>nn</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/modules.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Modules</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/linear.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Linear Layers</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/conv.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Convolution Layers</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/pooling.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Pooling Layers</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/activation.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Activation Functions</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/normalization.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Normalization Layers</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/loss.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Loss Functions</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/dropout.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Dropout Layers</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/container.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Container</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-5d98c3a5 data-v-1428d186><div class="VPDoc has-sidebar has-aside" data-v-1428d186 data-v-39a288b8><!--[--><!--]--><div class="container" data-v-39a288b8><div class="aside" data-v-39a288b8><div class="aside-curtain" data-v-39a288b8></div><div class="aside-container" data-v-39a288b8><div class="aside-content" data-v-39a288b8><div class="VPDocAside" data-v-39a288b8 data-v-3f215769><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-3f215769 data-v-a5bbad30><div class="content" data-v-a5bbad30><div class="outline-marker" data-v-a5bbad30></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-a5bbad30>On this page</div><ul class="VPDocOutlineItem root" data-v-a5bbad30 data-v-b933a997><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-3f215769></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-39a288b8><div class="content-container" data-v-39a288b8><!--[--><!--]--><main class="main" data-v-39a288b8><div style="position:relative;" class="vp-doc _zh_api_parameter" data-v-39a288b8><div><div class="warning custom-block"><p class="custom-block-title">WARNING</p><p>该页面尚未翻译。 以下内容为英文原版。</p></div><h1 id="tensorplay-nn-parameter" tabindex="-1">tensorplay.nn.parameter <a class="header-anchor" href="#tensorplay-nn-parameter" aria-label="Permalink to &quot;tensorplay.nn.parameter&quot;">​</a></h1><h2 id="classes" tabindex="-1">Classes <a class="header-anchor" href="#classes" aria-label="Permalink to &quot;Classes&quot;">​</a></h2><h3 id="class-buffer-source" tabindex="-1"><code>class Buffer</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L158" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-buffer-source" aria-label="Permalink to &quot;`class Buffer` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L158)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Buffer(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">data</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">persistent</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>TensorBase</code></p><p>A kind of Tensor that should not be considered a model parameter. For example, BatchNorm&#39;s <code>running_mean</code> is not a parameter, but is part of the module&#39;s state.</p><p>Buffers are <code>~tensorplay.Tensor</code> subclasses, that have a very special property when used with <code>Module</code> s -- when they&#39;re assigned as Module attributes they are automatically added to the list of its buffers, and will appear e.g. in <code>~tensorplay.nn.Module.buffers</code> iterator. Assigning a Tensor doesn&#39;t have such effect. One can still assign a Tensor as explicitly by using the <code>~tensorplay.nn.Module.register_buffer</code> function.</p><h4 id="args" tabindex="-1">Args <a class="header-anchor" href="#args" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>data</strong> (<code>Tensor</code>): buffer tensor.</li><li><strong>persistent</strong> (<code>bool, optional</code>): whether the buffer is part of the module&#39;s <code>state_dict</code>. Default: <code>True</code></li></ul><details><summary>Methods</summary><h4 id="init-self-data-none-persistent-true-source" tabindex="-1"><code>__init__(self, data=None, persistent=True)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L174" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-data-none-persistent-true-source" aria-label="Permalink to &quot;`__init__(self, data=None, persistent=True)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L174)&quot;">​</a></h4><p><strong>init</strong>(self) -&gt; None <strong>init</strong>(self, data: object, dtype: tensorplay.DType | None = None, device: tensorplay.Device | None = None, requires_grad: bool = False) -&gt; None</p><hr><h4 id="cpu-self-source" tabindex="-1"><code>cpu(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L123" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cpu-self-source" aria-label="Permalink to &quot;`cpu(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L123)&quot;">​</a></h4><p>Returns a copy of this object in CPU memory. If this object is already in CPU memory, then no copy is performed and the original object is returned.</p><hr><h4 id="cuda-self-device-none-non-blocking-false-source" tabindex="-1"><code>cuda(self, device=None, non_blocking=False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L107" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cuda-self-device-none-non-blocking-false-source" aria-label="Permalink to &quot;`cuda(self, device=None, non_blocking=False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L107)&quot;">​</a></h4><p>Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.</p><hr><h4 id="double-self-source" tabindex="-1"><code>double(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L104" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#double-self-source" aria-label="Permalink to &quot;`double(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L104)&quot;">​</a></h4><hr><h4 id="flatten-self-start-dim-0-end-dim-1-source" tabindex="-1"><code>flatten(self, start_dim=0, end_dim=-1)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L19" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#flatten-self-start-dim-0-end-dim-1-source" aria-label="Permalink to &quot;`flatten(self, start_dim=0, end_dim=-1)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L19)&quot;">​</a></h4><p>Flattens a contiguous range of dims.</p><hr><h4 id="float-self-source" tabindex="-1"><code>float(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L98" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#float-self-source" aria-label="Permalink to &quot;`float(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L98)&quot;">​</a></h4><hr><h4 id="int-self-source" tabindex="-1"><code>int(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L101" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#int-self-source" aria-label="Permalink to &quot;`int(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L101)&quot;">​</a></h4><hr><h4 id="is-float-self-bool-source" tabindex="-1"><code>is_float(self) -&gt; bool</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L7" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#is-float-self-bool-source" aria-label="Permalink to &quot;`is_float(self) -&gt; bool` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L7)&quot;">​</a></h4><p>Check if tensor is floating point.</p><hr><h4 id="long-self-source" tabindex="-1"><code>long(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L95" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#long-self-source" aria-label="Permalink to &quot;`long(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L95)&quot;">​</a></h4><hr><h4 id="ndimension-self-int-source" tabindex="-1"><code>ndimension(self) -&gt; int</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L13" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#ndimension-self-int-source" aria-label="Permalink to &quot;`ndimension(self) -&gt; int` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L13)&quot;">​</a></h4><p>Alias for dim()</p><hr><h4 id="t-self-source" tabindex="-1"><code>t(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L130" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#t-self-source" aria-label="Permalink to &quot;`t(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L130)&quot;">​</a></h4><p>Returns the transpose of the tensor. Aliased to transpose(0, 1) to ensure correct autograd behavior (TransposeBackward).</p><hr><h4 id="unflatten-self-dim-sizes-source" tabindex="-1"><code>unflatten(self, dim, sizes)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L51" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#unflatten-self-dim-sizes-source" aria-label="Permalink to &quot;`unflatten(self, dim, sizes)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L51)&quot;">​</a></h4><p>Expands a dimension of the input tensor over multiple dimensions.</p><hr></details><h3 id="class-parameter-source" tabindex="-1"><code>class Parameter</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L15" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-parameter-source" aria-label="Permalink to &quot;`class Parameter` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L15)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">data</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>TensorBase</code></p><p>A kind of Tensor that is to be considered a module parameter.</p><p>Parameters are <code>~tensorplay.Tensor</code> subclasses, that have a very special property when used with <code>Module</code> s - when they&#39;re assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in <code>~Module.parameters</code> iterator. Assigning a Tensor doesn&#39;t have such effect. This is because one might want to cache some temporary state, like last hidden state of the RNN, in the model. If there was no such class as <code>Parameter</code>, these temporaries would get registered too.</p><h4 id="args-1" tabindex="-1">Args <a class="header-anchor" href="#args-1" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>data</strong> (<code>Tensor</code>): parameter tensor.</li><li><strong>requires_grad</strong> (<code>bool, optional</code>): if the parameter requires gradient. Note that the tensorplay.no_grad() context does NOT affect the default behavior of Parameter creation--the Parameter will always have <code>requires_grad=True</code> unless given explicitly. Default: <code>True</code></li></ul><details><summary>Methods</summary><h4 id="init-self-data-none-requires-grad-true-source" tabindex="-1"><code>__init__(self, data=None, requires_grad=True)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L34" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-data-none-requires-grad-true-source" aria-label="Permalink to &quot;`__init__(self, data=None, requires_grad=True)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L34)&quot;">​</a></h4><p><strong>init</strong>(self) -&gt; None <strong>init</strong>(self, data: object, dtype: tensorplay.DType | None = None, device: tensorplay.Device | None = None, requires_grad: bool = False) -&gt; None</p><hr><h4 id="cpu-self-source-1" tabindex="-1"><code>cpu(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L123" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cpu-self-source-1" aria-label="Permalink to &quot;`cpu(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L123)&quot;">​</a></h4><p>Returns a copy of this object in CPU memory. If this object is already in CPU memory, then no copy is performed and the original object is returned.</p><hr><h4 id="cuda-self-device-none-non-blocking-false-source-1" tabindex="-1"><code>cuda(self, device=None, non_blocking=False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L107" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cuda-self-device-none-non-blocking-false-source-1" aria-label="Permalink to &quot;`cuda(self, device=None, non_blocking=False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L107)&quot;">​</a></h4><p>Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.</p><hr><h4 id="double-self-source-1" tabindex="-1"><code>double(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L104" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#double-self-source-1" aria-label="Permalink to &quot;`double(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L104)&quot;">​</a></h4><hr><h4 id="flatten-self-start-dim-0-end-dim-1-source-1" tabindex="-1"><code>flatten(self, start_dim=0, end_dim=-1)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L19" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#flatten-self-start-dim-0-end-dim-1-source-1" aria-label="Permalink to &quot;`flatten(self, start_dim=0, end_dim=-1)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L19)&quot;">​</a></h4><p>Flattens a contiguous range of dims.</p><hr><h4 id="float-self-source-1" tabindex="-1"><code>float(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L98" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#float-self-source-1" aria-label="Permalink to &quot;`float(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L98)&quot;">​</a></h4><hr><h4 id="int-self-source-1" tabindex="-1"><code>int(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L101" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#int-self-source-1" aria-label="Permalink to &quot;`int(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L101)&quot;">​</a></h4><hr><h4 id="is-float-self-bool-source-1" tabindex="-1"><code>is_float(self) -&gt; bool</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L7" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#is-float-self-bool-source-1" aria-label="Permalink to &quot;`is_float(self) -&gt; bool` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L7)&quot;">​</a></h4><p>Check if tensor is floating point.</p><hr><h4 id="long-self-source-1" tabindex="-1"><code>long(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L95" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#long-self-source-1" aria-label="Permalink to &quot;`long(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L95)&quot;">​</a></h4><hr><h4 id="ndimension-self-int-source-1" tabindex="-1"><code>ndimension(self) -&gt; int</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L13" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#ndimension-self-int-source-1" aria-label="Permalink to &quot;`ndimension(self) -&gt; int` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L13)&quot;">​</a></h4><p>Alias for dim()</p><hr><h4 id="t-self-source-1" tabindex="-1"><code>t(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L130" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#t-self-source-1" aria-label="Permalink to &quot;`t(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L130)&quot;">​</a></h4><p>Returns the transpose of the tensor. Aliased to transpose(0, 1) to ensure correct autograd behavior (TransposeBackward).</p><hr><h4 id="unflatten-self-dim-sizes-source-1" tabindex="-1"><code>unflatten(self, dim, sizes)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L51" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#unflatten-self-dim-sizes-source-1" aria-label="Permalink to &quot;`unflatten(self, dim, sizes)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L51)&quot;">​</a></h4><p>Expands a dimension of the input tensor over multiple dimensions.</p><hr></details><h3 id="class-uninitializedbuffer-source" tabindex="-1"><code>class UninitializedBuffer</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L195" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-uninitializedbuffer-source" aria-label="Permalink to &quot;`class UninitializedBuffer` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L195)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">UninitializedBuffer(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">persistent</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>TensorBase</code></p><p>A buffer that is not initialized.</p><p>Uninitialized Buffer is a a special case of <code>tensorplay.Tensor</code> where the shape of the data is still unknown.</p><p>Unlike a <code>tensorplay.Tensor</code>, uninitialized parameters hold no data and attempting to access some properties, like their shape, will throw a runtime error. The only operations that can be performed on a uninitialized parameter are changing its datatype, moving it to a different device and converting it to a regular <code>tensorplay.Tensor</code>.</p><p>The default device or dtype to use when the buffer is materialized can be set during construction using e.g. <code>device=&#39;cuda&#39;</code>.</p><details><summary>Methods</summary><h4 id="init-self-requires-grad-false-device-none-dtype-none-persistent-true-source" tabindex="-1"><code>__init__(self, requires_grad=False, device=None, dtype=None, persistent=True)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L212" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-requires-grad-false-device-none-dtype-none-persistent-true-source" aria-label="Permalink to &quot;`__init__(self, requires_grad=False, device=None, dtype=None, persistent=True)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L212)&quot;">​</a></h4><p><strong>init</strong>(self) -&gt; None <strong>init</strong>(self, data: object, dtype: tensorplay.DType | None = None, device: tensorplay.Device | None = None, requires_grad: bool = False) -&gt; None</p><hr><h4 id="cpu-self-source-2" tabindex="-1"><code>cpu(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L123" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cpu-self-source-2" aria-label="Permalink to &quot;`cpu(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L123)&quot;">​</a></h4><p>Returns a copy of this object in CPU memory. If this object is already in CPU memory, then no copy is performed and the original object is returned.</p><hr><h4 id="cuda-self-device-none-non-blocking-false-source-2" tabindex="-1"><code>cuda(self, device=None, non_blocking=False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L107" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cuda-self-device-none-non-blocking-false-source-2" aria-label="Permalink to &quot;`cuda(self, device=None, non_blocking=False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L107)&quot;">​</a></h4><p>Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.</p><hr><h4 id="double-self-source-2" tabindex="-1"><code>double(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L104" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#double-self-source-2" aria-label="Permalink to &quot;`double(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L104)&quot;">​</a></h4><hr><h4 id="flatten-self-start-dim-0-end-dim-1-source-2" tabindex="-1"><code>flatten(self, start_dim=0, end_dim=-1)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L19" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#flatten-self-start-dim-0-end-dim-1-source-2" aria-label="Permalink to &quot;`flatten(self, start_dim=0, end_dim=-1)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L19)&quot;">​</a></h4><p>Flattens a contiguous range of dims.</p><hr><h4 id="float-self-source-2" tabindex="-1"><code>float(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L98" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#float-self-source-2" aria-label="Permalink to &quot;`float(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L98)&quot;">​</a></h4><hr><h4 id="int-self-source-2" tabindex="-1"><code>int(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L101" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#int-self-source-2" aria-label="Permalink to &quot;`int(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L101)&quot;">​</a></h4><hr><h4 id="is-float-self-bool-source-2" tabindex="-1"><code>is_float(self) -&gt; bool</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L7" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#is-float-self-bool-source-2" aria-label="Permalink to &quot;`is_float(self) -&gt; bool` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L7)&quot;">​</a></h4><p>Check if tensor is floating point.</p><hr><h4 id="long-self-source-2" tabindex="-1"><code>long(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L95" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#long-self-source-2" aria-label="Permalink to &quot;`long(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L95)&quot;">​</a></h4><hr><h4 id="materialize-self-shape-device-none-dtype-none-source" tabindex="-1"><code>materialize(self, shape, device=None, dtype=None)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L224" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#materialize-self-shape-device-none-dtype-none-source" aria-label="Permalink to &quot;`materialize(self, shape, device=None, dtype=None)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L224)&quot;">​</a></h4><hr><h4 id="ndimension-self-int-source-2" tabindex="-1"><code>ndimension(self) -&gt; int</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L13" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#ndimension-self-int-source-2" aria-label="Permalink to &quot;`ndimension(self) -&gt; int` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L13)&quot;">​</a></h4><p>Alias for dim()</p><hr><h4 id="share-memory-self-source" tabindex="-1"><code>share_memory_(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L231" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#share-memory-self-source" aria-label="Permalink to &quot;`share_memory_(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L231)&quot;">​</a></h4><p>share_memory_(self) -&gt; object</p><hr><h4 id="t-self-source-2" tabindex="-1"><code>t(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L130" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#t-self-source-2" aria-label="Permalink to &quot;`t(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L130)&quot;">​</a></h4><p>Returns the transpose of the tensor. Aliased to transpose(0, 1) to ensure correct autograd behavior (TransposeBackward).</p><hr><h4 id="unflatten-self-dim-sizes-source-2" tabindex="-1"><code>unflatten(self, dim, sizes)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L51" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#unflatten-self-dim-sizes-source-2" aria-label="Permalink to &quot;`unflatten(self, dim, sizes)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L51)&quot;">​</a></h4><p>Expands a dimension of the input tensor over multiple dimensions.</p><hr></details><h3 id="class-uninitializedparameter-source" tabindex="-1"><code>class UninitializedParameter</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L112" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-uninitializedparameter-source" aria-label="Permalink to &quot;`class UninitializedParameter` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L112)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">UninitializedParameter(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>Parameter</code></p><p>A parameter that is not initialized.</p><p>Uninitialized Parameters are a special case of <code>tensorplay.nn.Parameter</code> where the shape of the data is still unknown.</p><p>Unlike a <code>tensorplay.nn.Parameter</code>, uninitialized parameters hold no data and attempting to access some properties, like their shape, will throw a runtime error. The only operations that can be performed on a uninitialized parameter are changing its datatype, moving it to a different device and converting it to a regular <code>tensorplay.nn.Parameter</code>.</p><p>The default device or dtype to use when the parameter is materialized can be set during construction using e.g. <code>device=&#39;cuda&#39;</code>.</p><details><summary>Methods</summary><h4 id="init-self-requires-grad-true-device-none-dtype-none-source" tabindex="-1"><code>__init__(self, requires_grad=True, device=None, dtype=None)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L129" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-requires-grad-true-device-none-dtype-none-source" aria-label="Permalink to &quot;`__init__(self, requires_grad=True, device=None, dtype=None)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L129)&quot;">​</a></h4><p><strong>init</strong>(self) -&gt; None <strong>init</strong>(self, data: object, dtype: tensorplay.DType | None = None, device: tensorplay.Device | None = None, requires_grad: bool = False) -&gt; None</p><hr><h4 id="cpu-self-source-3" tabindex="-1"><code>cpu(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L123" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cpu-self-source-3" aria-label="Permalink to &quot;`cpu(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L123)&quot;">​</a></h4><p>Returns a copy of this object in CPU memory. If this object is already in CPU memory, then no copy is performed and the original object is returned.</p><hr><h4 id="cuda-self-device-none-non-blocking-false-source-3" tabindex="-1"><code>cuda(self, device=None, non_blocking=False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L107" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cuda-self-device-none-non-blocking-false-source-3" aria-label="Permalink to &quot;`cuda(self, device=None, non_blocking=False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L107)&quot;">​</a></h4><p>Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.</p><hr><h4 id="double-self-source-3" tabindex="-1"><code>double(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L104" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#double-self-source-3" aria-label="Permalink to &quot;`double(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L104)&quot;">​</a></h4><hr><h4 id="flatten-self-start-dim-0-end-dim-1-source-3" tabindex="-1"><code>flatten(self, start_dim=0, end_dim=-1)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L19" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#flatten-self-start-dim-0-end-dim-1-source-3" aria-label="Permalink to &quot;`flatten(self, start_dim=0, end_dim=-1)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L19)&quot;">​</a></h4><p>Flattens a contiguous range of dims.</p><hr><h4 id="float-self-source-3" tabindex="-1"><code>float(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L98" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#float-self-source-3" aria-label="Permalink to &quot;`float(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L98)&quot;">​</a></h4><hr><h4 id="int-self-source-3" tabindex="-1"><code>int(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L101" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#int-self-source-3" aria-label="Permalink to &quot;`int(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L101)&quot;">​</a></h4><hr><h4 id="is-float-self-bool-source-3" tabindex="-1"><code>is_float(self) -&gt; bool</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L7" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#is-float-self-bool-source-3" aria-label="Permalink to &quot;`is_float(self) -&gt; bool` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L7)&quot;">​</a></h4><p>Check if tensor is floating point.</p><hr><h4 id="long-self-source-3" tabindex="-1"><code>long(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L95" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#long-self-source-3" aria-label="Permalink to &quot;`long(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L95)&quot;">​</a></h4><hr><h4 id="materialize-self-shape-device-none-dtype-none-source-1" tabindex="-1"><code>materialize(self, shape, device=None, dtype=None)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L138" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#materialize-self-shape-device-none-dtype-none-source-1" aria-label="Permalink to &quot;`materialize(self, shape, device=None, dtype=None)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L138)&quot;">​</a></h4><hr><h4 id="ndimension-self-int-source-3" tabindex="-1"><code>ndimension(self) -&gt; int</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L13" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#ndimension-self-int-source-3" aria-label="Permalink to &quot;`ndimension(self) -&gt; int` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L13)&quot;">​</a></h4><p>Alias for dim()</p><hr><h4 id="share-memory-self-source-1" tabindex="-1"><code>share_memory_(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L145" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#share-memory-self-source-1" aria-label="Permalink to &quot;`share_memory_(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L145)&quot;">​</a></h4><p>share_memory_(self) -&gt; object</p><hr><h4 id="t-self-source-3" tabindex="-1"><code>t(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L130" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#t-self-source-3" aria-label="Permalink to &quot;`t(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L130)&quot;">​</a></h4><p>Returns the transpose of the tensor. Aliased to transpose(0, 1) to ensure correct autograd behavior (TransposeBackward).</p><hr><h4 id="unflatten-self-dim-sizes-source-3" tabindex="-1"><code>unflatten(self, dim, sizes)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L51" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#unflatten-self-dim-sizes-source-3" aria-label="Permalink to &quot;`unflatten(self, dim, sizes)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/_tensor.py#L51)&quot;">​</a></h4><p>Expands a dimension of the input tensor over multiple dimensions.</p><hr></details><h3 id="class-uninitializedtensormixin-source" tabindex="-1"><code>class UninitializedTensorMixin</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L52" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-uninitializedtensormixin-source" aria-label="Permalink to &quot;`class UninitializedTensorMixin` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L52)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">UninitializedTensorMixin()</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><details><summary>Methods</summary><h4 id="materialize-self-shape-device-none-dtype-none-source-2" tabindex="-1"><code>materialize(self, shape, device=None, dtype=None)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L53" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#materialize-self-shape-device-none-dtype-none-source-2" aria-label="Permalink to &quot;`materialize(self, shape, device=None, dtype=None)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L53)&quot;">​</a></h4><p>Create a Parameter or Tensor with the same properties of the uninitialized one.</p><p>Given a shape, it materializes a parameter in the same device and with the same <code>dtype</code> as the current one or the specified ones in the arguments.</p><h4 id="args-2" tabindex="-1">Args <a class="header-anchor" href="#args-2" aria-label="Permalink to &quot;Args&quot;">​</a></h4><pre><code>shape : (tuple): the shape for the materialized tensor.
</code></pre><ul><li><strong>device</strong> (<code>tensorplay.device</code>): the desired device of the parameters and buffers in this module. Optional.</li><li><strong>dtype</strong> (<code>tensorplay.dtype</code>): the desired floating point type of the floating point parameters and buffers in this module. Optional.</li></ul><hr><h4 id="share-memory-self-source-2" tabindex="-1"><code>share_memory_(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L88" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#share-memory-self-source-2" aria-label="Permalink to &quot;`share_memory_(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L88)&quot;">​</a></h4><hr></details><h2 id="functions" tabindex="-1">Functions <a class="header-anchor" href="#functions" aria-label="Permalink to &quot;Functions&quot;">​</a></h2><h3 id="is-lazy-source" tabindex="-1"><code>is_lazy()</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L102" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#is-lazy-source" aria-label="Permalink to &quot;`is_lazy()` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/parameter.py#L102)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">is_lazy(param: Any) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> bool</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>Returns whether <code>param</code> is an <code>UninitializedParameter</code> or <code>UninitializedBuffer</code>.</p><h4 id="args-3" tabindex="-1">Args <a class="header-anchor" href="#args-3" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>param</strong> (<code>Any</code>): the input to check.</li></ul></div></div></main><footer class="VPDocFooter" data-v-39a288b8 data-v-e257564d><!--[--><!--]--><!----><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-e257564d><span class="visually-hidden" id="doc-footer-aria-label" data-v-e257564d>Pager</span><div class="pager" data-v-e257564d><!----></div><div class="pager" data-v-e257564d><a class="VPLink link pager-link next" href="/zh/api/" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>Next page</span><span class="title" data-v-e257564d>概览</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-5d98c3a5 data-v-e315a0ad><div class="container" data-v-e315a0ad><p class="message" data-v-e315a0ad>基于 Apache 2.0 许可发布。</p><p class="copyright" data-v-e315a0ad>版权所有 © 2025 zlx。保留所有权利。</p></div></footer><!--[--><a href="https://deepwiki.com/bluemoon-o2/TensorPlay" target="_blank" rel="noopener noreferrer" class="deepwiki-floating-badge" title="View on DeepWiki" data-v-5bf2a798><div class="badge-content" data-v-5bf2a798><span class="badge-icon" data-v-5bf2a798>📚</span><span class="badge-text" data-v-5bf2a798>DeepWiki</span></div></a><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"about_index.md\":\"CivpnJLS\",\"api__c.md\":\"C_J0EWHA\",\"api__reduction.md\":\"CcQTGa52\",\"api_activation.md\":\"BGBcfbA_\",\"api_adagrad.md\":\"Duucap4L\",\"api_adam.md\":\"B6F6HfPc\",\"api_adamw.md\":\"BRkR1iuY\",\"api_alexnet.md\":\"DY6a19PR\",\"api_amp.md\":\"C-bU3qHi\",\"api_audio.md\":\"COxDUPkl\",\"api_autocast_mode.md\":\"0iN_nBmE\",\"api_autograd.md\":\"9kpZfXtl\",\"api_backend.md\":\"CXUhZEyb\",\"api_backends.md\":\"B_bG74Gi\",\"api_batchnorm.md\":\"XLqeoBPh\",\"api_collate.md\":\"D4ONEdYq\",\"api_common_types.md\":\"Dh75-e2Y\",\"api_comparison.md\":\"COlCxpxH\",\"api_container.md\":\"DMH3KEiI\",\"api_conv.md\":\"DF0GFaql\",\"api_cpp.md\":\"B9E_vT7t\",\"api_cpu.md\":\"BOjn8BaM\",\"api_cuda.md\":\"v6ikcKwC\",\"api_data.md\":\"B7m_AC2B\",\"api_dataloader.md\":\"C057K852\",\"api_dataset.md\":\"B-DwApTM\",\"api_datasets.md\":\"i2152NFq\",\"api_dropout.md\":\"DzMMlVs2\",\"api_flatten.md\":\"D4zA_qjP\",\"api_folder.md\":\"DaSoqjRC\",\"api_function.md\":\"nCZMqEoD\",\"api_functional.md\":\"DRB1e583\",\"api_grad_mode.md\":\"CwdRNt1O\",\"api_grad_scaler.md\":\"Cx6Iula5\",\"api_hooks.md\":\"BIKb4iGy\",\"api_hub.md\":\"BlxbfAE8\",\"api_index.md\":\"CU1NhOLw\",\"api_init.md\":\"yxiO6Ie6\",\"api_instancenorm.md\":\"CyhqGI7E\",\"api_io.md\":\"D5L0xo2J\",\"api_lazy.md\":\"C1xShkoe\",\"api_linear.md\":\"D0-JMnEM\",\"api_loss.md\":\"BJQVFhNb\",\"api_lr_scheduler.md\":\"qKIcmL6f\",\"api_mkl.md\":\"CGkKnAIU\",\"api_mkldnn.md\":\"DLcmdCYf\",\"api_mnist.md\":\"DvSH5G-2\",\"api_models.md\":\"PxIjWRF1\",\"api_module.md\":\"CnrRone4\",\"api_modules.md\":\"yK73B9y6\",\"api_multiprocessing.md\":\"DrisgWAY\",\"api_nn.md\":\"4XkbCLIj\",\"api_normalization.md\":\"D0JAaa1R\",\"api_onnx.md\":\"maU2_GEX\",\"api_openmp.md\":\"BCYWCTil\",\"api_ops.md\":\"BMv0aklF\",\"api_optim.md\":\"CaxPVV4U\",\"api_optimizer.md\":\"TLuDU7KR\",\"api_parameter.md\":\"BQlUd1Rx\",\"api_pooling.md\":\"Csm8n1fJ\",\"api_primitives.md\":\"h9kmOGWa\",\"api_rmsprop.md\":\"K8IflYqt\",\"api_sampler.md\":\"Docoa_mD\",\"api_serialization.md\":\"Yz3Vj80d\",\"api_sgd.md\":\"CvX_KTaM\",\"api_sparse.md\":\"BREaz85i\",\"api_stax.md\":\"XChG2EZZ\",\"api_tensorplay.md\":\"DmrkfdPn\",\"api_transforms.md\":\"Be1af3uD\",\"api_triton.md\":\"JKz49eZ7\",\"api_types.md\":\"943V7hWH\",\"api_utils.md\":\"DVKjBrp2\",\"api_vision.md\":\"zbCijo_o\",\"api_viz.md\":\"CiZx1vUZ\",\"api_worker.md\":\"Bz9t8sLd\",\"blog_index.md\":\"B2h2By8z\",\"blog_posts_deep-dive-p10-dispatcher.md\":\"Op6Q8VOt\",\"blog_posts_tensorplay-architecture-design.md\":\"C3rZ8MKN\",\"blog_posts_tpx-autograd-decoupling.md\":\"B5BuZeLr\",\"changelog.md\":\"BUIkw7Qb\",\"community_index.md\":\"ClpoEdVU\",\"contributing.md\":\"D8ZfsEom\",\"ecosystem_index.md\":\"gtvnBFE0\",\"guide_architecture.md\":\"6oaLgXdc\",\"guide_getting-started.md\":\"Buqy5xfl\",\"guide_install.md\":\"CMrQwZ6d\",\"guide_quickstart.md\":\"B6_UpAzb\",\"guide_resources.md\":\"BV5U6Os2\",\"guide_tutorials.md\":\"BZfLWtiC\",\"guide_tutorials_cnn-classification.md\":\"gUJGnFt_\",\"guide_tutorials_custom-autograd.md\":\"sQU_lsig\",\"guide_tutorials_linear-regression.md\":\"DgenOgcp\",\"guide_what-is-tensorplay.md\":\"Cr9LJ0YI\",\"index.md\":\"FiW_nFlh\",\"join_index.md\":\"DEJ6DZZ0\",\"privacy.md\":\"DEhyX8TP\",\"zh_about_index.md\":\"C0qMuj3_\",\"zh_api__c.md\":\"CSuKrWkC\",\"zh_api__reduction.md\":\"B3daZSCA\",\"zh_api_activation.md\":\"CZMJ9jbW\",\"zh_api_adagrad.md\":\"DHu6ziwz\",\"zh_api_adam.md\":\"B0FY4EOy\",\"zh_api_adamw.md\":\"CWrIOxtJ\",\"zh_api_alexnet.md\":\"DFqPETqU\",\"zh_api_amp.md\":\"Cp6DUBtV\",\"zh_api_audio.md\":\"DzU_oAU1\",\"zh_api_autocast_mode.md\":\"C5QatGaz\",\"zh_api_autograd.md\":\"C3rUrDnv\",\"zh_api_backend.md\":\"BvUHlOkJ\",\"zh_api_backends.md\":\"D5m5NMmh\",\"zh_api_batchnorm.md\":\"Bo4Pl8ij\",\"zh_api_collate.md\":\"C9yeRKMz\",\"zh_api_common_types.md\":\"51LRPJtH\",\"zh_api_comparison.md\":\"CmoYjd2x\",\"zh_api_container.md\":\"ecpuGL9c\",\"zh_api_conv.md\":\"CCAFGFF2\",\"zh_api_cpp.md\":\"C_Qv-gh-\",\"zh_api_cuda.md\":\"Ch-0DTcR\",\"zh_api_data.md\":\"D8Ewk7N7\",\"zh_api_dataloader.md\":\"CcmiB811\",\"zh_api_dataset.md\":\"D-cegnxU\",\"zh_api_datasets.md\":\"DYHjDpEe\",\"zh_api_dropout.md\":\"B4KevHXm\",\"zh_api_flatten.md\":\"DEAvbacp\",\"zh_api_folder.md\":\"Cgxw6iWX\",\"zh_api_function.md\":\"BJWjNfM0\",\"zh_api_functional.md\":\"DQbKL0xC\",\"zh_api_grad_mode.md\":\"BFeDCfd7\",\"zh_api_grad_scaler.md\":\"DCTY_-th\",\"zh_api_hooks.md\":\"3iIg6lb4\",\"zh_api_hub.md\":\"CEDAtQ6E\",\"zh_api_index.md\":\"CbRR1k25\",\"zh_api_init.md\":\"BqQy98Zn\",\"zh_api_instancenorm.md\":\"LzLgKUnO\",\"zh_api_io.md\":\"D2S8h9kl\",\"zh_api_lazy.md\":\"aFzKlDTe\",\"zh_api_linear.md\":\"CNWJcDWd\",\"zh_api_loss.md\":\"CJx9AuDU\",\"zh_api_lr_scheduler.md\":\"CEmO09Z4\",\"zh_api_mkl.md\":\"CJb3speK\",\"zh_api_mkldnn.md\":\"IkiVLyDS\",\"zh_api_mnist.md\":\"B9ADoHZ5\",\"zh_api_models.md\":\"g3-O97YN\",\"zh_api_module.md\":\"BPDJ-1Kk\",\"zh_api_modules.md\":\"Cgu9RPlU\",\"zh_api_multiprocessing.md\":\"G2hJzSLM\",\"zh_api_nn.md\":\"DT072hnu\",\"zh_api_normalization.md\":\"CMzsTRlT\",\"zh_api_onnx.md\":\"CjhD6BJR\",\"zh_api_openmp.md\":\"D9v_3MHS\",\"zh_api_ops.md\":\"BkisPtY5\",\"zh_api_optim.md\":\"CP5UB3gJ\",\"zh_api_optimizer.md\":\"CPOByaaw\",\"zh_api_parameter.md\":\"XiiS9CGF\",\"zh_api_pooling.md\":\"D950IsK0\",\"zh_api_primitives.md\":\"Dd3Dk8aE\",\"zh_api_rmsprop.md\":\"CPe7ryaO\",\"zh_api_sampler.md\":\"Bfhil5Ns\",\"zh_api_serialization.md\":\"Cmp7efEt\",\"zh_api_sgd.md\":\"C5LAFD4S\",\"zh_api_sparse.md\":\"CARPRVJP\",\"zh_api_stax.md\":\"B38hG-b1\",\"zh_api_tensorplay.md\":\"CPudKJkw\",\"zh_api_transforms.md\":\"CqQu24bY\",\"zh_api_types.md\":\"CWZNrGJb\",\"zh_api_utils.md\":\"B2MXTLh6\",\"zh_api_vision.md\":\"nVKKiAhu\",\"zh_api_viz.md\":\"CgkacQOO\",\"zh_api_worker.md\":\"Ca4fmC1_\",\"zh_blog_index.md\":\"K40rMw4R\",\"zh_blog_posts_deep-dive-p10-dispatcher.md\":\"B9LpN9fE\",\"zh_blog_posts_tensorplay-architecture-design.md\":\"B1115zly\",\"zh_blog_posts_tpx-autograd-decoupling.md\":\"jONX5ja6\",\"zh_changelog.md\":\"DWHyZqpt\",\"zh_community_index.md\":\"eV1HDn5F\",\"zh_contributing.md\":\"Ba70JOpw\",\"zh_ecosystem_index.md\":\"CtS2mywe\",\"zh_guide_architecture.md\":\"C-yDD8tS\",\"zh_guide_getting-started.md\":\"yqgE04Td\",\"zh_guide_install.md\":\"DuL2QCJ_\",\"zh_guide_quickstart.md\":\"CPJMLO2F\",\"zh_guide_resources.md\":\"6P7d8N8C\",\"zh_guide_tutorials.md\":\"B1F8PlnM\",\"zh_guide_tutorials_cnn-classification.md\":\"6bqTLd2c\",\"zh_guide_tutorials_custom-autograd.md\":\"1odlIk1U\",\"zh_guide_tutorials_linear-regression.md\":\"CRwjWFUg\",\"zh_guide_what-is-tensorplay.md\":\"BjbN8xd_\",\"zh_index.md\":\"lfD4EwUL\",\"zh_join_index.md\":\"Conbfq5r\",\"zh_privacy.md\":\"CaMhTqnu\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"TensorPlay\",\"description\":\"A transparent, educational, and PyTorch-compatible deep learning framework.\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"logo\":\"/images/logo-0.png\",\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/bluemoon-o2/tensorplay\"}],\"search\":{\"provider\":\"local\",\"options\":{\"locales\":{\"zh\":{\"translations\":{\"button\":{\"buttonText\":\"搜索文档\",\"buttonAriaLabel\":\"搜索文档\"},\"modal\":{\"noResultsText\":\"无法找到相关结果\",\"resetButtonTitle\":\"清除查询条件\",\"footer\":{\"selectText\":\"选择\",\"navigateText\":\"切换\",\"closeText\":\"关闭\"}}}}}}}},\"locales\":{\"root\":{\"label\":\"English\",\"lang\":\"en\",\"description\":\"A simple deep learning framework designed for educational purposes and small-scale experiments.\",\"themeConfig\":{\"nav\":[{\"text\":\"Learn\",\"items\":[{\"text\":\"Getting Started\",\"link\":\"/guide/getting-started\"},{\"text\":\"Tutorials\",\"link\":\"/guide/tutorials\"}]},{\"text\":\"Community\",\"link\":\"/community/\"},{\"text\":\"Projects\",\"link\":\"/ecosystem/\"},{\"text\":\"Docs\",\"link\":\"/api/\"},{\"text\":\"Blog & News\",\"link\":\"/blog/\"},{\"text\":\"About\",\"link\":\"/about/\"},{\"text\":\"JOIN\",\"link\":\"/join/\"}],\"sidebar\":{\"/guide/\":[{\"text\":\"Introduction\",\"items\":[{\"text\":\"What is TensorPlay?\",\"link\":\"/guide/what-is-tensorplay\"},{\"text\":\"Getting Started\",\"link\":\"/guide/getting-started\"},{\"text\":\"Installation\",\"link\":\"/guide/install\"},{\"text\":\"Quickstart\",\"link\":\"/guide/quickstart\"},{\"text\":\"Architecture\",\"link\":\"/guide/architecture\"}]},{\"text\":\"Guides & Resources\",\"items\":[{\"text\":\"Tutorials\",\"link\":\"/guide/tutorials\",\"items\":[{\"text\":\"Image Classification\",\"link\":\"/guide/tutorials/cnn-classification\"},{\"text\":\"Linear Regression\",\"link\":\"/guide/tutorials/linear-regression\"},{\"text\":\"Custom Autograd\",\"link\":\"/guide/tutorials/custom-autograd\"}]},{\"text\":\"Resources\",\"link\":\"/guide/resources\"},{\"text\":\"API Reference\",\"link\":\"/api/\"}]}],\"/api/\":[{\"text\":\"Core API\",\"items\":[{\"text\":\"Overview\",\"link\":\"/api/\"},{\"text\":\"tensorplay\",\"link\":\"/api/tensorplay\"},{\"text\":\"autograd\",\"link\":\"/api/autograd\"},{\"text\":\"functional\",\"link\":\"/api/functional\"},{\"text\":\"optim\",\"link\":\"/api/optim\"},{\"text\":\"cuda\",\"link\":\"/api/cuda\"}]},{\"text\":\"Neural Networks (nn)\",\"collapsed\":false,\"items\":[{\"text\":\"nn\",\"link\":\"/api/nn\"},{\"text\":\"Modules\",\"link\":\"/api/modules\"},{\"text\":\"Linear Layers\",\"link\":\"/api/linear\"},{\"text\":\"Convolution Layers\",\"link\":\"/api/conv\"},{\"text\":\"Pooling Layers\",\"link\":\"/api/pooling\"},{\"text\":\"Activation Functions\",\"link\":\"/api/activation\"},{\"text\":\"Normalization Layers\",\"link\":\"/api/normalization\"},{\"text\":\"Loss Functions\",\"link\":\"/api/loss\"},{\"text\":\"Dropout Layers\",\"link\":\"/api/dropout\"},{\"text\":\"Container\",\"link\":\"/api/container\"}]}]},\"footer\":{\"message\":\"Released under the Apache 2.0 License.\",\"copyright\":\"Copyright © 2025 zlx. All rights reserved.\"}}},\"zh\":{\"label\":\"简体中文\",\"lang\":\"zh\",\"link\":\"/zh/\",\"description\":\"一个适合学习者、兼容 PyTorch 的深度学习框架。\",\"themeConfig\":{\"nav\":[{\"text\":\"学习\",\"items\":[{\"text\":\"快速开始\",\"link\":\"/zh/guide/getting-started\"},{\"text\":\"教程\",\"link\":\"/zh/guide/tutorials\"}]},{\"text\":\"社区\",\"link\":\"/zh/community/\"},{\"text\":\"项目\",\"link\":\"/zh/ecosystem/\"},{\"text\":\"文档\",\"link\":\"/zh/api/\"},{\"text\":\"博客与新闻\",\"link\":\"/zh/blog/\"},{\"text\":\"关于\",\"link\":\"/zh/about/\"},{\"text\":\"加入我们\",\"link\":\"/zh/join/\"}],\"sidebar\":{\"/zh/guide/\":[{\"text\":\"介绍\",\"items\":[{\"text\":\"什么是 TensorPlay?\",\"link\":\"/zh/guide/what-is-tensorplay\"},{\"text\":\"快速开始\",\"link\":\"/zh/guide/getting-started\"},{\"text\":\"安装\",\"link\":\"/zh/guide/install\"},{\"text\":\"快速入门\",\"link\":\"/zh/guide/quickstart\"},{\"text\":\"架构设计\",\"link\":\"/zh/guide/architecture\"}]},{\"text\":\"指南与资源\",\"items\":[{\"text\":\"教程\",\"link\":\"/zh/guide/tutorials\",\"items\":[{\"text\":\"图像分类\",\"link\":\"/zh/guide/tutorials/cnn-classification\"},{\"text\":\"线性回归\",\"link\":\"/zh/guide/tutorials/linear-regression\"},{\"text\":\"自定义自动微分\",\"link\":\"/zh/guide/tutorials/custom-autograd\"}]},{\"text\":\"资源\",\"link\":\"/zh/guide/resources\"},{\"text\":\"API 参考\",\"link\":\"/zh/api/\"}]}],\"/zh/api/\":[{\"text\":\"核心 API\",\"items\":[{\"text\":\"概览\",\"link\":\"/zh/api/\"},{\"text\":\"tensorplay\",\"link\":\"/zh/api/tensorplay\"},{\"text\":\"autograd\",\"link\":\"/zh/api/autograd\"},{\"text\":\"functional\",\"link\":\"/zh/api/functional\"},{\"text\":\"optim\",\"link\":\"/zh/api/optim\"},{\"text\":\"cuda\",\"link\":\"/zh/api/cuda\"}]},{\"text\":\"神经网络 (nn)\",\"collapsed\":false,\"items\":[{\"text\":\"nn\",\"link\":\"/zh/api/nn\"},{\"text\":\"Modules\",\"link\":\"/zh/api/modules\"},{\"text\":\"Linear Layers\",\"link\":\"/zh/api/linear\"},{\"text\":\"Convolution Layers\",\"link\":\"/zh/api/conv\"},{\"text\":\"Pooling Layers\",\"link\":\"/zh/api/pooling\"},{\"text\":\"Activation Functions\",\"link\":\"/zh/api/activation\"},{\"text\":\"Normalization Layers\",\"link\":\"/zh/api/normalization\"},{\"text\":\"Loss Functions\",\"link\":\"/zh/api/loss\"},{\"text\":\"Dropout Layers\",\"link\":\"/zh/api/dropout\"},{\"text\":\"Container\",\"link\":\"/zh/api/container\"}]}]},\"footer\":{\"message\":\"基于 Apache 2.0 许可发布。\",\"copyright\":\"版权所有 © 2025 zlx。保留所有权利。\"}}}},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>