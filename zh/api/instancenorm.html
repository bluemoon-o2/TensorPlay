<!DOCTYPE html>
<html lang="zh" dir="ltr">
  <head>
    <meta charset="utf-8">
    
    <title>tensorplay.nn.modules.instancenorm | TensorPlay</title>
    <meta name="description" content="一个适合学习者、兼容 PyTorch 的深度学习框架。">
    <meta name="generator" content="VitePress v1.6.4">
    <link rel="preload stylesheet" href="/assets/style.r3WZttRK.css" as="style">
    <link rel="preload stylesheet" href="/vp-icons.css" as="style">
    
    <script type="module" src="/assets/app.DFc_LqUT.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/theme.B96TkyqM.js">
    <link rel="modulepreload" href="/assets/chunks/framework.DJtYh1Hc.js">
    <link rel="modulepreload" href="/assets/zh_api_instancenorm.md.Bx8asyFc.lean.js">
    <link rel="icon" type="image/png" href="/images/logo-0.png">
    <link rel="apple-touch-icon" href="/images/logo-0.png">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <meta name="keywords" content="deep learning, machine learning, ai framework, pytorch compatible, educational, c++, python, tensorplay, 深度学习, 机器学习, 自动微分">
    <meta name="author" content="TensorPlay Team">
    <meta property="og:type" content="website">
    <meta property="og:title" content="TensorPlay | Transparent &amp; Compatible AI Framework">
    <meta property="og:description" content="A transparent, educational, and PyTorch-compatible deep learning framework for the modern AI era.">
    <meta property="og:site_name" content="TensorPlay">
    <meta property="og:url" content="https://www.tensorplay.cn">
    <meta property="og:image" content="https://www.tensorplay.cn/images/logo-1.png">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="TensorPlay | Transparent &amp; Compatible AI Framework">
    <meta name="twitter:description" content="A transparent, educational, and PyTorch-compatible deep learning framework for the modern AI era.">
    <meta name="twitter:image" content="https://www.tensorplay.cn/images/logo-1.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@300;400;500;600;700&amp;display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-5d98c3a5><!--[--><canvas id="cyberpunk-particles" data-v-02d3b8a7></canvas><!----><!--]--><!--[--><span tabindex="-1" data-v-0b0ada53></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-0b0ada53>Skip to content</a><!--]--><!----><header class="VPNav" data-v-5d98c3a5 data-v-ae24b3ad><div class="VPNavBar" data-v-ae24b3ad data-v-6aa21345><div class="wrapper" data-v-6aa21345><div class="container" data-v-6aa21345><div class="title" data-v-6aa21345><div class="VPNavBarTitle has-sidebar" data-v-6aa21345 data-v-1168a8e4><a class="title" href="/zh/" data-v-1168a8e4><!--[--><!--]--><!--[--><img class="VPImage logo" src="/images/logo-0.png" alt data-v-8426fc1a><!--]--><span data-v-1168a8e4>TensorPlay</span><!--[--><!--]--></a></div></div><div class="content" data-v-6aa21345><div class="content-body" data-v-6aa21345><!--[--><!--]--><div class="VPNavBarSearch search" data-v-6aa21345><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="搜索文档"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">搜索文档</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-6aa21345 data-v-dc692963><span id="main-nav-aria-label" class="visually-hidden" data-v-dc692963> Main Navigation </span><!--[--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-dc692963 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-cf11d7a2><span class="text" data-v-cf11d7a2><!----><span data-v-cf11d7a2>学习</span><span class="vpi-chevron-down text-icon" data-v-cf11d7a2></span></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><div class="items" data-v-b98bc113><!--[--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/zh/guide/getting-started" data-v-35975db6><!--[--><span data-v-35975db6>快速开始</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/zh/guide/tutorials" data-v-35975db6><!--[--><span data-v-35975db6>教程</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/community/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>社区</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/ecosystem/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>项目</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/api/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>文档</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/blog/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>博客与新闻</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/about/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>关于</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/join/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>加入我们</span><!--]--></a><!--]--><!--]--></nav><div class="VPFlyout VPNavBarTranslations translations" data-v-6aa21345 data-v-88af2de4 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="Change language" data-v-cf11d7a2><span class="text" data-v-cf11d7a2><span class="vpi-languages option-icon" data-v-cf11d7a2></span><!----><span class="vpi-chevron-down text-icon" data-v-cf11d7a2></span></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><!----><!--[--><!--[--><div class="items" data-v-88af2de4><p class="title" data-v-88af2de4>简体中文</p><!--[--><div class="VPMenuLink" data-v-88af2de4 data-v-35975db6><a class="VPLink link" href="/api/instancenorm" data-v-35975db6><!--[--><span data-v-35975db6>English</span><!--]--></a></div><!--]--></div><!--]--><!--]--></div></div></div><div class="VPNavBarAppearance appearance" data-v-6aa21345 data-v-6c893767><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-6c893767 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-6aa21345 data-v-0394ad82 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/bluemoon-o2/tensorplay" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-6aa21345 data-v-bb2aa2f0 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-cf11d7a2><span class="vpi-more-horizontal icon" data-v-cf11d7a2></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><!----><!--[--><!--[--><div class="group translations" data-v-bb2aa2f0><p class="trans-title" data-v-bb2aa2f0>简体中文</p><!--[--><div class="VPMenuLink" data-v-bb2aa2f0 data-v-35975db6><a class="VPLink link" href="/api/instancenorm" data-v-35975db6><!--[--><span data-v-35975db6>English</span><!--]--></a></div><!--]--></div><div class="group" data-v-bb2aa2f0><div class="item appearance" data-v-bb2aa2f0><p class="label" data-v-bb2aa2f0>Appearance</p><div class="appearance-action" data-v-bb2aa2f0><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-bb2aa2f0 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div></div></div><div class="group" data-v-bb2aa2f0><div class="item social-links" data-v-bb2aa2f0><div class="VPSocialLinks social-links-list" data-v-bb2aa2f0 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/bluemoon-o2/tensorplay" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-6aa21345 data-v-e5dd9c1c><span class="container" data-v-e5dd9c1c><span class="top" data-v-e5dd9c1c></span><span class="middle" data-v-e5dd9c1c></span><span class="bottom" data-v-e5dd9c1c></span></span></button></div></div></div></div><div class="divider" data-v-6aa21345><div class="divider-line" data-v-6aa21345></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-5d98c3a5 data-v-a6f0e41e><div class="container" data-v-a6f0e41e><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-a6f0e41e><span class="vpi-align-left menu-icon" data-v-a6f0e41e></span><span class="menu-text" data-v-a6f0e41e>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-a6f0e41e data-v-8a42e2b4><button data-v-8a42e2b4>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-5d98c3a5 data-v-319d5ca6><div class="curtain" data-v-319d5ca6></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-319d5ca6><span class="visually-hidden" id="sidebar-aria-label" data-v-319d5ca6> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-c40bc020><section class="VPSidebarItem level-0" data-v-c40bc020 data-v-b3fd67f8><div class="item" role="button" tabindex="0" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><h2 class="text" data-v-b3fd67f8>核心 API</h2><!----></div><div class="items" data-v-b3fd67f8><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>概览</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/tensorplay" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>tensorplay</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/autograd" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>autograd</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/functional" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>functional</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/optim" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>optim</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/cuda" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>cuda</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-c40bc020><section class="VPSidebarItem level-0 collapsible" data-v-c40bc020 data-v-b3fd67f8><div class="item" role="button" tabindex="0" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><h2 class="text" data-v-b3fd67f8>神经网络 (nn)</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-b3fd67f8><span class="vpi-chevron-right caret-icon" data-v-b3fd67f8></span></div></div><div class="items" data-v-b3fd67f8><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/nn" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>nn</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/modules" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Modules</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/linear" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Linear Layers</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/conv" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Convolution Layers</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/pooling" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Pooling Layers</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/activation" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Activation Functions</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/normalization" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Normalization Layers</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/loss" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Loss Functions</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/dropout" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Dropout Layers</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/container" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Container</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-5d98c3a5 data-v-1428d186><div class="VPDoc has-sidebar has-aside" data-v-1428d186 data-v-39a288b8><!--[--><!--]--><div class="container" data-v-39a288b8><div class="aside" data-v-39a288b8><div class="aside-curtain" data-v-39a288b8></div><div class="aside-container" data-v-39a288b8><div class="aside-content" data-v-39a288b8><div class="VPDocAside" data-v-39a288b8 data-v-3f215769><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-3f215769 data-v-a5bbad30><div class="content" data-v-a5bbad30><div class="outline-marker" data-v-a5bbad30></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-a5bbad30>On this page</div><ul class="VPDocOutlineItem root" data-v-a5bbad30 data-v-b933a997><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-3f215769></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-39a288b8><div class="content-container" data-v-39a288b8><!--[--><!--[--><!--[--><!----><!--]--><!--]--><!--]--><main class="main" data-v-39a288b8><div style="position:relative;" class="vp-doc _zh_api_instancenorm" data-v-39a288b8><div><div class="warning custom-block"><p class="custom-block-title">WARNING</p><p>该页面尚未翻译。 以下内容为英文原版。</p></div><h1 id="tensorplay-nn-modules-instancenorm" tabindex="-1">tensorplay.nn.modules.instancenorm <a class="header-anchor" href="#tensorplay-nn-modules-instancenorm" aria-label="Permalink to &quot;tensorplay.nn.modules.instancenorm&quot;">​</a></h1><h2 id="classes" tabindex="-1">Classes <a class="header-anchor" href="#classes" aria-label="Permalink to &quot;Classes&quot;">​</a></h2><h3 id="class-instancenorm1d-source" tabindex="-1"><code>class InstanceNorm1d</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/instancenorm.py#L125" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-instancenorm1d-source" aria-label="Permalink to &quot;`class InstanceNorm1d` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/instancenorm.py#L125)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">InstanceNorm1d(num_features: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, eps: </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">float</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1e-05</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, momentum: </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">float</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, affine: </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bool</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, track_running_stats: </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bool</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>_InstanceNorm</code></p><p>Applies Instance Normalization.</p><p>This operation applies Instance Normalization over a 2D (unbatched) or 3D (batched) input as described in the paper <code>Instance Normalization: The Missing Ingredient for Fast Stylization &lt;https://arxiv.org/abs/1607.08022&gt;</code>__.</p><p>.. math</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> \</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.489ex;" xmlns="http://www.w3.org/2000/svg" width="1.229ex" height="1.486ex" role="img" focusable="false" viewBox="0 -441 543 657" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FE" d="M31 249Q11 249 11 258Q11 275 26 304T66 365T129 418T206 441Q233 441 239 440Q287 429 318 386T371 255Q385 195 385 170Q385 166 386 166L398 193Q418 244 443 300T486 391T508 430Q510 431 524 431H537Q543 425 543 422Q543 418 522 378T463 251T391 71Q385 55 378 6T357 -100Q341 -165 330 -190T303 -216Q286 -216 286 -188Q286 -138 340 32L346 51L347 69Q348 79 348 100Q348 257 291 317Q251 355 196 355Q148 355 108 329T51 260Q49 251 47 251Q45 249 31 249Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>γ</mi></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.281ex" height="2.034ex" role="img" focusable="false" viewBox="0 -705 566 899" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>β</mi></math></mjx-assistive-mml></mjx-container> are learnable parameter vectors of size <code>C</code> (where <code>C</code> is the number of features or channels of the input) if <code>affine</code> is <code>True</code>. The variance is calculated via the biased estimator, equivalent to <code>tensorplay.var(input, unbiased=False)</code>.</p><p>By default, this layer uses instance statistics computed from input data in both training and evaluation modes.</p><p>If <code>track_running_stats</code> is set to <code>True</code>, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default <code>momentum</code> of 0.1.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This <code>momentum</code> argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="46.933ex" height="2.398ex" role="img" focusable="false" viewBox="0 -810 20744.2 1060" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(313.8,16) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z" style="stroke-width:3;"></path></g></g></g><g data-mml-node="mtext" transform="translate(605,-150) scale(0.707)"><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" style="stroke-width:3;"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(556,0)" style="stroke-width:3;"></path><path data-c="77" d="M90 368Q84 378 76 380T40 385H18V431H24L43 430Q62 430 84 429T116 428Q206 428 221 431H229V385H215Q177 383 177 368Q177 367 221 239L265 113L339 328L333 345Q323 374 316 379Q308 384 278 385H258V431H264Q270 428 348 428Q439 428 454 431H461V385H452Q404 385 404 369Q404 366 418 324T449 234T481 143L496 100L537 219Q579 341 579 347Q579 363 564 373T530 385H522V431H529Q541 428 624 428Q692 428 698 431H703V385H697Q696 385 691 385T682 384Q635 377 619 334L559 161Q546 124 528 71Q508 12 503 1T487 -11H479Q460 -11 456 -4Q455 -3 407 133L361 267Q359 263 266 -4Q261 -11 243 -11H238Q225 -11 220 -3L90 368Z" transform="translate(1000,0)" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(2150.4,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3206.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(3595.2,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(4317.4,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mtext" transform="translate(5317.6,0)"><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" style="stroke-width:3;"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(833,0)" style="stroke-width:3;"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1333,0)" style="stroke-width:3;"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(2166,0)" style="stroke-width:3;"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(2610,0)" style="stroke-width:3;"></path><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(3166,0)" style="stroke-width:3;"></path><path data-c="75" d="M383 58Q327 -10 256 -10H249Q124 -10 105 89Q104 96 103 226Q102 335 102 348T96 369Q86 385 36 385H25V408Q25 431 27 431L38 432Q48 433 67 434T105 436Q122 437 142 438T172 441T184 442H187V261Q188 77 190 64Q193 49 204 40Q224 26 264 26Q290 26 311 35T343 58T363 90T375 120T379 144Q379 145 379 161T380 201T380 248V315Q380 361 370 372T320 385H302V431Q304 431 378 436T457 442H464V264Q464 84 465 81Q468 61 479 55T524 46H542V0Q540 0 467 -5T390 -11H383V58Z" transform="translate(3555,0)" style="stroke-width:3;"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(4111,0)" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(10261.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(10872.9,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(11873.1,0)"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(313.8,16) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z" style="stroke-width:3;"></path></g></g></g><g data-mml-node="mo" transform="translate(12667.3,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mtext" transform="translate(13667.5,0)"><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" style="stroke-width:3;"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(833,0)" style="stroke-width:3;"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1333,0)" style="stroke-width:3;"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(2166,0)" style="stroke-width:3;"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(2610,0)" style="stroke-width:3;"></path><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(3166,0)" style="stroke-width:3;"></path><path data-c="75" d="M383 58Q327 -10 256 -10H249Q124 -10 105 89Q104 96 103 226Q102 335 102 348T96 369Q86 385 36 385H25V408Q25 431 27 431L38 432Q48 433 67 434T105 436Q122 437 142 438T172 441T184 442H187V261Q188 77 190 64Q193 49 204 40Q224 26 264 26Q290 26 311 35T343 58T363 90T375 120T379 144Q379 145 379 161T380 201T380 248V315Q380 361 370 372T320 385H302V431Q304 431 378 436T457 442H464V264Q464 84 465 81Q468 61 479 55T524 46H542V0Q540 0 467 -5T390 -11H383V58Z" transform="translate(3555,0)" style="stroke-width:3;"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(4111,0)" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(18833.7,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(19834,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" style="stroke-width:3;"></path></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mover><mi>x</mi><mo stretchy="false">^</mo></mover></mrow><mtext>new</mtext></msub><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mtext>momentum</mtext><mo stretchy="false">)</mo><mo>×</mo><mrow data-mjx-texclass="ORD"><mover><mi>x</mi><mo stretchy="false">^</mo></mover></mrow><mo>+</mo><mtext>momentum</mtext><mo>×</mo><msub><mi>x</mi><mi>t</mi></msub></math></mjx-assistive-mml></mjx-container>, where <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.857ex" role="img" focusable="false" viewBox="0 -810 572 821" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(313.8,16) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z" style="stroke-width:3;"></path></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mover><mi>x</mi><mo stretchy="false">^</mo></mover></mrow></math></mjx-assistive-mml></mjx-container> is the estimated statistic and <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.059ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 910.3 599.8" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" style="stroke-width:3;"></path></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mi>t</mi></msub></math></mjx-assistive-mml></mjx-container> is the new observed value.</p></div><div class="info custom-block"><p class="custom-block-title">INFO</p><p><code>InstanceNorm1d</code> and <code>LayerNorm</code> are very similar, but have some subtle differences. <code>InstanceNorm1d</code> is applied on each channel of channeled data like multidimensional time series, but <code>LayerNorm</code> is usually applied on entire sample and often in NLP tasks. Additionally, <code>LayerNorm</code> applies elementwise affine transform, while <code>InstanceNorm1d</code> usually don&#39;t apply affine transform.</p></div><h4 id="args" tabindex="-1">Args <a class="header-anchor" href="#args" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>num_features</strong>: number of features or channels <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.719ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 760 727" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi></math></mjx-assistive-mml></mjx-container> of the input</li><li><strong>eps</strong>: a value added to the denominator for numerical stability. Default: 1e-5</li><li><strong>momentum</strong>: the value used for the running_mean and running_var computation. Default: 0.1</li><li><strong>affine</strong>: a boolean value that when set to <code>True</code>, this module has learnable affine parameters, initialized the same way as done for batch normalization.</li><li><strong>Default</strong>: <code>False</code>.</li><li><strong>track_running_stats</strong>: a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: <code>False</code></li></ul><h4 id="shape" tabindex="-1">Shape <a class="header-anchor" href="#shape" aria-label="Permalink to &quot;Shape&quot;">​</a></h4><ul><li>Input: <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="9.041ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3996.3 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1277,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1721.7,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2481.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2926.3,0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3607.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>N</mi><mo>,</mo><mi>C</mi><mo>,</mo><mi>L</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> or <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="6.026ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2663.7 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1149,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1593.7,0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2274.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>C</mi><mo>,</mo><mi>L</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container></li><li>Output: <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="9.041ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3996.3 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1277,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1721.7,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2481.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2926.3,0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3607.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>N</mi><mo>,</mo><mi>C</mi><mo>,</mo><mi>L</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> or <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="6.026ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2663.7 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1149,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1593.7,0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2274.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>C</mi><mo>,</mo><mi>L</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> (same shape as input)</li></ul><h4 id="examples" tabindex="-1">Examples <a class="header-anchor" href="#examples" aria-label="Permalink to &quot;Examples&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Without Learnable Parameters</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.InstanceNorm1d(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">100</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># With Learnable Parameters</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.InstanceNorm1d(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">100</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">affine</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.randn(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">100</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">40</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">output </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> m(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><details><summary>Methods</summary><h4 id="init-self-num-features-int-eps-float-1e-05-momentum-float-0-1-affine-bool-false-track-running-stats-bool-false-device-none-dtype-none-none-source" tabindex="-1"><code>__init__(self, num_features: int, eps: float = 1e-05, momentum: float = 0.1, affine: bool = False, track_running_stats: bool = False, device=None, dtype=None) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/instancenorm.py#L20" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-num-features-int-eps-float-1e-05-momentum-float-0-1-affine-bool-false-track-running-stats-bool-false-device-none-dtype-none-none-source" aria-label="Permalink to &quot;`__init__(self, num_features: int, eps: float = 1e-05, momentum: float = 0.1, affine: bool = False, track_running_stats: bool = False, device=None, dtype=None) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/instancenorm.py#L20)&quot;">​</a></h4><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p><hr><h4 id="add-module-self-name-str-module-optional-forwardref-module-none-source" tabindex="-1"><code>add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#add-module-self-name-str-module-optional-forwardref-module-none-source" aria-label="Permalink to &quot;`add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667)&quot;">​</a></h4><p>Add a child module to the current module.</p><p>The module can be accessed as an attribute using the given name.</p><h4 id="args-1" tabindex="-1">Args <a class="header-anchor" href="#args-1" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the child module. The child module can be accessed from this module using the given name</li><li><strong>module</strong> (<code>Module</code>): child module to be added to the module.</li></ul><hr><h4 id="apply-self-fn-callable-forwardref-module-nonetype-self-source" tabindex="-1"><code>apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#apply-self-fn-callable-forwardref-module-nonetype-self-source" aria-label="Permalink to &quot;`apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990)&quot;">​</a></h4><p>Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self.</p><p>Typical use includes initializing the parameters of a model (see also <code>nn-init-doc</code>).</p><h4 id="args-2" tabindex="-1">Args <a class="header-anchor" href="#args-2" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>fn</strong> (``Module<code> -&gt; None</code>): function to be applied to each submodule</li></ul><h4 id="returns" tabindex="-1">Returns <a class="header-anchor" href="#returns" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="example" tabindex="-1">Example <a class="header-anchor" href="#example" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@tensorplay.no_grad</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> init_weights</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        m.weight.fill_(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m.weight)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net.apply(init_weights)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><hr><h4 id="bfloat16-self-self-source" tabindex="-1"><code>bfloat16(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#bfloat16-self-self-source" aria-label="Permalink to &quot;`bfloat16(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-1" tabindex="-1">Returns <a class="header-anchor" href="#returns-1" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source" tabindex="-1"><code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source" aria-label="Permalink to &quot;`buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627)&quot;">​</a></h4><p>Return an iterator over module buffers.</p><h4 id="args-3" tabindex="-1">Args <a class="header-anchor" href="#args-3" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.</li></ul><h4 id="yields" tabindex="-1">Yields <a class="header-anchor" href="#yields" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>tensorplay.Tensor: module buffer
</code></pre><h4 id="example-1" tabindex="-1">Example <a class="header-anchor" href="#example-1" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.buffers():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf), buf.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="children-self-collections-abc-iterator-module-source" tabindex="-1"><code>children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#children-self-collections-abc-iterator-module-source" aria-label="Permalink to &quot;`children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681)&quot;">​</a></h4><p>Return an iterator over immediate children modules.</p><h4 id="yields-1" tabindex="-1">Yields <a class="header-anchor" href="#yields-1" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a child module</li></ul><hr><h4 id="compile-self-args-kwargs-source" tabindex="-1"><code>compile(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#compile-self-args-kwargs-source" aria-label="Permalink to &quot;`compile(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944)&quot;">​</a></h4><p>Compile this Module&#39;s forward using <code>tensorplay.compile</code>.</p><p>This Module&#39;s <code>__call__</code> method is compiled and all arguments are passed as-is to <code>tensorplay.compile</code>.</p><p>See <code>tensorplay.compile</code> for details on the arguments for this function.</p><hr><h4 id="cpu-self-self-source" tabindex="-1"><code>cpu(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cpu-self-self-source" aria-label="Permalink to &quot;`cpu(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050)&quot;">​</a></h4><p>Move all model parameters and buffers to the CPU.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-2" tabindex="-1">Returns <a class="header-anchor" href="#returns-2" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source" tabindex="-1"><code>cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source" aria-label="Permalink to &quot;`cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031)&quot;">​</a></h4><p>Move all model parameters and buffers to the GPU.</p><p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on GPU while being optimized.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-4" tabindex="-1">Args <a class="header-anchor" href="#args-4" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>int, optional</code>): if specified, all parameters will be copied to that device</li></ul><h4 id="returns-3" tabindex="-1">Returns <a class="header-anchor" href="#returns-3" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="double-self-self-source" tabindex="-1"><code>double(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#double-self-self-source" aria-label="Permalink to &quot;`double(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-4" tabindex="-1">Returns <a class="header-anchor" href="#returns-4" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="eval-self-self-source" tabindex="-1"><code>eval(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#eval-self-self-source" aria-label="Permalink to &quot;`eval(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808)&quot;">​</a></h4><p>Set the module in evaluation mode.</p><p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e. whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, etc.</p><p>This is equivalent with <code>self.train(False) &lt;tensorplay.nn.Module.train&gt;</code>.</p><p>See <code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.</p><h4 id="returns-5" tabindex="-1">Returns <a class="header-anchor" href="#returns-5" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="extra-repr-self-source" tabindex="-1"><code>extra_repr(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L97" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#extra-repr-self-source" aria-label="Permalink to &quot;`extra_repr(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L97)&quot;">​</a></h4><p>Return the extra representation of the module.</p><p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p><hr><h4 id="float-self-self-source" tabindex="-1"><code>float(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#float-self-self-source" aria-label="Permalink to &quot;`float(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-6" tabindex="-1">Returns <a class="header-anchor" href="#returns-6" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="forward-self-input-tensorplay-c-tensorbase-tensorplay-c-tensorbase-source" tabindex="-1"><code>forward(self, input: tensorplay._C.TensorBase) -&gt; tensorplay._C.TensorBase</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/instancenorm.py#L102" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#forward-self-input-tensorplay-c-tensorbase-tensorplay-c-tensorbase-source" aria-label="Permalink to &quot;`forward(self, input: tensorplay._C.TensorBase) -&gt; tensorplay._C.TensorBase` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/instancenorm.py#L102)&quot;">​</a></h4><p>Define the computation performed at every call.</p><p>Should be overridden by all subclasses.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code> instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.</p></div><hr><h4 id="get-buffer-self-target-str-tensor-source" tabindex="-1"><code>get_buffer(self, target: str) -&gt; &#39;Tensor&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-buffer-self-target-str-tensor-source" aria-label="Permalink to &quot;`get_buffer(self, target: str) -&gt; &#39;Tensor&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881)&quot;">​</a></h4><p>Return the buffer given by <code>target</code> if it exists, otherwise throw an error.</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-5" tabindex="-1">Args <a class="header-anchor" href="#args-5" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the buffer to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-7" tabindex="-1">Returns <a class="header-anchor" href="#returns-7" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.Tensor: The buffer referenced by ``target``
</code></pre><h4 id="raises" tabindex="-1">Raises <a class="header-anchor" href="#raises" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not a buffer</li></ul><hr><h4 id="get-extra-state-self-any-source" tabindex="-1"><code>get_extra_state(self) -&gt; Any</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-extra-state-self-any-source" aria-label="Permalink to &quot;`get_extra_state(self) -&gt; Any` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917)&quot;">​</a></h4><p>Return any extra state to include in the module&#39;s state_dict.</p><p>Implement this and a corresponding <code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module&#39;s <code>state_dict()</code>.</p><p>Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.</p><h4 id="returns-8" tabindex="-1">Returns <a class="header-anchor" href="#returns-8" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>object</strong>: Any extra state to store in the module&#39;s state_dict</li></ul><hr><h4 id="get-parameter-self-target-str-parameter-source" tabindex="-1"><code>get_parameter(self, target: str) -&gt; &#39;Parameter&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-parameter-self-target-str-parameter-source" aria-label="Permalink to &quot;`get_parameter(self, target: str) -&gt; &#39;Parameter&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845)&quot;">​</a></h4><p>Return the parameter given by <code>target</code> if it exists, otherwise throw an error.</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-6" tabindex="-1">Args <a class="header-anchor" href="#args-6" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the Parameter to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-9" tabindex="-1">Returns <a class="header-anchor" href="#returns-9" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Parameter: The Parameter referenced by ``target``
</code></pre><h4 id="raises-1" tabindex="-1">Raises <a class="header-anchor" href="#raises-1" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not an <code>nn.Parameter</code></li></ul><hr><h4 id="get-submodule-self-target-str-module-source" tabindex="-1"><code>get_submodule(self, target: str) -&gt; &#39;Module&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-submodule-self-target-str-module-source" aria-label="Permalink to &quot;`get_submodule(self, target: str) -&gt; &#39;Module&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699)&quot;">​</a></h4><p>Return the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> which has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(&quot;net_b.linear&quot;)</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(&quot;net_b.net_c.conv&quot;)</code>.</p><p>The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.</p><h4 id="args-7" tabindex="-1">Args <a class="header-anchor" href="#args-7" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li></ul><h4 id="returns-10" tabindex="-1">Returns <a class="header-anchor" href="#returns-10" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Module: The submodule referenced by ``target``
</code></pre><h4 id="raises-2" tabindex="-1">Raises <a class="header-anchor" href="#raises-2" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If at any point along the path resulting from the target string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="half-self-self-source" tabindex="-1"><code>half(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#half-self-self-source" aria-label="Permalink to &quot;`half(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-11" tabindex="-1">Returns <a class="header-anchor" href="#returns-11" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source" tabindex="-1"><code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source" aria-label="Permalink to &quot;`load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436)&quot;">​</a></h4><p>Copy parameters and buffers from <code>state_dict</code> into this module and its descendants.</p><p>If <code>strict</code> is <code>True</code>, then the keys of <code>state_dict</code> must exactly match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function.</p><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>If <code>assign</code> is <code>True</code> the optimizer must be created after the call to <code>load_state_dict</code> unless <code>~tensorplay.__future__.get_swap_module_params_on_conversion</code> is <code>True</code>.</p></div><h4 id="args-8" tabindex="-1">Args <a class="header-anchor" href="#args-8" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state_dict</strong> (<code>dict</code>): a dict containing parameters and persistent buffers.</li><li><strong>strict</strong> (<code>bool, optional</code>): whether to strictly enforce that the keys in <code>state_dict</code> match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function. Default: <code>True</code></li><li><strong>assign</strong> (<code>bool, optional</code>): When set to <code>False</code>, the properties of the tensors in the current module are preserved whereas setting it to <code>True</code> preserves properties of the Tensors in the state dict. The only exception is the <code>requires_grad</code> field of <code>~tensorplay.nn.Parameter</code> for which the value from the module is preserved. Default: <code>False</code></li></ul><h4 id="returns-12" tabindex="-1">Returns <a class="header-anchor" href="#returns-12" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:
    * ``missing_keys`` is a list of str containing any keys that are expected
        by this module but missing from the provided ``state_dict``.
    * ``unexpected_keys`` is a list of str containing the keys that are not
        expected by this module but present in the provided ``state_dict``.
</code></pre><h4 id="note" tabindex="-1">Note <a class="header-anchor" href="#note" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>If a parameter or buffer is registered as ``None`` and its corresponding key
exists in `state_dict`, `load_state_dict` will raise a
``RuntimeError``.
</code></pre><hr><h4 id="modules-self-collections-abc-iterator-module-source" tabindex="-1"><code>modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#modules-self-collections-abc-iterator-module-source" aria-label="Permalink to &quot;`modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710)&quot;">​</a></h4><p>Return an iterator over all modules in the network.</p><h4 id="yields-2" tabindex="-1">Yields <a class="header-anchor" href="#yields-2" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a module in the network</li></ul><h4 id="note-1" tabindex="-1">Note <a class="header-anchor" href="#note-1" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-2" tabindex="-1">Example <a class="header-anchor" href="#example-2" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source" tabindex="-1"><code>named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source" aria-label="Permalink to &quot;`named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650)&quot;">​</a></h4><p>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p><h4 id="args-9" tabindex="-1">Args <a class="header-anchor" href="#args-9" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all buffer names.</li><li><strong>recurse</strong> (<code>bool, optional</code>): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Defaults to True.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated buffers in the result. Defaults to True.</li></ul><h4 id="yields-3" tabindex="-1">Yields <a class="header-anchor" href="#yields-3" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, tensorplay.Tensor): Tuple containing the name and buffer
</code></pre><h4 id="example-3" tabindex="-1">Example <a class="header-anchor" href="#example-3" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_buffers():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_var&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-children-self-collections-abc-iterator-tuple-str-module-source" tabindex="-1"><code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-children-self-collections-abc-iterator-tuple-str-module-source" aria-label="Permalink to &quot;`named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690)&quot;">​</a></h4><p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p><h4 id="yields-4" tabindex="-1">Yields <a class="header-anchor" href="#yields-4" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple containing a name and child module
</code></pre><h4 id="example-4" tabindex="-1">Example <a class="header-anchor" href="#example-4" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, module </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.named_children():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv4&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv5&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(module)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source" tabindex="-1"><code>named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source" aria-label="Permalink to &quot;`named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737)&quot;">​</a></h4><p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p><h4 id="args-10" tabindex="-1">Args <a class="header-anchor" href="#args-10" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>memo</strong>: a memo to store the set of modules already added to the result</li><li><strong>prefix</strong>: a prefix that will be added to the name of the module</li><li><strong>remove_duplicate</strong>: whether to remove the duplicated module instances in the result or not</li></ul><h4 id="yields-5" tabindex="-1">Yields <a class="header-anchor" href="#yields-5" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple of name and module
</code></pre><h4 id="note-2" tabindex="-1">Note <a class="header-anchor" href="#note-2" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-5" tabindex="-1">Example <a class="header-anchor" href="#example-5" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.named_modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;0&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source" tabindex="-1"><code>named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source" aria-label="Permalink to &quot;`named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595)&quot;">​</a></h4><p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p><h4 id="args-11" tabindex="-1">Args <a class="header-anchor" href="#args-11" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all parameter names.</li><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated parameters in the result. Defaults to True.</li></ul><h4 id="yields-6" tabindex="-1">Yields <a class="header-anchor" href="#yields-6" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Parameter): Tuple containing the name and parameter
</code></pre><h4 id="example-6" tabindex="-1">Example <a class="header-anchor" href="#example-6" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_parameters():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source" tabindex="-1"><code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source" aria-label="Permalink to &quot;`parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570)&quot;">​</a></h4><p>Return an iterator over module parameters.</p><p>This is typically passed to an optimizer.</p><h4 id="args-12" tabindex="-1">Args <a class="header-anchor" href="#args-12" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</li></ul><h4 id="yields-7" tabindex="-1">Yields <a class="header-anchor" href="#yields-7" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Parameter</strong>: module parameter</li></ul><h4 id="example-7" tabindex="-1">Example <a class="header-anchor" href="#example-7" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.parameters():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param), param.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source" tabindex="-1"><code>register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source" aria-label="Permalink to &quot;`register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>This function is deprecated in favor of <code>~tensorplay.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.</p><h4 id="returns-13" tabindex="-1">Returns <a class="header-anchor" href="#returns-13" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source" tabindex="-1"><code>register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source" aria-label="Permalink to &quot;`register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553)&quot;">​</a></h4><p>Add a buffer to the module.</p><p>This is typically used to register a buffer that should not be considered a model parameter. For example, BatchNorm&#39;s <code>running_mean</code> is not a parameter, but is part of the module&#39;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting <code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&#39;s <code>state_dict</code>.</p><p>Buffers can be accessed as attributes using given names.</p><h4 id="args-13" tabindex="-1">Args <a class="header-anchor" href="#args-13" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the buffer. The buffer can be accessed from this module using the given name</li><li><strong>tensor</strong> (<code>Tensor or None</code>): buffer to be registered. If <code>None</code>, then operations that run on buffers, such as <code>cuda</code>, are ignored. If <code>None</code>, the buffer is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li><li><strong>persistent</strong> (<code>bool</code>): whether the buffer is part of this module&#39;s <code>state_dict</code>.</li></ul><h4 id="example-8" tabindex="-1">Example <a class="header-anchor" href="#example-8" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.register_buffer(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_mean&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, tensorplay.zeros(num_features))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><hr><h4 id="register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source" tabindex="-1"><code>register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source" aria-label="Permalink to &quot;`register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592)&quot;">​</a></h4><p>Register a forward hook on the module.</p><p>The hook will be called every time after <code>forward</code> has computed an output.</p><p>If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after <code>forward</code> is called. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-14" tabindex="-1">Args <a class="header-anchor" href="#args-14" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If <code>True</code>, the provided <code>hook</code> will be fired before all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward</code> hooks registered with <code>register_module_forward_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If <code>True</code>, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>always_call</strong> (<code>bool</code>): If <code>True</code> the <code>hook</code> will be run regardless of whether an exception is raised while calling the Module.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-14" tabindex="-1">Returns <a class="header-anchor" href="#returns-14" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source" tabindex="-1"><code>register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source" aria-label="Permalink to &quot;`register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526)&quot;">​</a></h4><p>Register a forward pre-hook on the module.</p><p>The hook will be called every time before <code>forward</code> is invoked.</p><p>If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> of modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> and</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> kwargs</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-15" tabindex="-1">Args <a class="header-anchor" href="#args-15" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward_pre</code> hooks registered with <code>register_module_forward_pre_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If true, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-15" tabindex="-1">Returns <a class="header-anchor" href="#returns-15" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source" tabindex="-1"><code>register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source" aria-label="Permalink to &quot;`register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>The hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:</p><pre><code>1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.
2. If none of the module inputs require gradients, the hook will fire when the gradients are computed
   with respect to module outputs.
3. If none of the module outputs require gradients, then the hooks will not fire.
</code></pre><p>The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_input, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Tensor) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_input</code> and <code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of <code>grad_input</code> in subsequent computations. <code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in <code>grad_input</code> and <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> outputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-16" tabindex="-1">Args <a class="header-anchor" href="#args-16" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward</code> hooks registered with <code>register_module_full_backward_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-16" tabindex="-1">Returns <a class="header-anchor" href="#returns-16" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source" tabindex="-1"><code>register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source" aria-label="Permalink to &quot;`register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287)&quot;">​</a></h4><p>Register a backward pre-hook on the module.</p><p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tuple[Tensor] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of <code>grad_output</code> in subsequent computations. Entries in <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-17" tabindex="-1">Args <a class="header-anchor" href="#args-17" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward_pre</code> hooks registered with <code>register_module_full_backward_pre_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-17" tabindex="-1">Returns <a class="header-anchor" href="#returns-17" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-post-hook-self-hook-source" tabindex="-1"><code>register_load_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-post-hook-self-hook-source" aria-label="Permalink to &quot;`register_load_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226)&quot;">​</a></h4><p>Register a post-hook to be run after module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, incompatible_keys) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.</p><p>The given incompatible_keys can be modified inplace if needed.</p><p>Note that the checks performed when calling <code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.</p><h4 id="returns-18" tabindex="-1">Returns <a class="header-anchor" href="#returns-18" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-pre-hook-self-hook-source" tabindex="-1"><code>register_load_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-pre-hook-self-hook-source" aria-label="Permalink to &quot;`register_load_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214)&quot;">​</a></h4><p>Register a pre-hook to be run before module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # noqa: B950</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="arguments" tabindex="-1">Arguments <a class="header-anchor" href="#arguments" aria-label="Permalink to &quot;Arguments&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): Callable hook that will be invoked before loading the state dict.</li></ul><hr><h4 id="register-module-self-name-str-module-optional-forwardref-module-none-source" tabindex="-1"><code>register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-module-self-name-str-module-optional-forwardref-module-none-source" aria-label="Permalink to &quot;`register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695)&quot;">​</a></h4><p>Alias for <code>add_module</code>.</p><hr><h4 id="register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source" tabindex="-1"><code>register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source" aria-label="Permalink to &quot;`register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617)&quot;">​</a></h4><p>Add a parameter to the module.</p><p>The parameter can be accessed as an attribute using given name.</p><h4 id="args-18" tabindex="-1">Args <a class="header-anchor" href="#args-18" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the parameter. The parameter can be accessed from this module using the given name</li><li><strong>param</strong> (<code>Parameter or None</code>): parameter to be added to the module. If <code>None</code>, then operations that run on parameters, such as <code>cuda</code>, are ignored. If <code>None</code>, the parameter is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li></ul><hr><h4 id="register-state-dict-post-hook-self-hook-source" tabindex="-1"><code>register_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-post-hook-self-hook-source" aria-label="Permalink to &quot;`register_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017)&quot;">​</a></h4><p>Register a post-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can modify the <code>state_dict</code> inplace.</p><hr><h4 id="register-state-dict-pre-hook-self-hook-source" tabindex="-1"><code>register_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-pre-hook-self-hook-source" aria-label="Permalink to &quot;`register_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041)&quot;">​</a></h4><p>Register a pre-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, prefix, keep_vars) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p><hr><h4 id="requires-grad-self-requires-grad-bool-true-self-source" tabindex="-1"><code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#requires-grad-self-requires-grad-bool-true-self-source" aria-label="Permalink to &quot;`requires_grad_(self, requires_grad: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826)&quot;">​</a></h4><p>Change if autograd should record operations on parameters in this module.</p><p>This method sets the parameters&#39; <code>requires_grad</code> attributes in-place.</p><p>This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).</p><p>See <code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.</p><h4 id="args-19" tabindex="-1">Args <a class="header-anchor" href="#args-19" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>requires_grad</strong> (<code>bool</code>): whether autograd should record operations on parameters in this module. Default: <code>True</code>.</li></ul><h4 id="returns-19" tabindex="-1">Returns <a class="header-anchor" href="#returns-19" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="reset-parameters-self-none-source" tabindex="-1"><code>reset_parameters(self) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L88" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#reset-parameters-self-none-source" aria-label="Permalink to &quot;`reset_parameters(self) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L88)&quot;">​</a></h4><hr><h4 id="reset-running-stats-self-none-source" tabindex="-1"><code>reset_running_stats(self) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L80" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#reset-running-stats-self-none-source" aria-label="Permalink to &quot;`reset_running_stats(self) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L80)&quot;">​</a></h4><hr><h4 id="set-extra-state-self-state-any-none-source" tabindex="-1"><code>set_extra_state(self, state: Any) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-extra-state-self-state-any-none-source" aria-label="Permalink to &quot;`set_extra_state(self, state: Any) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938)&quot;">​</a></h4><p>Set extra state contained in the loaded <code>state_dict</code>.</p><p>This function is called from <code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding <code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.</p><h4 id="args-20" tabindex="-1">Args <a class="header-anchor" href="#args-20" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state</strong> (<code>dict</code>): Extra state from the <code>state_dict</code></li></ul><hr><h4 id="set-submodule-self-target-str-module-module-strict-bool-false-none-source" tabindex="-1"><code>set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-submodule-self-target-str-module-module-strict-bool-false-none-source" aria-label="Permalink to &quot;`set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764)&quot;">​</a></h4><p>Set the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>If <code>strict</code> is set to <code>False</code> (default), the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>strict</code> is set to <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule does not exist.</p></div><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(3, 3, 3) ) (linear): Linear(3, 3) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To override the <code>Conv2d</code> with a new submodule <code>Linear</code>, you could call <code>set_submodule(&quot;net_b.net_c.conv&quot;, nn.Linear(1, 1))</code> where <code>strict</code> could be <code>True</code> or <code>False</code></p><p>To add a new submodule <code>Conv2d</code> to the existing <code>net_b</code> module, you would call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1))</code>.</p><p>In the above if you set <code>strict=True</code> and call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1), strict=True)</code>, an AttributeError will be raised because <code>net_b</code> does not have a submodule named <code>conv</code>.</p><h4 id="args-21" tabindex="-1">Args <a class="header-anchor" href="#args-21" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li><li><strong>module</strong>: The module to set the submodule to.</li><li><strong>strict</strong>: If <code>False</code>, the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule doesn&#39;t already exist.</li></ul><h4 id="raises-3" tabindex="-1">Raises <a class="header-anchor" href="#raises-3" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>ValueError</strong>: If the <code>target</code> string is empty or if <code>module</code> is not an instance of <code>nn.Module</code>.</li><li><strong>AttributeError</strong>: If at any point along the path resulting from the <code>target</code> string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="share-memory-self-self-source" tabindex="-1"><code>share_memory(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#share-memory-self-self-source" aria-label="Permalink to &quot;`share_memory(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877)&quot;">​</a></h4><p>See <code>tensorplay.Tensor.share_memory_</code>.</p><hr><h4 id="state-dict-self-args-destination-none-prefix-keep-vars-false-source" tabindex="-1"><code>state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#state-dict-self-args-destination-none-prefix-keep-vars-false-source" aria-label="Permalink to &quot;`state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105)&quot;">​</a></h4><p>Return a dictionary containing references to the whole state of the module.</p><p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>The returned object is a shallow copy. It contains references to the module&#39;s parameters and buffers.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Currently <code>state_dict()</code> also accepts positional arguments for <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However, this is being deprecated and keyword arguments will be enforced in future releases.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Please avoid the use of argument <code>destination</code> as it is not designed for end-users.</p></div><h4 id="args-22" tabindex="-1">Args <a class="header-anchor" href="#args-22" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>destination</strong> (<code>dict, optional</code>): If provided, the state of module will be updated into the dict and the same object is returned. Otherwise, an <code>OrderedDict</code> will be created and returned.</li><li><strong>Default</strong>: <code>None</code>.</li><li><strong>prefix</strong> (<code>str, optional</code>): a prefix added to parameter and buffer names to compose the keys in state_dict. Default: <code>&#39;&#39;</code>.</li><li><strong>keep_vars</strong> (<code>bool, optional</code>): by default the <code>~tensorplay.Tensor</code> s returned in the state dict are detached from autograd. If it&#39;s set to <code>True</code>, detaching will not be performed.</li><li><strong>Default</strong>: <code>False</code>.</li></ul><h4 id="returns-20" tabindex="-1">Returns <a class="header-anchor" href="#returns-20" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>dict</strong>: a dictionary containing a whole state of the module</li></ul><h4 id="example-9" tabindex="-1">Example <a class="header-anchor" href="#example-9" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">module.state_dict().keys()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;weight&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><hr><h4 id="to-self-args-kwargs-source" tabindex="-1"><code>to(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-self-args-kwargs-source" aria-label="Permalink to &quot;`to(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151)&quot;">​</a></h4><p>Move and/or cast the parameters and buffers.</p><p>This can be called as</p><p>.. function:: to(device=None, dtype=None, non_blocking=False) :noindex:</p><p>.. function:: to(dtype, non_blocking=False) :noindex:</p><p>.. function:: to(tensor, non_blocking=False) :noindex:</p><p>.. function:: to(memory_format=tensorplay.channels_last) :noindex:</p><p>Its signature is similar to <code>tensorplay.Tensor.to</code>, but only accepts floating point or complex <code>dtype</code>\ s. In addition, this method will only cast the floating point or complex parameters and buffers to <code>dtype</code> (if given). The integral parameters and buffers will be moved <code>device</code>, if that is given, but with dtypes unchanged. When <code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.</p><p>See below for examples.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-23" tabindex="-1">Args <a class="header-anchor" href="#args-23" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): the desired device of the parameters and buffers in this module</li><li><strong>dtype</strong> (<code>tensorplay.dtype</code>): the desired floating point or complex dtype of the parameters and buffers in this module</li><li><strong>tensor</strong> (<code>tensorplay.Tensor</code>): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module</li><li><strong>memory_format</strong> (<code>tensorplay.memory_format</code>): the desired memory format for 4D parameters and buffers in this module (keyword only argument)</li></ul><h4 id="returns-21" tabindex="-1">Returns <a class="header-anchor" href="#returns-21" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="examples-1" tabindex="-1">Examples <a class="header-anchor" href="#examples-1" aria-label="Permalink to &quot;Examples&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +IGNORE_WANT(&quot;non-deterministic&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(tensorplay.double)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float64)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +REQUIRES(env:TENSORPLAY_DOCTEST_CUDA1)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">gpu1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cuda:1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(gpu1, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.half, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">non_blocking</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;cuda:1&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cpu </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cpu&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(cpu)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).to(tensorplay.cdouble)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3741</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,  </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2382</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5593</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.4443</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear(tensorplay.ones(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.cdouble))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br></div></div><hr><h4 id="to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source" tabindex="-1"><code>to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source" aria-label="Permalink to &quot;`to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119)&quot;">​</a></h4><p>Move the parameters and buffers to the specified device without copying storage.</p><h4 id="args-24" tabindex="-1">Args <a class="header-anchor" href="#args-24" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): The desired device of the parameters and buffers in this module.</li><li><strong>recurse</strong> (<code>bool</code>): Whether parameters and buffers of submodules should be recursively moved to the specified device.</li></ul><h4 id="returns-22" tabindex="-1">Returns <a class="header-anchor" href="#returns-22" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="train-self-mode-bool-true-self-source" tabindex="-1"><code>train(self, mode: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#train-self-mode-bool-true-self-source" aria-label="Permalink to &quot;`train(self, mode: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786)&quot;">​</a></h4><p>Set the module in training mode.</p><p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e., whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, etc.</p><h4 id="args-25" tabindex="-1">Args <a class="header-anchor" href="#args-25" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>mode</strong> (<code>bool</code>): whether to set training mode (<code>True</code>) or evaluation mode (<code>False</code>). Default: <code>True</code>.</li></ul><h4 id="returns-23" tabindex="-1">Returns <a class="header-anchor" href="#returns-23" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="type-self-dst-type-union-tensorplay-dtype-str-self-source" tabindex="-1"><code>type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#type-self-dst-type-union-tensorplay-dtype-str-self-source" aria-label="Permalink to &quot;`type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061)&quot;">​</a></h4><p>Casts all parameters and buffers to <code>dst_type</code>.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-26" tabindex="-1">Args <a class="header-anchor" href="#args-26" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>dst_type</strong> (<code>type or string</code>): the desired type</li></ul><h4 id="returns-24" tabindex="-1">Returns <a class="header-anchor" href="#returns-24" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="zero-grad-self-set-to-none-bool-true-none-source" tabindex="-1"><code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#zero-grad-self-set-to-none-bool-true-none-source" aria-label="Permalink to &quot;`zero_grad(self, set_to_none: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849)&quot;">​</a></h4><p>Reset gradients of all model parameters.</p><p>See similar function under <code>tensorplay.optim.Optimizer</code> for more context.</p><h4 id="args-27" tabindex="-1">Args <a class="header-anchor" href="#args-27" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>set_to_none</strong> (<code>bool</code>): instead of setting to zero, set the grads to None. See <code>tensorplay.optim.Optimizer.zero_grad</code> for details.</li></ul><hr></details><h3 id="class-instancenorm2d-source" tabindex="-1"><code>class InstanceNorm2d</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/instancenorm.py#L239" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-instancenorm2d-source" aria-label="Permalink to &quot;`class InstanceNorm2d` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/instancenorm.py#L239)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">InstanceNorm2d(num_features: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, eps: </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">float</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1e-05</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, momentum: </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">float</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, affine: </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bool</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, track_running_stats: </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bool</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>_InstanceNorm</code></p><p>Applies Instance Normalization.</p><p>This operation applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper <code>Instance Normalization: The Missing Ingredient for Fast Stylization &lt;https://arxiv.org/abs/1607.08022&gt;</code>__.</p><p>.. math</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> \</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.489ex;" xmlns="http://www.w3.org/2000/svg" width="1.229ex" height="1.486ex" role="img" focusable="false" viewBox="0 -441 543 657" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FE" d="M31 249Q11 249 11 258Q11 275 26 304T66 365T129 418T206 441Q233 441 239 440Q287 429 318 386T371 255Q385 195 385 170Q385 166 386 166L398 193Q418 244 443 300T486 391T508 430Q510 431 524 431H537Q543 425 543 422Q543 418 522 378T463 251T391 71Q385 55 378 6T357 -100Q341 -165 330 -190T303 -216Q286 -216 286 -188Q286 -138 340 32L346 51L347 69Q348 79 348 100Q348 257 291 317Q251 355 196 355Q148 355 108 329T51 260Q49 251 47 251Q45 249 31 249Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>γ</mi></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.281ex" height="2.034ex" role="img" focusable="false" viewBox="0 -705 566 899" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>β</mi></math></mjx-assistive-mml></mjx-container> are learnable parameter vectors of size <code>C</code> (where <code>C</code> is the input size) if <code>affine</code> is <code>True</code>. The standard-deviation is calculated via the biased estimator, equivalent to <code>tensorplay.var(input, unbiased=False)</code>.</p><p>By default, this layer uses instance statistics computed from input data in both training and evaluation modes.</p><p>If <code>track_running_stats</code> is set to <code>True</code>, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default <code>momentum</code> of 0.1.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This <code>momentum</code> argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="46.933ex" height="2.398ex" role="img" focusable="false" viewBox="0 -810 20744.2 1060" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(313.8,16) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z" style="stroke-width:3;"></path></g></g></g><g data-mml-node="mtext" transform="translate(605,-150) scale(0.707)"><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" style="stroke-width:3;"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(556,0)" style="stroke-width:3;"></path><path data-c="77" d="M90 368Q84 378 76 380T40 385H18V431H24L43 430Q62 430 84 429T116 428Q206 428 221 431H229V385H215Q177 383 177 368Q177 367 221 239L265 113L339 328L333 345Q323 374 316 379Q308 384 278 385H258V431H264Q270 428 348 428Q439 428 454 431H461V385H452Q404 385 404 369Q404 366 418 324T449 234T481 143L496 100L537 219Q579 341 579 347Q579 363 564 373T530 385H522V431H529Q541 428 624 428Q692 428 698 431H703V385H697Q696 385 691 385T682 384Q635 377 619 334L559 161Q546 124 528 71Q508 12 503 1T487 -11H479Q460 -11 456 -4Q455 -3 407 133L361 267Q359 263 266 -4Q261 -11 243 -11H238Q225 -11 220 -3L90 368Z" transform="translate(1000,0)" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(2150.4,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3206.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(3595.2,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(4317.4,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mtext" transform="translate(5317.6,0)"><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" style="stroke-width:3;"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(833,0)" style="stroke-width:3;"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1333,0)" style="stroke-width:3;"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(2166,0)" style="stroke-width:3;"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(2610,0)" style="stroke-width:3;"></path><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(3166,0)" style="stroke-width:3;"></path><path data-c="75" d="M383 58Q327 -10 256 -10H249Q124 -10 105 89Q104 96 103 226Q102 335 102 348T96 369Q86 385 36 385H25V408Q25 431 27 431L38 432Q48 433 67 434T105 436Q122 437 142 438T172 441T184 442H187V261Q188 77 190 64Q193 49 204 40Q224 26 264 26Q290 26 311 35T343 58T363 90T375 120T379 144Q379 145 379 161T380 201T380 248V315Q380 361 370 372T320 385H302V431Q304 431 378 436T457 442H464V264Q464 84 465 81Q468 61 479 55T524 46H542V0Q540 0 467 -5T390 -11H383V58Z" transform="translate(3555,0)" style="stroke-width:3;"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(4111,0)" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(10261.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(10872.9,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(11873.1,0)"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(313.8,16) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z" style="stroke-width:3;"></path></g></g></g><g data-mml-node="mo" transform="translate(12667.3,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mtext" transform="translate(13667.5,0)"><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" style="stroke-width:3;"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(833,0)" style="stroke-width:3;"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1333,0)" style="stroke-width:3;"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(2166,0)" style="stroke-width:3;"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(2610,0)" style="stroke-width:3;"></path><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(3166,0)" style="stroke-width:3;"></path><path data-c="75" d="M383 58Q327 -10 256 -10H249Q124 -10 105 89Q104 96 103 226Q102 335 102 348T96 369Q86 385 36 385H25V408Q25 431 27 431L38 432Q48 433 67 434T105 436Q122 437 142 438T172 441T184 442H187V261Q188 77 190 64Q193 49 204 40Q224 26 264 26Q290 26 311 35T343 58T363 90T375 120T379 144Q379 145 379 161T380 201T380 248V315Q380 361 370 372T320 385H302V431Q304 431 378 436T457 442H464V264Q464 84 465 81Q468 61 479 55T524 46H542V0Q540 0 467 -5T390 -11H383V58Z" transform="translate(3555,0)" style="stroke-width:3;"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(4111,0)" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(18833.7,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(19834,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" style="stroke-width:3;"></path></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mover><mi>x</mi><mo stretchy="false">^</mo></mover></mrow><mtext>new</mtext></msub><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mtext>momentum</mtext><mo stretchy="false">)</mo><mo>×</mo><mrow data-mjx-texclass="ORD"><mover><mi>x</mi><mo stretchy="false">^</mo></mover></mrow><mo>+</mo><mtext>momentum</mtext><mo>×</mo><msub><mi>x</mi><mi>t</mi></msub></math></mjx-assistive-mml></mjx-container>, where <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.857ex" role="img" focusable="false" viewBox="0 -810 572 821" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(313.8,16) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z" style="stroke-width:3;"></path></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mover><mi>x</mi><mo stretchy="false">^</mo></mover></mrow></math></mjx-assistive-mml></mjx-container> is the estimated statistic and <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.059ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 910.3 599.8" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" style="stroke-width:3;"></path></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mi>t</mi></msub></math></mjx-assistive-mml></mjx-container> is the new observed value.</p></div><div class="info custom-block"><p class="custom-block-title">INFO</p><p><code>InstanceNorm2d</code> and <code>LayerNorm</code> are very similar, but have some subtle differences. <code>InstanceNorm2d</code> is applied on each channel of channeled data like RGB images, but <code>LayerNorm</code> is usually applied on entire sample and often in NLP tasks. Additionally, <code>LayerNorm</code> applies elementwise affine transform, while <code>InstanceNorm2d</code> usually don&#39;t apply affine transform.</p></div><h4 id="args-28" tabindex="-1">Args <a class="header-anchor" href="#args-28" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>num_features</strong>: <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.719ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 760 727" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi></math></mjx-assistive-mml></mjx-container> from an expected input of size <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="12.887ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 5696 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1277,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1721.7,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2481.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2926.3,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3814.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(4259,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5307,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>N</mi><mo>,</mo><mi>C</mi><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> or <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="9.872ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4363.3 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1149,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1593.7,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2481.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2926.3,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3974.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>C</mi><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container></li><li><strong>eps</strong>: a value added to the denominator for numerical stability. Default: 1e-5</li><li><strong>momentum</strong>: the value used for the running_mean and running_var computation. Default: 0.1</li><li><strong>affine</strong>: a boolean value that when set to <code>True</code>, this module has learnable affine parameters, initialized the same way as done for batch normalization.</li><li><strong>Default</strong>: <code>False</code>.</li><li><strong>track_running_stats</strong>: a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: <code>False</code></li></ul><h4 id="shape-1" tabindex="-1">Shape <a class="header-anchor" href="#shape-1" aria-label="Permalink to &quot;Shape&quot;">​</a></h4><ul><li>Input: <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="12.887ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 5696 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1277,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1721.7,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2481.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2926.3,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3814.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(4259,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5307,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>N</mi><mo>,</mo><mi>C</mi><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> or <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="9.872ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4363.3 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1149,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1593.7,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2481.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2926.3,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3974.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>C</mi><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container></li><li>Output: <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="12.887ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 5696 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1277,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1721.7,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2481.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2926.3,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3814.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(4259,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5307,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>N</mi><mo>,</mo><mi>C</mi><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> or <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="9.872ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4363.3 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1149,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1593.7,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2481.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2926.3,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3974.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>C</mi><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> (same shape as input)</li></ul><h4 id="examples-2" tabindex="-1">Examples <a class="header-anchor" href="#examples-2" aria-label="Permalink to &quot;Examples&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Without Learnable Parameters</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.InstanceNorm2d(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">100</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># With Learnable Parameters</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.InstanceNorm2d(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">100</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">affine</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.randn(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">100</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">35</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">45</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">output </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> m(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><details><summary>Methods</summary><h4 id="init-self-num-features-int-eps-float-1e-05-momentum-float-0-1-affine-bool-false-track-running-stats-bool-false-device-none-dtype-none-none-source-1" tabindex="-1"><code>__init__(self, num_features: int, eps: float = 1e-05, momentum: float = 0.1, affine: bool = False, track_running_stats: bool = False, device=None, dtype=None) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/instancenorm.py#L20" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-num-features-int-eps-float-1e-05-momentum-float-0-1-affine-bool-false-track-running-stats-bool-false-device-none-dtype-none-none-source-1" aria-label="Permalink to &quot;`__init__(self, num_features: int, eps: float = 1e-05, momentum: float = 0.1, affine: bool = False, track_running_stats: bool = False, device=None, dtype=None) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/instancenorm.py#L20)&quot;">​</a></h4><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p><hr><h4 id="add-module-self-name-str-module-optional-forwardref-module-none-source-1" tabindex="-1"><code>add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#add-module-self-name-str-module-optional-forwardref-module-none-source-1" aria-label="Permalink to &quot;`add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667)&quot;">​</a></h4><p>Add a child module to the current module.</p><p>The module can be accessed as an attribute using the given name.</p><h4 id="args-29" tabindex="-1">Args <a class="header-anchor" href="#args-29" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the child module. The child module can be accessed from this module using the given name</li><li><strong>module</strong> (<code>Module</code>): child module to be added to the module.</li></ul><hr><h4 id="apply-self-fn-callable-forwardref-module-nonetype-self-source-1" tabindex="-1"><code>apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#apply-self-fn-callable-forwardref-module-nonetype-self-source-1" aria-label="Permalink to &quot;`apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990)&quot;">​</a></h4><p>Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self.</p><p>Typical use includes initializing the parameters of a model (see also <code>nn-init-doc</code>).</p><h4 id="args-30" tabindex="-1">Args <a class="header-anchor" href="#args-30" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>fn</strong> (``Module<code> -&gt; None</code>): function to be applied to each submodule</li></ul><h4 id="returns-25" tabindex="-1">Returns <a class="header-anchor" href="#returns-25" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="example-10" tabindex="-1">Example <a class="header-anchor" href="#example-10" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@tensorplay.no_grad</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> init_weights</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        m.weight.fill_(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m.weight)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net.apply(init_weights)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><hr><h4 id="bfloat16-self-self-source-1" tabindex="-1"><code>bfloat16(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#bfloat16-self-self-source-1" aria-label="Permalink to &quot;`bfloat16(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-26" tabindex="-1">Returns <a class="header-anchor" href="#returns-26" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source-1" tabindex="-1"><code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source-1" aria-label="Permalink to &quot;`buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627)&quot;">​</a></h4><p>Return an iterator over module buffers.</p><h4 id="args-31" tabindex="-1">Args <a class="header-anchor" href="#args-31" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.</li></ul><h4 id="yields-8" tabindex="-1">Yields <a class="header-anchor" href="#yields-8" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>tensorplay.Tensor: module buffer
</code></pre><h4 id="example-11" tabindex="-1">Example <a class="header-anchor" href="#example-11" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.buffers():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf), buf.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="children-self-collections-abc-iterator-module-source-1" tabindex="-1"><code>children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#children-self-collections-abc-iterator-module-source-1" aria-label="Permalink to &quot;`children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681)&quot;">​</a></h4><p>Return an iterator over immediate children modules.</p><h4 id="yields-9" tabindex="-1">Yields <a class="header-anchor" href="#yields-9" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a child module</li></ul><hr><h4 id="compile-self-args-kwargs-source-1" tabindex="-1"><code>compile(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#compile-self-args-kwargs-source-1" aria-label="Permalink to &quot;`compile(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944)&quot;">​</a></h4><p>Compile this Module&#39;s forward using <code>tensorplay.compile</code>.</p><p>This Module&#39;s <code>__call__</code> method is compiled and all arguments are passed as-is to <code>tensorplay.compile</code>.</p><p>See <code>tensorplay.compile</code> for details on the arguments for this function.</p><hr><h4 id="cpu-self-self-source-1" tabindex="-1"><code>cpu(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cpu-self-self-source-1" aria-label="Permalink to &quot;`cpu(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050)&quot;">​</a></h4><p>Move all model parameters and buffers to the CPU.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-27" tabindex="-1">Returns <a class="header-anchor" href="#returns-27" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source-1" tabindex="-1"><code>cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source-1" aria-label="Permalink to &quot;`cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031)&quot;">​</a></h4><p>Move all model parameters and buffers to the GPU.</p><p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on GPU while being optimized.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-32" tabindex="-1">Args <a class="header-anchor" href="#args-32" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>int, optional</code>): if specified, all parameters will be copied to that device</li></ul><h4 id="returns-28" tabindex="-1">Returns <a class="header-anchor" href="#returns-28" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="double-self-self-source-1" tabindex="-1"><code>double(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#double-self-self-source-1" aria-label="Permalink to &quot;`double(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-29" tabindex="-1">Returns <a class="header-anchor" href="#returns-29" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="eval-self-self-source-1" tabindex="-1"><code>eval(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#eval-self-self-source-1" aria-label="Permalink to &quot;`eval(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808)&quot;">​</a></h4><p>Set the module in evaluation mode.</p><p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e. whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, etc.</p><p>This is equivalent with <code>self.train(False) &lt;tensorplay.nn.Module.train&gt;</code>.</p><p>See <code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.</p><h4 id="returns-30" tabindex="-1">Returns <a class="header-anchor" href="#returns-30" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="extra-repr-self-source-1" tabindex="-1"><code>extra_repr(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L97" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#extra-repr-self-source-1" aria-label="Permalink to &quot;`extra_repr(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L97)&quot;">​</a></h4><p>Return the extra representation of the module.</p><p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p><hr><h4 id="float-self-self-source-1" tabindex="-1"><code>float(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#float-self-self-source-1" aria-label="Permalink to &quot;`float(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-31" tabindex="-1">Returns <a class="header-anchor" href="#returns-31" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="forward-self-input-tensorplay-c-tensorbase-tensorplay-c-tensorbase-source-1" tabindex="-1"><code>forward(self, input: tensorplay._C.TensorBase) -&gt; tensorplay._C.TensorBase</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/instancenorm.py#L102" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#forward-self-input-tensorplay-c-tensorbase-tensorplay-c-tensorbase-source-1" aria-label="Permalink to &quot;`forward(self, input: tensorplay._C.TensorBase) -&gt; tensorplay._C.TensorBase` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/instancenorm.py#L102)&quot;">​</a></h4><p>Define the computation performed at every call.</p><p>Should be overridden by all subclasses.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code> instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.</p></div><hr><h4 id="get-buffer-self-target-str-tensor-source-1" tabindex="-1"><code>get_buffer(self, target: str) -&gt; &#39;Tensor&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-buffer-self-target-str-tensor-source-1" aria-label="Permalink to &quot;`get_buffer(self, target: str) -&gt; &#39;Tensor&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881)&quot;">​</a></h4><p>Return the buffer given by <code>target</code> if it exists, otherwise throw an error.</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-33" tabindex="-1">Args <a class="header-anchor" href="#args-33" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the buffer to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-32" tabindex="-1">Returns <a class="header-anchor" href="#returns-32" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.Tensor: The buffer referenced by ``target``
</code></pre><h4 id="raises-4" tabindex="-1">Raises <a class="header-anchor" href="#raises-4" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not a buffer</li></ul><hr><h4 id="get-extra-state-self-any-source-1" tabindex="-1"><code>get_extra_state(self) -&gt; Any</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-extra-state-self-any-source-1" aria-label="Permalink to &quot;`get_extra_state(self) -&gt; Any` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917)&quot;">​</a></h4><p>Return any extra state to include in the module&#39;s state_dict.</p><p>Implement this and a corresponding <code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module&#39;s <code>state_dict()</code>.</p><p>Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.</p><h4 id="returns-33" tabindex="-1">Returns <a class="header-anchor" href="#returns-33" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>object</strong>: Any extra state to store in the module&#39;s state_dict</li></ul><hr><h4 id="get-parameter-self-target-str-parameter-source-1" tabindex="-1"><code>get_parameter(self, target: str) -&gt; &#39;Parameter&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-parameter-self-target-str-parameter-source-1" aria-label="Permalink to &quot;`get_parameter(self, target: str) -&gt; &#39;Parameter&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845)&quot;">​</a></h4><p>Return the parameter given by <code>target</code> if it exists, otherwise throw an error.</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-34" tabindex="-1">Args <a class="header-anchor" href="#args-34" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the Parameter to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-34" tabindex="-1">Returns <a class="header-anchor" href="#returns-34" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Parameter: The Parameter referenced by ``target``
</code></pre><h4 id="raises-5" tabindex="-1">Raises <a class="header-anchor" href="#raises-5" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not an <code>nn.Parameter</code></li></ul><hr><h4 id="get-submodule-self-target-str-module-source-1" tabindex="-1"><code>get_submodule(self, target: str) -&gt; &#39;Module&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-submodule-self-target-str-module-source-1" aria-label="Permalink to &quot;`get_submodule(self, target: str) -&gt; &#39;Module&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699)&quot;">​</a></h4><p>Return the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> which has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(&quot;net_b.linear&quot;)</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(&quot;net_b.net_c.conv&quot;)</code>.</p><p>The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.</p><h4 id="args-35" tabindex="-1">Args <a class="header-anchor" href="#args-35" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li></ul><h4 id="returns-35" tabindex="-1">Returns <a class="header-anchor" href="#returns-35" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Module: The submodule referenced by ``target``
</code></pre><h4 id="raises-6" tabindex="-1">Raises <a class="header-anchor" href="#raises-6" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If at any point along the path resulting from the target string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="half-self-self-source-1" tabindex="-1"><code>half(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#half-self-self-source-1" aria-label="Permalink to &quot;`half(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-36" tabindex="-1">Returns <a class="header-anchor" href="#returns-36" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source-1" tabindex="-1"><code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source-1" aria-label="Permalink to &quot;`load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436)&quot;">​</a></h4><p>Copy parameters and buffers from <code>state_dict</code> into this module and its descendants.</p><p>If <code>strict</code> is <code>True</code>, then the keys of <code>state_dict</code> must exactly match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function.</p><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>If <code>assign</code> is <code>True</code> the optimizer must be created after the call to <code>load_state_dict</code> unless <code>~tensorplay.__future__.get_swap_module_params_on_conversion</code> is <code>True</code>.</p></div><h4 id="args-36" tabindex="-1">Args <a class="header-anchor" href="#args-36" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state_dict</strong> (<code>dict</code>): a dict containing parameters and persistent buffers.</li><li><strong>strict</strong> (<code>bool, optional</code>): whether to strictly enforce that the keys in <code>state_dict</code> match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function. Default: <code>True</code></li><li><strong>assign</strong> (<code>bool, optional</code>): When set to <code>False</code>, the properties of the tensors in the current module are preserved whereas setting it to <code>True</code> preserves properties of the Tensors in the state dict. The only exception is the <code>requires_grad</code> field of <code>~tensorplay.nn.Parameter</code> for which the value from the module is preserved. Default: <code>False</code></li></ul><h4 id="returns-37" tabindex="-1">Returns <a class="header-anchor" href="#returns-37" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:
    * ``missing_keys`` is a list of str containing any keys that are expected
        by this module but missing from the provided ``state_dict``.
    * ``unexpected_keys`` is a list of str containing the keys that are not
        expected by this module but present in the provided ``state_dict``.
</code></pre><h4 id="note-3" tabindex="-1">Note <a class="header-anchor" href="#note-3" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>If a parameter or buffer is registered as ``None`` and its corresponding key
exists in `state_dict`, `load_state_dict` will raise a
``RuntimeError``.
</code></pre><hr><h4 id="modules-self-collections-abc-iterator-module-source-1" tabindex="-1"><code>modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#modules-self-collections-abc-iterator-module-source-1" aria-label="Permalink to &quot;`modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710)&quot;">​</a></h4><p>Return an iterator over all modules in the network.</p><h4 id="yields-10" tabindex="-1">Yields <a class="header-anchor" href="#yields-10" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a module in the network</li></ul><h4 id="note-4" tabindex="-1">Note <a class="header-anchor" href="#note-4" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-12" tabindex="-1">Example <a class="header-anchor" href="#example-12" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source-1" tabindex="-1"><code>named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source-1" aria-label="Permalink to &quot;`named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650)&quot;">​</a></h4><p>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p><h4 id="args-37" tabindex="-1">Args <a class="header-anchor" href="#args-37" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all buffer names.</li><li><strong>recurse</strong> (<code>bool, optional</code>): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Defaults to True.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated buffers in the result. Defaults to True.</li></ul><h4 id="yields-11" tabindex="-1">Yields <a class="header-anchor" href="#yields-11" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, tensorplay.Tensor): Tuple containing the name and buffer
</code></pre><h4 id="example-13" tabindex="-1">Example <a class="header-anchor" href="#example-13" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_buffers():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_var&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-children-self-collections-abc-iterator-tuple-str-module-source-1" tabindex="-1"><code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-children-self-collections-abc-iterator-tuple-str-module-source-1" aria-label="Permalink to &quot;`named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690)&quot;">​</a></h4><p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p><h4 id="yields-12" tabindex="-1">Yields <a class="header-anchor" href="#yields-12" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple containing a name and child module
</code></pre><h4 id="example-14" tabindex="-1">Example <a class="header-anchor" href="#example-14" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, module </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.named_children():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv4&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv5&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(module)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source-1" tabindex="-1"><code>named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source-1" aria-label="Permalink to &quot;`named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737)&quot;">​</a></h4><p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p><h4 id="args-38" tabindex="-1">Args <a class="header-anchor" href="#args-38" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>memo</strong>: a memo to store the set of modules already added to the result</li><li><strong>prefix</strong>: a prefix that will be added to the name of the module</li><li><strong>remove_duplicate</strong>: whether to remove the duplicated module instances in the result or not</li></ul><h4 id="yields-13" tabindex="-1">Yields <a class="header-anchor" href="#yields-13" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple of name and module
</code></pre><h4 id="note-5" tabindex="-1">Note <a class="header-anchor" href="#note-5" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-15" tabindex="-1">Example <a class="header-anchor" href="#example-15" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.named_modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;0&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source-1" tabindex="-1"><code>named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source-1" aria-label="Permalink to &quot;`named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595)&quot;">​</a></h4><p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p><h4 id="args-39" tabindex="-1">Args <a class="header-anchor" href="#args-39" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all parameter names.</li><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated parameters in the result. Defaults to True.</li></ul><h4 id="yields-14" tabindex="-1">Yields <a class="header-anchor" href="#yields-14" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Parameter): Tuple containing the name and parameter
</code></pre><h4 id="example-16" tabindex="-1">Example <a class="header-anchor" href="#example-16" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_parameters():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source-1" tabindex="-1"><code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source-1" aria-label="Permalink to &quot;`parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570)&quot;">​</a></h4><p>Return an iterator over module parameters.</p><p>This is typically passed to an optimizer.</p><h4 id="args-40" tabindex="-1">Args <a class="header-anchor" href="#args-40" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</li></ul><h4 id="yields-15" tabindex="-1">Yields <a class="header-anchor" href="#yields-15" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Parameter</strong>: module parameter</li></ul><h4 id="example-17" tabindex="-1">Example <a class="header-anchor" href="#example-17" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.parameters():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param), param.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source-1" tabindex="-1"><code>register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source-1" aria-label="Permalink to &quot;`register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>This function is deprecated in favor of <code>~tensorplay.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.</p><h4 id="returns-38" tabindex="-1">Returns <a class="header-anchor" href="#returns-38" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source-1" tabindex="-1"><code>register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source-1" aria-label="Permalink to &quot;`register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553)&quot;">​</a></h4><p>Add a buffer to the module.</p><p>This is typically used to register a buffer that should not be considered a model parameter. For example, BatchNorm&#39;s <code>running_mean</code> is not a parameter, but is part of the module&#39;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting <code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&#39;s <code>state_dict</code>.</p><p>Buffers can be accessed as attributes using given names.</p><h4 id="args-41" tabindex="-1">Args <a class="header-anchor" href="#args-41" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the buffer. The buffer can be accessed from this module using the given name</li><li><strong>tensor</strong> (<code>Tensor or None</code>): buffer to be registered. If <code>None</code>, then operations that run on buffers, such as <code>cuda</code>, are ignored. If <code>None</code>, the buffer is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li><li><strong>persistent</strong> (<code>bool</code>): whether the buffer is part of this module&#39;s <code>state_dict</code>.</li></ul><h4 id="example-18" tabindex="-1">Example <a class="header-anchor" href="#example-18" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.register_buffer(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_mean&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, tensorplay.zeros(num_features))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><hr><h4 id="register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source-1" tabindex="-1"><code>register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source-1" aria-label="Permalink to &quot;`register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592)&quot;">​</a></h4><p>Register a forward hook on the module.</p><p>The hook will be called every time after <code>forward</code> has computed an output.</p><p>If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after <code>forward</code> is called. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-42" tabindex="-1">Args <a class="header-anchor" href="#args-42" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If <code>True</code>, the provided <code>hook</code> will be fired before all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward</code> hooks registered with <code>register_module_forward_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If <code>True</code>, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>always_call</strong> (<code>bool</code>): If <code>True</code> the <code>hook</code> will be run regardless of whether an exception is raised while calling the Module.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-39" tabindex="-1">Returns <a class="header-anchor" href="#returns-39" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source-1" tabindex="-1"><code>register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source-1" aria-label="Permalink to &quot;`register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526)&quot;">​</a></h4><p>Register a forward pre-hook on the module.</p><p>The hook will be called every time before <code>forward</code> is invoked.</p><p>If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> of modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> and</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> kwargs</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-43" tabindex="-1">Args <a class="header-anchor" href="#args-43" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward_pre</code> hooks registered with <code>register_module_forward_pre_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If true, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-40" tabindex="-1">Returns <a class="header-anchor" href="#returns-40" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-1" tabindex="-1"><code>register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-1" aria-label="Permalink to &quot;`register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>The hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:</p><pre><code>1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.
2. If none of the module inputs require gradients, the hook will fire when the gradients are computed
   with respect to module outputs.
3. If none of the module outputs require gradients, then the hooks will not fire.
</code></pre><p>The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_input, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Tensor) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_input</code> and <code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of <code>grad_input</code> in subsequent computations. <code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in <code>grad_input</code> and <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> outputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-44" tabindex="-1">Args <a class="header-anchor" href="#args-44" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward</code> hooks registered with <code>register_module_full_backward_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-41" tabindex="-1">Returns <a class="header-anchor" href="#returns-41" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-1" tabindex="-1"><code>register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-1" aria-label="Permalink to &quot;`register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287)&quot;">​</a></h4><p>Register a backward pre-hook on the module.</p><p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tuple[Tensor] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of <code>grad_output</code> in subsequent computations. Entries in <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-45" tabindex="-1">Args <a class="header-anchor" href="#args-45" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward_pre</code> hooks registered with <code>register_module_full_backward_pre_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-42" tabindex="-1">Returns <a class="header-anchor" href="#returns-42" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-post-hook-self-hook-source-1" tabindex="-1"><code>register_load_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-post-hook-self-hook-source-1" aria-label="Permalink to &quot;`register_load_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226)&quot;">​</a></h4><p>Register a post-hook to be run after module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, incompatible_keys) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.</p><p>The given incompatible_keys can be modified inplace if needed.</p><p>Note that the checks performed when calling <code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.</p><h4 id="returns-43" tabindex="-1">Returns <a class="header-anchor" href="#returns-43" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-pre-hook-self-hook-source-1" tabindex="-1"><code>register_load_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-pre-hook-self-hook-source-1" aria-label="Permalink to &quot;`register_load_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214)&quot;">​</a></h4><p>Register a pre-hook to be run before module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # noqa: B950</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="arguments-1" tabindex="-1">Arguments <a class="header-anchor" href="#arguments-1" aria-label="Permalink to &quot;Arguments&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): Callable hook that will be invoked before loading the state dict.</li></ul><hr><h4 id="register-module-self-name-str-module-optional-forwardref-module-none-source-1" tabindex="-1"><code>register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-module-self-name-str-module-optional-forwardref-module-none-source-1" aria-label="Permalink to &quot;`register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695)&quot;">​</a></h4><p>Alias for <code>add_module</code>.</p><hr><h4 id="register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source-1" tabindex="-1"><code>register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source-1" aria-label="Permalink to &quot;`register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617)&quot;">​</a></h4><p>Add a parameter to the module.</p><p>The parameter can be accessed as an attribute using given name.</p><h4 id="args-46" tabindex="-1">Args <a class="header-anchor" href="#args-46" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the parameter. The parameter can be accessed from this module using the given name</li><li><strong>param</strong> (<code>Parameter or None</code>): parameter to be added to the module. If <code>None</code>, then operations that run on parameters, such as <code>cuda</code>, are ignored. If <code>None</code>, the parameter is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li></ul><hr><h4 id="register-state-dict-post-hook-self-hook-source-1" tabindex="-1"><code>register_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-post-hook-self-hook-source-1" aria-label="Permalink to &quot;`register_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017)&quot;">​</a></h4><p>Register a post-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can modify the <code>state_dict</code> inplace.</p><hr><h4 id="register-state-dict-pre-hook-self-hook-source-1" tabindex="-1"><code>register_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-pre-hook-self-hook-source-1" aria-label="Permalink to &quot;`register_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041)&quot;">​</a></h4><p>Register a pre-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, prefix, keep_vars) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p><hr><h4 id="requires-grad-self-requires-grad-bool-true-self-source-1" tabindex="-1"><code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#requires-grad-self-requires-grad-bool-true-self-source-1" aria-label="Permalink to &quot;`requires_grad_(self, requires_grad: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826)&quot;">​</a></h4><p>Change if autograd should record operations on parameters in this module.</p><p>This method sets the parameters&#39; <code>requires_grad</code> attributes in-place.</p><p>This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).</p><p>See <code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.</p><h4 id="args-47" tabindex="-1">Args <a class="header-anchor" href="#args-47" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>requires_grad</strong> (<code>bool</code>): whether autograd should record operations on parameters in this module. Default: <code>True</code>.</li></ul><h4 id="returns-44" tabindex="-1">Returns <a class="header-anchor" href="#returns-44" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="reset-parameters-self-none-source-1" tabindex="-1"><code>reset_parameters(self) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L88" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#reset-parameters-self-none-source-1" aria-label="Permalink to &quot;`reset_parameters(self) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L88)&quot;">​</a></h4><hr><h4 id="reset-running-stats-self-none-source-1" tabindex="-1"><code>reset_running_stats(self) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L80" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#reset-running-stats-self-none-source-1" aria-label="Permalink to &quot;`reset_running_stats(self) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L80)&quot;">​</a></h4><hr><h4 id="set-extra-state-self-state-any-none-source-1" tabindex="-1"><code>set_extra_state(self, state: Any) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-extra-state-self-state-any-none-source-1" aria-label="Permalink to &quot;`set_extra_state(self, state: Any) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938)&quot;">​</a></h4><p>Set extra state contained in the loaded <code>state_dict</code>.</p><p>This function is called from <code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding <code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.</p><h4 id="args-48" tabindex="-1">Args <a class="header-anchor" href="#args-48" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state</strong> (<code>dict</code>): Extra state from the <code>state_dict</code></li></ul><hr><h4 id="set-submodule-self-target-str-module-module-strict-bool-false-none-source-1" tabindex="-1"><code>set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-submodule-self-target-str-module-module-strict-bool-false-none-source-1" aria-label="Permalink to &quot;`set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764)&quot;">​</a></h4><p>Set the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>If <code>strict</code> is set to <code>False</code> (default), the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>strict</code> is set to <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule does not exist.</p></div><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(3, 3, 3) ) (linear): Linear(3, 3) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To override the <code>Conv2d</code> with a new submodule <code>Linear</code>, you could call <code>set_submodule(&quot;net_b.net_c.conv&quot;, nn.Linear(1, 1))</code> where <code>strict</code> could be <code>True</code> or <code>False</code></p><p>To add a new submodule <code>Conv2d</code> to the existing <code>net_b</code> module, you would call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1))</code>.</p><p>In the above if you set <code>strict=True</code> and call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1), strict=True)</code>, an AttributeError will be raised because <code>net_b</code> does not have a submodule named <code>conv</code>.</p><h4 id="args-49" tabindex="-1">Args <a class="header-anchor" href="#args-49" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li><li><strong>module</strong>: The module to set the submodule to.</li><li><strong>strict</strong>: If <code>False</code>, the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule doesn&#39;t already exist.</li></ul><h4 id="raises-7" tabindex="-1">Raises <a class="header-anchor" href="#raises-7" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>ValueError</strong>: If the <code>target</code> string is empty or if <code>module</code> is not an instance of <code>nn.Module</code>.</li><li><strong>AttributeError</strong>: If at any point along the path resulting from the <code>target</code> string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="share-memory-self-self-source-1" tabindex="-1"><code>share_memory(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#share-memory-self-self-source-1" aria-label="Permalink to &quot;`share_memory(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877)&quot;">​</a></h4><p>See <code>tensorplay.Tensor.share_memory_</code>.</p><hr><h4 id="state-dict-self-args-destination-none-prefix-keep-vars-false-source-1" tabindex="-1"><code>state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#state-dict-self-args-destination-none-prefix-keep-vars-false-source-1" aria-label="Permalink to &quot;`state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105)&quot;">​</a></h4><p>Return a dictionary containing references to the whole state of the module.</p><p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>The returned object is a shallow copy. It contains references to the module&#39;s parameters and buffers.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Currently <code>state_dict()</code> also accepts positional arguments for <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However, this is being deprecated and keyword arguments will be enforced in future releases.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Please avoid the use of argument <code>destination</code> as it is not designed for end-users.</p></div><h4 id="args-50" tabindex="-1">Args <a class="header-anchor" href="#args-50" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>destination</strong> (<code>dict, optional</code>): If provided, the state of module will be updated into the dict and the same object is returned. Otherwise, an <code>OrderedDict</code> will be created and returned.</li><li><strong>Default</strong>: <code>None</code>.</li><li><strong>prefix</strong> (<code>str, optional</code>): a prefix added to parameter and buffer names to compose the keys in state_dict. Default: <code>&#39;&#39;</code>.</li><li><strong>keep_vars</strong> (<code>bool, optional</code>): by default the <code>~tensorplay.Tensor</code> s returned in the state dict are detached from autograd. If it&#39;s set to <code>True</code>, detaching will not be performed.</li><li><strong>Default</strong>: <code>False</code>.</li></ul><h4 id="returns-45" tabindex="-1">Returns <a class="header-anchor" href="#returns-45" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>dict</strong>: a dictionary containing a whole state of the module</li></ul><h4 id="example-19" tabindex="-1">Example <a class="header-anchor" href="#example-19" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">module.state_dict().keys()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;weight&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><hr><h4 id="to-self-args-kwargs-source-1" tabindex="-1"><code>to(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-self-args-kwargs-source-1" aria-label="Permalink to &quot;`to(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151)&quot;">​</a></h4><p>Move and/or cast the parameters and buffers.</p><p>This can be called as</p><p>.. function:: to(device=None, dtype=None, non_blocking=False) :noindex:</p><p>.. function:: to(dtype, non_blocking=False) :noindex:</p><p>.. function:: to(tensor, non_blocking=False) :noindex:</p><p>.. function:: to(memory_format=tensorplay.channels_last) :noindex:</p><p>Its signature is similar to <code>tensorplay.Tensor.to</code>, but only accepts floating point or complex <code>dtype</code>\ s. In addition, this method will only cast the floating point or complex parameters and buffers to <code>dtype</code> (if given). The integral parameters and buffers will be moved <code>device</code>, if that is given, but with dtypes unchanged. When <code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.</p><p>See below for examples.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-51" tabindex="-1">Args <a class="header-anchor" href="#args-51" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): the desired device of the parameters and buffers in this module</li><li><strong>dtype</strong> (<code>tensorplay.dtype</code>): the desired floating point or complex dtype of the parameters and buffers in this module</li><li><strong>tensor</strong> (<code>tensorplay.Tensor</code>): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module</li><li><strong>memory_format</strong> (<code>tensorplay.memory_format</code>): the desired memory format for 4D parameters and buffers in this module (keyword only argument)</li></ul><h4 id="returns-46" tabindex="-1">Returns <a class="header-anchor" href="#returns-46" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="examples-3" tabindex="-1">Examples <a class="header-anchor" href="#examples-3" aria-label="Permalink to &quot;Examples&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +IGNORE_WANT(&quot;non-deterministic&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(tensorplay.double)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float64)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +REQUIRES(env:TENSORPLAY_DOCTEST_CUDA1)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">gpu1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cuda:1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(gpu1, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.half, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">non_blocking</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;cuda:1&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cpu </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cpu&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(cpu)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).to(tensorplay.cdouble)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3741</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,  </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2382</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5593</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.4443</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear(tensorplay.ones(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.cdouble))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br></div></div><hr><h4 id="to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source-1" tabindex="-1"><code>to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source-1" aria-label="Permalink to &quot;`to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119)&quot;">​</a></h4><p>Move the parameters and buffers to the specified device without copying storage.</p><h4 id="args-52" tabindex="-1">Args <a class="header-anchor" href="#args-52" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): The desired device of the parameters and buffers in this module.</li><li><strong>recurse</strong> (<code>bool</code>): Whether parameters and buffers of submodules should be recursively moved to the specified device.</li></ul><h4 id="returns-47" tabindex="-1">Returns <a class="header-anchor" href="#returns-47" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="train-self-mode-bool-true-self-source-1" tabindex="-1"><code>train(self, mode: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#train-self-mode-bool-true-self-source-1" aria-label="Permalink to &quot;`train(self, mode: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786)&quot;">​</a></h4><p>Set the module in training mode.</p><p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e., whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, etc.</p><h4 id="args-53" tabindex="-1">Args <a class="header-anchor" href="#args-53" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>mode</strong> (<code>bool</code>): whether to set training mode (<code>True</code>) or evaluation mode (<code>False</code>). Default: <code>True</code>.</li></ul><h4 id="returns-48" tabindex="-1">Returns <a class="header-anchor" href="#returns-48" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="type-self-dst-type-union-tensorplay-dtype-str-self-source-1" tabindex="-1"><code>type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#type-self-dst-type-union-tensorplay-dtype-str-self-source-1" aria-label="Permalink to &quot;`type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061)&quot;">​</a></h4><p>Casts all parameters and buffers to <code>dst_type</code>.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-54" tabindex="-1">Args <a class="header-anchor" href="#args-54" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>dst_type</strong> (<code>type or string</code>): the desired type</li></ul><h4 id="returns-49" tabindex="-1">Returns <a class="header-anchor" href="#returns-49" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="zero-grad-self-set-to-none-bool-true-none-source-1" tabindex="-1"><code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#zero-grad-self-set-to-none-bool-true-none-source-1" aria-label="Permalink to &quot;`zero_grad(self, set_to_none: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849)&quot;">​</a></h4><p>Reset gradients of all model parameters.</p><p>See similar function under <code>tensorplay.optim.Optimizer</code> for more context.</p><h4 id="args-55" tabindex="-1">Args <a class="header-anchor" href="#args-55" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>set_to_none</strong> (<code>bool</code>): instead of setting to zero, set the grads to None. See <code>tensorplay.optim.Optimizer.zero_grad</code> for details.</li></ul><hr></details><h3 id="class-instancenorm3d-source" tabindex="-1"><code>class InstanceNorm3d</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/instancenorm.py#L356" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-instancenorm3d-source" aria-label="Permalink to &quot;`class InstanceNorm3d` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/instancenorm.py#L356)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">InstanceNorm3d(num_features: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, eps: </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">float</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1e-05</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, momentum: </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">float</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, affine: </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bool</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, track_running_stats: </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bool</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>_InstanceNorm</code></p><p>Applies Instance Normalization.</p><p>This operation applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper <code>Instance Normalization: The Missing Ingredient for Fast Stylization &lt;https://arxiv.org/abs/1607.08022&gt;</code>__.</p><p>.. math</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> \</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.489ex;" xmlns="http://www.w3.org/2000/svg" width="1.229ex" height="1.486ex" role="img" focusable="false" viewBox="0 -441 543 657" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FE" d="M31 249Q11 249 11 258Q11 275 26 304T66 365T129 418T206 441Q233 441 239 440Q287 429 318 386T371 255Q385 195 385 170Q385 166 386 166L398 193Q418 244 443 300T486 391T508 430Q510 431 524 431H537Q543 425 543 422Q543 418 522 378T463 251T391 71Q385 55 378 6T357 -100Q341 -165 330 -190T303 -216Q286 -216 286 -188Q286 -138 340 32L346 51L347 69Q348 79 348 100Q348 257 291 317Q251 355 196 355Q148 355 108 329T51 260Q49 251 47 251Q45 249 31 249Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>γ</mi></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.281ex" height="2.034ex" role="img" focusable="false" viewBox="0 -705 566 899" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>β</mi></math></mjx-assistive-mml></mjx-container> are learnable parameter vectors of size C (where C is the input size) if <code>affine</code> is <code>True</code>. The standard-deviation is calculated via the biased estimator, equivalent to <code>tensorplay.var(input, unbiased=False)</code>.</p><p>By default, this layer uses instance statistics computed from input data in both training and evaluation modes.</p><p>If <code>track_running_stats</code> is set to <code>True</code>, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default <code>momentum</code> of 0.1.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This <code>momentum</code> argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="46.933ex" height="2.398ex" role="img" focusable="false" viewBox="0 -810 20744.2 1060" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(313.8,16) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z" style="stroke-width:3;"></path></g></g></g><g data-mml-node="mtext" transform="translate(605,-150) scale(0.707)"><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" style="stroke-width:3;"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(556,0)" style="stroke-width:3;"></path><path data-c="77" d="M90 368Q84 378 76 380T40 385H18V431H24L43 430Q62 430 84 429T116 428Q206 428 221 431H229V385H215Q177 383 177 368Q177 367 221 239L265 113L339 328L333 345Q323 374 316 379Q308 384 278 385H258V431H264Q270 428 348 428Q439 428 454 431H461V385H452Q404 385 404 369Q404 366 418 324T449 234T481 143L496 100L537 219Q579 341 579 347Q579 363 564 373T530 385H522V431H529Q541 428 624 428Q692 428 698 431H703V385H697Q696 385 691 385T682 384Q635 377 619 334L559 161Q546 124 528 71Q508 12 503 1T487 -11H479Q460 -11 456 -4Q455 -3 407 133L361 267Q359 263 266 -4Q261 -11 243 -11H238Q225 -11 220 -3L90 368Z" transform="translate(1000,0)" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(2150.4,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3206.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(3595.2,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(4317.4,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mtext" transform="translate(5317.6,0)"><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" style="stroke-width:3;"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(833,0)" style="stroke-width:3;"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1333,0)" style="stroke-width:3;"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(2166,0)" style="stroke-width:3;"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(2610,0)" style="stroke-width:3;"></path><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(3166,0)" style="stroke-width:3;"></path><path data-c="75" d="M383 58Q327 -10 256 -10H249Q124 -10 105 89Q104 96 103 226Q102 335 102 348T96 369Q86 385 36 385H25V408Q25 431 27 431L38 432Q48 433 67 434T105 436Q122 437 142 438T172 441T184 442H187V261Q188 77 190 64Q193 49 204 40Q224 26 264 26Q290 26 311 35T343 58T363 90T375 120T379 144Q379 145 379 161T380 201T380 248V315Q380 361 370 372T320 385H302V431Q304 431 378 436T457 442H464V264Q464 84 465 81Q468 61 479 55T524 46H542V0Q540 0 467 -5T390 -11H383V58Z" transform="translate(3555,0)" style="stroke-width:3;"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(4111,0)" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(10261.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(10872.9,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(11873.1,0)"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(313.8,16) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z" style="stroke-width:3;"></path></g></g></g><g data-mml-node="mo" transform="translate(12667.3,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mtext" transform="translate(13667.5,0)"><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" style="stroke-width:3;"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(833,0)" style="stroke-width:3;"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1333,0)" style="stroke-width:3;"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(2166,0)" style="stroke-width:3;"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(2610,0)" style="stroke-width:3;"></path><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(3166,0)" style="stroke-width:3;"></path><path data-c="75" d="M383 58Q327 -10 256 -10H249Q124 -10 105 89Q104 96 103 226Q102 335 102 348T96 369Q86 385 36 385H25V408Q25 431 27 431L38 432Q48 433 67 434T105 436Q122 437 142 438T172 441T184 442H187V261Q188 77 190 64Q193 49 204 40Q224 26 264 26Q290 26 311 35T343 58T363 90T375 120T379 144Q379 145 379 161T380 201T380 248V315Q380 361 370 372T320 385H302V431Q304 431 378 436T457 442H464V264Q464 84 465 81Q468 61 479 55T524 46H542V0Q540 0 467 -5T390 -11H383V58Z" transform="translate(3555,0)" style="stroke-width:3;"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(4111,0)" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(18833.7,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(19834,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" style="stroke-width:3;"></path></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mover><mi>x</mi><mo stretchy="false">^</mo></mover></mrow><mtext>new</mtext></msub><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mtext>momentum</mtext><mo stretchy="false">)</mo><mo>×</mo><mrow data-mjx-texclass="ORD"><mover><mi>x</mi><mo stretchy="false">^</mo></mover></mrow><mo>+</mo><mtext>momentum</mtext><mo>×</mo><msub><mi>x</mi><mi>t</mi></msub></math></mjx-assistive-mml></mjx-container>, where <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.857ex" role="img" focusable="false" viewBox="0 -810 572 821" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(313.8,16) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z" style="stroke-width:3;"></path></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mover><mi>x</mi><mo stretchy="false">^</mo></mover></mrow></math></mjx-assistive-mml></mjx-container> is the estimated statistic and <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.059ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 910.3 599.8" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" style="stroke-width:3;"></path></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mi>t</mi></msub></math></mjx-assistive-mml></mjx-container> is the new observed value.</p></div><div class="info custom-block"><p class="custom-block-title">INFO</p><p><code>InstanceNorm3d</code> and <code>LayerNorm</code> are very similar, but have some subtle differences. <code>InstanceNorm3d</code> is applied on each channel of channeled data like 3D models with RGB color, but <code>LayerNorm</code> is usually applied on entire sample and often in NLP tasks. Additionally, <code>LayerNorm</code> applies elementwise affine transform, while <code>InstanceNorm3d</code> usually don&#39;t apply affine transform.</p></div><h4 id="args-56" tabindex="-1">Args <a class="header-anchor" href="#args-56" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>num_features</strong>: <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.719ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 760 727" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi></math></mjx-assistive-mml></mjx-container> from an expected input of size <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="15.766ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 6968.7 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1277,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1721.7,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2481.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2926.3,0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3754.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(4199,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5087,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(5531.7,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(6579.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>N</mi><mo>,</mo><mi>C</mi><mo>,</mo><mi>D</mi><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> or <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="12.751ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 5636 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1149,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1593.7,0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2421.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2866.3,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3754.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(4199,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5247,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>C</mi><mo>,</mo><mi>D</mi><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container></li><li><strong>eps</strong>: a value added to the denominator for numerical stability. Default: 1e-5</li><li><strong>momentum</strong>: the value used for the running_mean and running_var computation. Default: 0.1</li><li><strong>affine</strong>: a boolean value that when set to <code>True</code>, this module has learnable affine parameters, initialized the same way as done for batch normalization.</li><li><strong>Default</strong>: <code>False</code>.</li><li><strong>track_running_stats</strong>: a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: <code>False</code></li></ul><h4 id="shape-2" tabindex="-1">Shape <a class="header-anchor" href="#shape-2" aria-label="Permalink to &quot;Shape&quot;">​</a></h4><ul><li>Input: <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="15.766ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 6968.7 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1277,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1721.7,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2481.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2926.3,0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3754.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(4199,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5087,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(5531.7,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(6579.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>N</mi><mo>,</mo><mi>C</mi><mo>,</mo><mi>D</mi><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> or <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="12.751ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 5636 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1149,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1593.7,0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2421.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2866.3,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3754.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(4199,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5247,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>C</mi><mo>,</mo><mi>D</mi><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container></li><li>Output: <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="15.766ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 6968.7 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1277,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1721.7,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2481.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2926.3,0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3754.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(4199,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5087,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(5531.7,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(6579.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>N</mi><mo>,</mo><mi>C</mi><mo>,</mo><mi>D</mi><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> or <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="12.751ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 5636 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1149,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1593.7,0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2421.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2866.3,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3754.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(4199,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5247,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>C</mi><mo>,</mo><mi>D</mi><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> (same shape as input)</li></ul><h4 id="examples-4" tabindex="-1">Examples <a class="header-anchor" href="#examples-4" aria-label="Permalink to &quot;Examples&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Without Learnable Parameters</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.InstanceNorm3d(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">100</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># With Learnable Parameters</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.InstanceNorm3d(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">100</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">affine</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.randn(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">100</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">35</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">45</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">output </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> m(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><details><summary>Methods</summary><h4 id="init-self-num-features-int-eps-float-1e-05-momentum-float-0-1-affine-bool-false-track-running-stats-bool-false-device-none-dtype-none-none-source-2" tabindex="-1"><code>__init__(self, num_features: int, eps: float = 1e-05, momentum: float = 0.1, affine: bool = False, track_running_stats: bool = False, device=None, dtype=None) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/instancenorm.py#L20" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-num-features-int-eps-float-1e-05-momentum-float-0-1-affine-bool-false-track-running-stats-bool-false-device-none-dtype-none-none-source-2" aria-label="Permalink to &quot;`__init__(self, num_features: int, eps: float = 1e-05, momentum: float = 0.1, affine: bool = False, track_running_stats: bool = False, device=None, dtype=None) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/instancenorm.py#L20)&quot;">​</a></h4><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p><hr><h4 id="add-module-self-name-str-module-optional-forwardref-module-none-source-2" tabindex="-1"><code>add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#add-module-self-name-str-module-optional-forwardref-module-none-source-2" aria-label="Permalink to &quot;`add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667)&quot;">​</a></h4><p>Add a child module to the current module.</p><p>The module can be accessed as an attribute using the given name.</p><h4 id="args-57" tabindex="-1">Args <a class="header-anchor" href="#args-57" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the child module. The child module can be accessed from this module using the given name</li><li><strong>module</strong> (<code>Module</code>): child module to be added to the module.</li></ul><hr><h4 id="apply-self-fn-callable-forwardref-module-nonetype-self-source-2" tabindex="-1"><code>apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#apply-self-fn-callable-forwardref-module-nonetype-self-source-2" aria-label="Permalink to &quot;`apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990)&quot;">​</a></h4><p>Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self.</p><p>Typical use includes initializing the parameters of a model (see also <code>nn-init-doc</code>).</p><h4 id="args-58" tabindex="-1">Args <a class="header-anchor" href="#args-58" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>fn</strong> (``Module<code> -&gt; None</code>): function to be applied to each submodule</li></ul><h4 id="returns-50" tabindex="-1">Returns <a class="header-anchor" href="#returns-50" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="example-20" tabindex="-1">Example <a class="header-anchor" href="#example-20" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@tensorplay.no_grad</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> init_weights</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        m.weight.fill_(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m.weight)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net.apply(init_weights)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><hr><h4 id="bfloat16-self-self-source-2" tabindex="-1"><code>bfloat16(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#bfloat16-self-self-source-2" aria-label="Permalink to &quot;`bfloat16(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-51" tabindex="-1">Returns <a class="header-anchor" href="#returns-51" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source-2" tabindex="-1"><code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source-2" aria-label="Permalink to &quot;`buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627)&quot;">​</a></h4><p>Return an iterator over module buffers.</p><h4 id="args-59" tabindex="-1">Args <a class="header-anchor" href="#args-59" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.</li></ul><h4 id="yields-16" tabindex="-1">Yields <a class="header-anchor" href="#yields-16" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>tensorplay.Tensor: module buffer
</code></pre><h4 id="example-21" tabindex="-1">Example <a class="header-anchor" href="#example-21" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.buffers():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf), buf.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="children-self-collections-abc-iterator-module-source-2" tabindex="-1"><code>children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#children-self-collections-abc-iterator-module-source-2" aria-label="Permalink to &quot;`children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681)&quot;">​</a></h4><p>Return an iterator over immediate children modules.</p><h4 id="yields-17" tabindex="-1">Yields <a class="header-anchor" href="#yields-17" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a child module</li></ul><hr><h4 id="compile-self-args-kwargs-source-2" tabindex="-1"><code>compile(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#compile-self-args-kwargs-source-2" aria-label="Permalink to &quot;`compile(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944)&quot;">​</a></h4><p>Compile this Module&#39;s forward using <code>tensorplay.compile</code>.</p><p>This Module&#39;s <code>__call__</code> method is compiled and all arguments are passed as-is to <code>tensorplay.compile</code>.</p><p>See <code>tensorplay.compile</code> for details on the arguments for this function.</p><hr><h4 id="cpu-self-self-source-2" tabindex="-1"><code>cpu(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cpu-self-self-source-2" aria-label="Permalink to &quot;`cpu(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050)&quot;">​</a></h4><p>Move all model parameters and buffers to the CPU.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-52" tabindex="-1">Returns <a class="header-anchor" href="#returns-52" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source-2" tabindex="-1"><code>cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source-2" aria-label="Permalink to &quot;`cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031)&quot;">​</a></h4><p>Move all model parameters and buffers to the GPU.</p><p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on GPU while being optimized.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-60" tabindex="-1">Args <a class="header-anchor" href="#args-60" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>int, optional</code>): if specified, all parameters will be copied to that device</li></ul><h4 id="returns-53" tabindex="-1">Returns <a class="header-anchor" href="#returns-53" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="double-self-self-source-2" tabindex="-1"><code>double(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#double-self-self-source-2" aria-label="Permalink to &quot;`double(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-54" tabindex="-1">Returns <a class="header-anchor" href="#returns-54" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="eval-self-self-source-2" tabindex="-1"><code>eval(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#eval-self-self-source-2" aria-label="Permalink to &quot;`eval(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808)&quot;">​</a></h4><p>Set the module in evaluation mode.</p><p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e. whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, etc.</p><p>This is equivalent with <code>self.train(False) &lt;tensorplay.nn.Module.train&gt;</code>.</p><p>See <code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.</p><h4 id="returns-55" tabindex="-1">Returns <a class="header-anchor" href="#returns-55" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="extra-repr-self-source-2" tabindex="-1"><code>extra_repr(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L97" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#extra-repr-self-source-2" aria-label="Permalink to &quot;`extra_repr(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L97)&quot;">​</a></h4><p>Return the extra representation of the module.</p><p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p><hr><h4 id="float-self-self-source-2" tabindex="-1"><code>float(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#float-self-self-source-2" aria-label="Permalink to &quot;`float(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-56" tabindex="-1">Returns <a class="header-anchor" href="#returns-56" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="forward-self-input-tensorplay-c-tensorbase-tensorplay-c-tensorbase-source-2" tabindex="-1"><code>forward(self, input: tensorplay._C.TensorBase) -&gt; tensorplay._C.TensorBase</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/instancenorm.py#L102" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#forward-self-input-tensorplay-c-tensorbase-tensorplay-c-tensorbase-source-2" aria-label="Permalink to &quot;`forward(self, input: tensorplay._C.TensorBase) -&gt; tensorplay._C.TensorBase` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/instancenorm.py#L102)&quot;">​</a></h4><p>Define the computation performed at every call.</p><p>Should be overridden by all subclasses.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code> instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.</p></div><hr><h4 id="get-buffer-self-target-str-tensor-source-2" tabindex="-1"><code>get_buffer(self, target: str) -&gt; &#39;Tensor&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-buffer-self-target-str-tensor-source-2" aria-label="Permalink to &quot;`get_buffer(self, target: str) -&gt; &#39;Tensor&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881)&quot;">​</a></h4><p>Return the buffer given by <code>target</code> if it exists, otherwise throw an error.</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-61" tabindex="-1">Args <a class="header-anchor" href="#args-61" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the buffer to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-57" tabindex="-1">Returns <a class="header-anchor" href="#returns-57" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.Tensor: The buffer referenced by ``target``
</code></pre><h4 id="raises-8" tabindex="-1">Raises <a class="header-anchor" href="#raises-8" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not a buffer</li></ul><hr><h4 id="get-extra-state-self-any-source-2" tabindex="-1"><code>get_extra_state(self) -&gt; Any</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-extra-state-self-any-source-2" aria-label="Permalink to &quot;`get_extra_state(self) -&gt; Any` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917)&quot;">​</a></h4><p>Return any extra state to include in the module&#39;s state_dict.</p><p>Implement this and a corresponding <code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module&#39;s <code>state_dict()</code>.</p><p>Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.</p><h4 id="returns-58" tabindex="-1">Returns <a class="header-anchor" href="#returns-58" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>object</strong>: Any extra state to store in the module&#39;s state_dict</li></ul><hr><h4 id="get-parameter-self-target-str-parameter-source-2" tabindex="-1"><code>get_parameter(self, target: str) -&gt; &#39;Parameter&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-parameter-self-target-str-parameter-source-2" aria-label="Permalink to &quot;`get_parameter(self, target: str) -&gt; &#39;Parameter&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845)&quot;">​</a></h4><p>Return the parameter given by <code>target</code> if it exists, otherwise throw an error.</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-62" tabindex="-1">Args <a class="header-anchor" href="#args-62" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the Parameter to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-59" tabindex="-1">Returns <a class="header-anchor" href="#returns-59" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Parameter: The Parameter referenced by ``target``
</code></pre><h4 id="raises-9" tabindex="-1">Raises <a class="header-anchor" href="#raises-9" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not an <code>nn.Parameter</code></li></ul><hr><h4 id="get-submodule-self-target-str-module-source-2" tabindex="-1"><code>get_submodule(self, target: str) -&gt; &#39;Module&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-submodule-self-target-str-module-source-2" aria-label="Permalink to &quot;`get_submodule(self, target: str) -&gt; &#39;Module&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699)&quot;">​</a></h4><p>Return the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> which has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(&quot;net_b.linear&quot;)</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(&quot;net_b.net_c.conv&quot;)</code>.</p><p>The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.</p><h4 id="args-63" tabindex="-1">Args <a class="header-anchor" href="#args-63" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li></ul><h4 id="returns-60" tabindex="-1">Returns <a class="header-anchor" href="#returns-60" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Module: The submodule referenced by ``target``
</code></pre><h4 id="raises-10" tabindex="-1">Raises <a class="header-anchor" href="#raises-10" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If at any point along the path resulting from the target string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="half-self-self-source-2" tabindex="-1"><code>half(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#half-self-self-source-2" aria-label="Permalink to &quot;`half(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-61" tabindex="-1">Returns <a class="header-anchor" href="#returns-61" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source-2" tabindex="-1"><code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source-2" aria-label="Permalink to &quot;`load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436)&quot;">​</a></h4><p>Copy parameters and buffers from <code>state_dict</code> into this module and its descendants.</p><p>If <code>strict</code> is <code>True</code>, then the keys of <code>state_dict</code> must exactly match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function.</p><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>If <code>assign</code> is <code>True</code> the optimizer must be created after the call to <code>load_state_dict</code> unless <code>~tensorplay.__future__.get_swap_module_params_on_conversion</code> is <code>True</code>.</p></div><h4 id="args-64" tabindex="-1">Args <a class="header-anchor" href="#args-64" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state_dict</strong> (<code>dict</code>): a dict containing parameters and persistent buffers.</li><li><strong>strict</strong> (<code>bool, optional</code>): whether to strictly enforce that the keys in <code>state_dict</code> match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function. Default: <code>True</code></li><li><strong>assign</strong> (<code>bool, optional</code>): When set to <code>False</code>, the properties of the tensors in the current module are preserved whereas setting it to <code>True</code> preserves properties of the Tensors in the state dict. The only exception is the <code>requires_grad</code> field of <code>~tensorplay.nn.Parameter</code> for which the value from the module is preserved. Default: <code>False</code></li></ul><h4 id="returns-62" tabindex="-1">Returns <a class="header-anchor" href="#returns-62" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:
    * ``missing_keys`` is a list of str containing any keys that are expected
        by this module but missing from the provided ``state_dict``.
    * ``unexpected_keys`` is a list of str containing the keys that are not
        expected by this module but present in the provided ``state_dict``.
</code></pre><h4 id="note-6" tabindex="-1">Note <a class="header-anchor" href="#note-6" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>If a parameter or buffer is registered as ``None`` and its corresponding key
exists in `state_dict`, `load_state_dict` will raise a
``RuntimeError``.
</code></pre><hr><h4 id="modules-self-collections-abc-iterator-module-source-2" tabindex="-1"><code>modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#modules-self-collections-abc-iterator-module-source-2" aria-label="Permalink to &quot;`modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710)&quot;">​</a></h4><p>Return an iterator over all modules in the network.</p><h4 id="yields-18" tabindex="-1">Yields <a class="header-anchor" href="#yields-18" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a module in the network</li></ul><h4 id="note-7" tabindex="-1">Note <a class="header-anchor" href="#note-7" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-22" tabindex="-1">Example <a class="header-anchor" href="#example-22" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source-2" tabindex="-1"><code>named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source-2" aria-label="Permalink to &quot;`named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650)&quot;">​</a></h4><p>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p><h4 id="args-65" tabindex="-1">Args <a class="header-anchor" href="#args-65" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all buffer names.</li><li><strong>recurse</strong> (<code>bool, optional</code>): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Defaults to True.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated buffers in the result. Defaults to True.</li></ul><h4 id="yields-19" tabindex="-1">Yields <a class="header-anchor" href="#yields-19" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, tensorplay.Tensor): Tuple containing the name and buffer
</code></pre><h4 id="example-23" tabindex="-1">Example <a class="header-anchor" href="#example-23" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_buffers():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_var&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-children-self-collections-abc-iterator-tuple-str-module-source-2" tabindex="-1"><code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-children-self-collections-abc-iterator-tuple-str-module-source-2" aria-label="Permalink to &quot;`named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690)&quot;">​</a></h4><p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p><h4 id="yields-20" tabindex="-1">Yields <a class="header-anchor" href="#yields-20" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple containing a name and child module
</code></pre><h4 id="example-24" tabindex="-1">Example <a class="header-anchor" href="#example-24" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, module </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.named_children():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv4&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv5&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(module)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source-2" tabindex="-1"><code>named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source-2" aria-label="Permalink to &quot;`named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737)&quot;">​</a></h4><p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p><h4 id="args-66" tabindex="-1">Args <a class="header-anchor" href="#args-66" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>memo</strong>: a memo to store the set of modules already added to the result</li><li><strong>prefix</strong>: a prefix that will be added to the name of the module</li><li><strong>remove_duplicate</strong>: whether to remove the duplicated module instances in the result or not</li></ul><h4 id="yields-21" tabindex="-1">Yields <a class="header-anchor" href="#yields-21" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple of name and module
</code></pre><h4 id="note-8" tabindex="-1">Note <a class="header-anchor" href="#note-8" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-25" tabindex="-1">Example <a class="header-anchor" href="#example-25" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.named_modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;0&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source-2" tabindex="-1"><code>named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source-2" aria-label="Permalink to &quot;`named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595)&quot;">​</a></h4><p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p><h4 id="args-67" tabindex="-1">Args <a class="header-anchor" href="#args-67" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all parameter names.</li><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated parameters in the result. Defaults to True.</li></ul><h4 id="yields-22" tabindex="-1">Yields <a class="header-anchor" href="#yields-22" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Parameter): Tuple containing the name and parameter
</code></pre><h4 id="example-26" tabindex="-1">Example <a class="header-anchor" href="#example-26" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_parameters():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source-2" tabindex="-1"><code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source-2" aria-label="Permalink to &quot;`parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570)&quot;">​</a></h4><p>Return an iterator over module parameters.</p><p>This is typically passed to an optimizer.</p><h4 id="args-68" tabindex="-1">Args <a class="header-anchor" href="#args-68" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</li></ul><h4 id="yields-23" tabindex="-1">Yields <a class="header-anchor" href="#yields-23" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Parameter</strong>: module parameter</li></ul><h4 id="example-27" tabindex="-1">Example <a class="header-anchor" href="#example-27" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.parameters():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param), param.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source-2" tabindex="-1"><code>register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source-2" aria-label="Permalink to &quot;`register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>This function is deprecated in favor of <code>~tensorplay.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.</p><h4 id="returns-63" tabindex="-1">Returns <a class="header-anchor" href="#returns-63" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source-2" tabindex="-1"><code>register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source-2" aria-label="Permalink to &quot;`register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553)&quot;">​</a></h4><p>Add a buffer to the module.</p><p>This is typically used to register a buffer that should not be considered a model parameter. For example, BatchNorm&#39;s <code>running_mean</code> is not a parameter, but is part of the module&#39;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting <code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&#39;s <code>state_dict</code>.</p><p>Buffers can be accessed as attributes using given names.</p><h4 id="args-69" tabindex="-1">Args <a class="header-anchor" href="#args-69" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the buffer. The buffer can be accessed from this module using the given name</li><li><strong>tensor</strong> (<code>Tensor or None</code>): buffer to be registered. If <code>None</code>, then operations that run on buffers, such as <code>cuda</code>, are ignored. If <code>None</code>, the buffer is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li><li><strong>persistent</strong> (<code>bool</code>): whether the buffer is part of this module&#39;s <code>state_dict</code>.</li></ul><h4 id="example-28" tabindex="-1">Example <a class="header-anchor" href="#example-28" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.register_buffer(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_mean&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, tensorplay.zeros(num_features))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><hr><h4 id="register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source-2" tabindex="-1"><code>register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source-2" aria-label="Permalink to &quot;`register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592)&quot;">​</a></h4><p>Register a forward hook on the module.</p><p>The hook will be called every time after <code>forward</code> has computed an output.</p><p>If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after <code>forward</code> is called. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-70" tabindex="-1">Args <a class="header-anchor" href="#args-70" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If <code>True</code>, the provided <code>hook</code> will be fired before all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward</code> hooks registered with <code>register_module_forward_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If <code>True</code>, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>always_call</strong> (<code>bool</code>): If <code>True</code> the <code>hook</code> will be run regardless of whether an exception is raised while calling the Module.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-64" tabindex="-1">Returns <a class="header-anchor" href="#returns-64" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source-2" tabindex="-1"><code>register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source-2" aria-label="Permalink to &quot;`register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526)&quot;">​</a></h4><p>Register a forward pre-hook on the module.</p><p>The hook will be called every time before <code>forward</code> is invoked.</p><p>If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> of modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> and</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> kwargs</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-71" tabindex="-1">Args <a class="header-anchor" href="#args-71" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward_pre</code> hooks registered with <code>register_module_forward_pre_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If true, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-65" tabindex="-1">Returns <a class="header-anchor" href="#returns-65" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-2" tabindex="-1"><code>register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-2" aria-label="Permalink to &quot;`register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>The hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:</p><pre><code>1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.
2. If none of the module inputs require gradients, the hook will fire when the gradients are computed
   with respect to module outputs.
3. If none of the module outputs require gradients, then the hooks will not fire.
</code></pre><p>The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_input, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Tensor) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_input</code> and <code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of <code>grad_input</code> in subsequent computations. <code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in <code>grad_input</code> and <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> outputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-72" tabindex="-1">Args <a class="header-anchor" href="#args-72" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward</code> hooks registered with <code>register_module_full_backward_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-66" tabindex="-1">Returns <a class="header-anchor" href="#returns-66" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-2" tabindex="-1"><code>register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-2" aria-label="Permalink to &quot;`register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287)&quot;">​</a></h4><p>Register a backward pre-hook on the module.</p><p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tuple[Tensor] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of <code>grad_output</code> in subsequent computations. Entries in <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-73" tabindex="-1">Args <a class="header-anchor" href="#args-73" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward_pre</code> hooks registered with <code>register_module_full_backward_pre_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-67" tabindex="-1">Returns <a class="header-anchor" href="#returns-67" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-post-hook-self-hook-source-2" tabindex="-1"><code>register_load_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-post-hook-self-hook-source-2" aria-label="Permalink to &quot;`register_load_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226)&quot;">​</a></h4><p>Register a post-hook to be run after module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, incompatible_keys) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.</p><p>The given incompatible_keys can be modified inplace if needed.</p><p>Note that the checks performed when calling <code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.</p><h4 id="returns-68" tabindex="-1">Returns <a class="header-anchor" href="#returns-68" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-pre-hook-self-hook-source-2" tabindex="-1"><code>register_load_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-pre-hook-self-hook-source-2" aria-label="Permalink to &quot;`register_load_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214)&quot;">​</a></h4><p>Register a pre-hook to be run before module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # noqa: B950</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="arguments-2" tabindex="-1">Arguments <a class="header-anchor" href="#arguments-2" aria-label="Permalink to &quot;Arguments&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): Callable hook that will be invoked before loading the state dict.</li></ul><hr><h4 id="register-module-self-name-str-module-optional-forwardref-module-none-source-2" tabindex="-1"><code>register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-module-self-name-str-module-optional-forwardref-module-none-source-2" aria-label="Permalink to &quot;`register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695)&quot;">​</a></h4><p>Alias for <code>add_module</code>.</p><hr><h4 id="register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source-2" tabindex="-1"><code>register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source-2" aria-label="Permalink to &quot;`register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617)&quot;">​</a></h4><p>Add a parameter to the module.</p><p>The parameter can be accessed as an attribute using given name.</p><h4 id="args-74" tabindex="-1">Args <a class="header-anchor" href="#args-74" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the parameter. The parameter can be accessed from this module using the given name</li><li><strong>param</strong> (<code>Parameter or None</code>): parameter to be added to the module. If <code>None</code>, then operations that run on parameters, such as <code>cuda</code>, are ignored. If <code>None</code>, the parameter is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li></ul><hr><h4 id="register-state-dict-post-hook-self-hook-source-2" tabindex="-1"><code>register_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-post-hook-self-hook-source-2" aria-label="Permalink to &quot;`register_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017)&quot;">​</a></h4><p>Register a post-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can modify the <code>state_dict</code> inplace.</p><hr><h4 id="register-state-dict-pre-hook-self-hook-source-2" tabindex="-1"><code>register_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-pre-hook-self-hook-source-2" aria-label="Permalink to &quot;`register_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041)&quot;">​</a></h4><p>Register a pre-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, prefix, keep_vars) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p><hr><h4 id="requires-grad-self-requires-grad-bool-true-self-source-2" tabindex="-1"><code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#requires-grad-self-requires-grad-bool-true-self-source-2" aria-label="Permalink to &quot;`requires_grad_(self, requires_grad: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826)&quot;">​</a></h4><p>Change if autograd should record operations on parameters in this module.</p><p>This method sets the parameters&#39; <code>requires_grad</code> attributes in-place.</p><p>This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).</p><p>See <code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.</p><h4 id="args-75" tabindex="-1">Args <a class="header-anchor" href="#args-75" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>requires_grad</strong> (<code>bool</code>): whether autograd should record operations on parameters in this module. Default: <code>True</code>.</li></ul><h4 id="returns-69" tabindex="-1">Returns <a class="header-anchor" href="#returns-69" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="reset-parameters-self-none-source-2" tabindex="-1"><code>reset_parameters(self) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L88" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#reset-parameters-self-none-source-2" aria-label="Permalink to &quot;`reset_parameters(self) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L88)&quot;">​</a></h4><hr><h4 id="reset-running-stats-self-none-source-2" tabindex="-1"><code>reset_running_stats(self) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L80" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#reset-running-stats-self-none-source-2" aria-label="Permalink to &quot;`reset_running_stats(self) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L80)&quot;">​</a></h4><hr><h4 id="set-extra-state-self-state-any-none-source-2" tabindex="-1"><code>set_extra_state(self, state: Any) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-extra-state-self-state-any-none-source-2" aria-label="Permalink to &quot;`set_extra_state(self, state: Any) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938)&quot;">​</a></h4><p>Set extra state contained in the loaded <code>state_dict</code>.</p><p>This function is called from <code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding <code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.</p><h4 id="args-76" tabindex="-1">Args <a class="header-anchor" href="#args-76" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state</strong> (<code>dict</code>): Extra state from the <code>state_dict</code></li></ul><hr><h4 id="set-submodule-self-target-str-module-module-strict-bool-false-none-source-2" tabindex="-1"><code>set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-submodule-self-target-str-module-module-strict-bool-false-none-source-2" aria-label="Permalink to &quot;`set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764)&quot;">​</a></h4><p>Set the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>If <code>strict</code> is set to <code>False</code> (default), the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>strict</code> is set to <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule does not exist.</p></div><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(3, 3, 3) ) (linear): Linear(3, 3) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To override the <code>Conv2d</code> with a new submodule <code>Linear</code>, you could call <code>set_submodule(&quot;net_b.net_c.conv&quot;, nn.Linear(1, 1))</code> where <code>strict</code> could be <code>True</code> or <code>False</code></p><p>To add a new submodule <code>Conv2d</code> to the existing <code>net_b</code> module, you would call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1))</code>.</p><p>In the above if you set <code>strict=True</code> and call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1), strict=True)</code>, an AttributeError will be raised because <code>net_b</code> does not have a submodule named <code>conv</code>.</p><h4 id="args-77" tabindex="-1">Args <a class="header-anchor" href="#args-77" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li><li><strong>module</strong>: The module to set the submodule to.</li><li><strong>strict</strong>: If <code>False</code>, the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule doesn&#39;t already exist.</li></ul><h4 id="raises-11" tabindex="-1">Raises <a class="header-anchor" href="#raises-11" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>ValueError</strong>: If the <code>target</code> string is empty or if <code>module</code> is not an instance of <code>nn.Module</code>.</li><li><strong>AttributeError</strong>: If at any point along the path resulting from the <code>target</code> string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="share-memory-self-self-source-2" tabindex="-1"><code>share_memory(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#share-memory-self-self-source-2" aria-label="Permalink to &quot;`share_memory(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877)&quot;">​</a></h4><p>See <code>tensorplay.Tensor.share_memory_</code>.</p><hr><h4 id="state-dict-self-args-destination-none-prefix-keep-vars-false-source-2" tabindex="-1"><code>state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#state-dict-self-args-destination-none-prefix-keep-vars-false-source-2" aria-label="Permalink to &quot;`state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105)&quot;">​</a></h4><p>Return a dictionary containing references to the whole state of the module.</p><p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>The returned object is a shallow copy. It contains references to the module&#39;s parameters and buffers.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Currently <code>state_dict()</code> also accepts positional arguments for <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However, this is being deprecated and keyword arguments will be enforced in future releases.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Please avoid the use of argument <code>destination</code> as it is not designed for end-users.</p></div><h4 id="args-78" tabindex="-1">Args <a class="header-anchor" href="#args-78" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>destination</strong> (<code>dict, optional</code>): If provided, the state of module will be updated into the dict and the same object is returned. Otherwise, an <code>OrderedDict</code> will be created and returned.</li><li><strong>Default</strong>: <code>None</code>.</li><li><strong>prefix</strong> (<code>str, optional</code>): a prefix added to parameter and buffer names to compose the keys in state_dict. Default: <code>&#39;&#39;</code>.</li><li><strong>keep_vars</strong> (<code>bool, optional</code>): by default the <code>~tensorplay.Tensor</code> s returned in the state dict are detached from autograd. If it&#39;s set to <code>True</code>, detaching will not be performed.</li><li><strong>Default</strong>: <code>False</code>.</li></ul><h4 id="returns-70" tabindex="-1">Returns <a class="header-anchor" href="#returns-70" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>dict</strong>: a dictionary containing a whole state of the module</li></ul><h4 id="example-29" tabindex="-1">Example <a class="header-anchor" href="#example-29" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">module.state_dict().keys()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;weight&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><hr><h4 id="to-self-args-kwargs-source-2" tabindex="-1"><code>to(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-self-args-kwargs-source-2" aria-label="Permalink to &quot;`to(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151)&quot;">​</a></h4><p>Move and/or cast the parameters and buffers.</p><p>This can be called as</p><p>.. function:: to(device=None, dtype=None, non_blocking=False) :noindex:</p><p>.. function:: to(dtype, non_blocking=False) :noindex:</p><p>.. function:: to(tensor, non_blocking=False) :noindex:</p><p>.. function:: to(memory_format=tensorplay.channels_last) :noindex:</p><p>Its signature is similar to <code>tensorplay.Tensor.to</code>, but only accepts floating point or complex <code>dtype</code>\ s. In addition, this method will only cast the floating point or complex parameters and buffers to <code>dtype</code> (if given). The integral parameters and buffers will be moved <code>device</code>, if that is given, but with dtypes unchanged. When <code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.</p><p>See below for examples.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-79" tabindex="-1">Args <a class="header-anchor" href="#args-79" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): the desired device of the parameters and buffers in this module</li><li><strong>dtype</strong> (<code>tensorplay.dtype</code>): the desired floating point or complex dtype of the parameters and buffers in this module</li><li><strong>tensor</strong> (<code>tensorplay.Tensor</code>): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module</li><li><strong>memory_format</strong> (<code>tensorplay.memory_format</code>): the desired memory format for 4D parameters and buffers in this module (keyword only argument)</li></ul><h4 id="returns-71" tabindex="-1">Returns <a class="header-anchor" href="#returns-71" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="examples-5" tabindex="-1">Examples <a class="header-anchor" href="#examples-5" aria-label="Permalink to &quot;Examples&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +IGNORE_WANT(&quot;non-deterministic&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(tensorplay.double)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float64)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +REQUIRES(env:TENSORPLAY_DOCTEST_CUDA1)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">gpu1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cuda:1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(gpu1, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.half, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">non_blocking</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;cuda:1&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cpu </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cpu&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(cpu)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).to(tensorplay.cdouble)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3741</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,  </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2382</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5593</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.4443</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear(tensorplay.ones(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.cdouble))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br></div></div><hr><h4 id="to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source-2" tabindex="-1"><code>to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source-2" aria-label="Permalink to &quot;`to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119)&quot;">​</a></h4><p>Move the parameters and buffers to the specified device without copying storage.</p><h4 id="args-80" tabindex="-1">Args <a class="header-anchor" href="#args-80" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): The desired device of the parameters and buffers in this module.</li><li><strong>recurse</strong> (<code>bool</code>): Whether parameters and buffers of submodules should be recursively moved to the specified device.</li></ul><h4 id="returns-72" tabindex="-1">Returns <a class="header-anchor" href="#returns-72" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="train-self-mode-bool-true-self-source-2" tabindex="-1"><code>train(self, mode: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#train-self-mode-bool-true-self-source-2" aria-label="Permalink to &quot;`train(self, mode: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786)&quot;">​</a></h4><p>Set the module in training mode.</p><p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e., whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, etc.</p><h4 id="args-81" tabindex="-1">Args <a class="header-anchor" href="#args-81" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>mode</strong> (<code>bool</code>): whether to set training mode (<code>True</code>) or evaluation mode (<code>False</code>). Default: <code>True</code>.</li></ul><h4 id="returns-73" tabindex="-1">Returns <a class="header-anchor" href="#returns-73" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="type-self-dst-type-union-tensorplay-dtype-str-self-source-2" tabindex="-1"><code>type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#type-self-dst-type-union-tensorplay-dtype-str-self-source-2" aria-label="Permalink to &quot;`type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061)&quot;">​</a></h4><p>Casts all parameters and buffers to <code>dst_type</code>.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-82" tabindex="-1">Args <a class="header-anchor" href="#args-82" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>dst_type</strong> (<code>type or string</code>): the desired type</li></ul><h4 id="returns-74" tabindex="-1">Returns <a class="header-anchor" href="#returns-74" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="zero-grad-self-set-to-none-bool-true-none-source-2" tabindex="-1"><code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#zero-grad-self-set-to-none-bool-true-none-source-2" aria-label="Permalink to &quot;`zero_grad(self, set_to_none: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849)&quot;">​</a></h4><p>Reset gradients of all model parameters.</p><p>See similar function under <code>tensorplay.optim.Optimizer</code> for more context.</p><h4 id="args-83" tabindex="-1">Args <a class="header-anchor" href="#args-83" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>set_to_none</strong> (<code>bool</code>): instead of setting to zero, set the grads to None. See <code>tensorplay.optim.Optimizer.zero_grad</code> for details.</li></ul><hr></details><h3 id="class-lazyinstancenorm1d-source" tabindex="-1"><code>class LazyInstanceNorm1d</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/instancenorm.py#L202" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-lazyinstancenorm1d-source" aria-label="Permalink to &quot;`class LazyInstanceNorm1d` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/instancenorm.py#L202)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">LazyInstanceNorm1d(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">eps</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1e-05</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">momentum</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">affine</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">track_running_stats</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>_LazyNormBase</code>, <code>_InstanceNorm</code></p><p>A <code>tensorplay.nn.InstanceNorm1d</code> module with lazy initialization of the <code>num_features</code> argument.</p><p>The <code>num_features</code> argument of the <code>InstanceNorm1d</code> is inferred from the <code>input.size(1)</code>. The attributes that will be lazily initialized are <code>weight</code>, <code>bias</code>, <code>running_mean</code> and <code>running_var</code>.</p><p>Check the <code>tensorplay.nn.modules.lazy.LazyModuleMixin</code> for further documentation on lazy modules and their limitations.</p><h4 id="args-84" tabindex="-1">Args <a class="header-anchor" href="#args-84" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>num_features</strong>: <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.719ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 760 727" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi></math></mjx-assistive-mml></mjx-container> from an expected input of size <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="9.041ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3996.3 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1277,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1721.7,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2481.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2926.3,0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3607.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>N</mi><mo>,</mo><mi>C</mi><mo>,</mo><mi>L</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> or <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="6.026ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2663.7 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1149,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1593.7,0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2274.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>C</mi><mo>,</mo><mi>L</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container></li><li><strong>eps</strong>: a value added to the denominator for numerical stability. Default: 1e-5</li><li><strong>momentum</strong>: the value used for the running_mean and running_var computation. Default: 0.1</li><li><strong>affine</strong>: a boolean value that when set to <code>True</code>, this module has learnable affine parameters, initialized the same way as done for batch normalization.</li><li><strong>Default</strong>: <code>False</code>.</li><li><strong>track_running_stats</strong>: a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: <code>False</code></li></ul><h4 id="shape-3" tabindex="-1">Shape <a class="header-anchor" href="#shape-3" aria-label="Permalink to &quot;Shape&quot;">​</a></h4><ul><li>Input: <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="9.041ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3996.3 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1277,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1721.7,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2481.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2926.3,0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3607.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>N</mi><mo>,</mo><mi>C</mi><mo>,</mo><mi>L</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> or <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="6.026ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2663.7 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1149,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1593.7,0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2274.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>C</mi><mo>,</mo><mi>L</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container></li><li>Output: <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="9.041ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3996.3 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1277,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1721.7,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2481.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2926.3,0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3607.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>N</mi><mo>,</mo><mi>C</mi><mo>,</mo><mi>L</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> or <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="6.026ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2663.7 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1149,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1593.7,0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2274.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>C</mi><mo>,</mo><mi>L</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> (same shape as input)</li></ul><details><summary>Methods</summary><h4 id="init-self-eps-1e-05-momentum-0-1-affine-true-track-running-stats-true-device-none-dtype-none-none-source" tabindex="-1"><code>__init__(self, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L209" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-eps-1e-05-momentum-0-1-affine-true-track-running-stats-true-device-none-dtype-none-none-source" aria-label="Permalink to &quot;`__init__(self, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L209)&quot;">​</a></h4><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p><hr><h4 id="add-module-self-name-str-module-optional-forwardref-module-none-source-3" tabindex="-1"><code>add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#add-module-self-name-str-module-optional-forwardref-module-none-source-3" aria-label="Permalink to &quot;`add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667)&quot;">​</a></h4><p>Add a child module to the current module.</p><p>The module can be accessed as an attribute using the given name.</p><h4 id="args-85" tabindex="-1">Args <a class="header-anchor" href="#args-85" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the child module. The child module can be accessed from this module using the given name</li><li><strong>module</strong> (<code>Module</code>): child module to be added to the module.</li></ul><hr><h4 id="apply-self-fn-callable-forwardref-module-nonetype-self-source-3" tabindex="-1"><code>apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#apply-self-fn-callable-forwardref-module-nonetype-self-source-3" aria-label="Permalink to &quot;`apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990)&quot;">​</a></h4><p>Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self.</p><p>Typical use includes initializing the parameters of a model (see also <code>nn-init-doc</code>).</p><h4 id="args-86" tabindex="-1">Args <a class="header-anchor" href="#args-86" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>fn</strong> (``Module<code> -&gt; None</code>): function to be applied to each submodule</li></ul><h4 id="returns-75" tabindex="-1">Returns <a class="header-anchor" href="#returns-75" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="example-30" tabindex="-1">Example <a class="header-anchor" href="#example-30" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@tensorplay.no_grad</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> init_weights</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        m.weight.fill_(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m.weight)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net.apply(init_weights)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><hr><h4 id="bfloat16-self-self-source-3" tabindex="-1"><code>bfloat16(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#bfloat16-self-self-source-3" aria-label="Permalink to &quot;`bfloat16(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-76" tabindex="-1">Returns <a class="header-anchor" href="#returns-76" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source-3" tabindex="-1"><code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source-3" aria-label="Permalink to &quot;`buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627)&quot;">​</a></h4><p>Return an iterator over module buffers.</p><h4 id="args-87" tabindex="-1">Args <a class="header-anchor" href="#args-87" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.</li></ul><h4 id="yields-24" tabindex="-1">Yields <a class="header-anchor" href="#yields-24" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>tensorplay.Tensor: module buffer
</code></pre><h4 id="example-31" tabindex="-1">Example <a class="header-anchor" href="#example-31" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.buffers():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf), buf.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="children-self-collections-abc-iterator-module-source-3" tabindex="-1"><code>children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#children-self-collections-abc-iterator-module-source-3" aria-label="Permalink to &quot;`children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681)&quot;">​</a></h4><p>Return an iterator over immediate children modules.</p><h4 id="yields-25" tabindex="-1">Yields <a class="header-anchor" href="#yields-25" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a child module</li></ul><hr><h4 id="compile-self-args-kwargs-source-3" tabindex="-1"><code>compile(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#compile-self-args-kwargs-source-3" aria-label="Permalink to &quot;`compile(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944)&quot;">​</a></h4><p>Compile this Module&#39;s forward using <code>tensorplay.compile</code>.</p><p>This Module&#39;s <code>__call__</code> method is compiled and all arguments are passed as-is to <code>tensorplay.compile</code>.</p><p>See <code>tensorplay.compile</code> for details on the arguments for this function.</p><hr><h4 id="cpu-self-self-source-3" tabindex="-1"><code>cpu(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cpu-self-self-source-3" aria-label="Permalink to &quot;`cpu(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050)&quot;">​</a></h4><p>Move all model parameters and buffers to the CPU.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-77" tabindex="-1">Returns <a class="header-anchor" href="#returns-77" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source-3" tabindex="-1"><code>cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source-3" aria-label="Permalink to &quot;`cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031)&quot;">​</a></h4><p>Move all model parameters and buffers to the GPU.</p><p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on GPU while being optimized.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-88" tabindex="-1">Args <a class="header-anchor" href="#args-88" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>int, optional</code>): if specified, all parameters will be copied to that device</li></ul><h4 id="returns-78" tabindex="-1">Returns <a class="header-anchor" href="#returns-78" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="double-self-self-source-3" tabindex="-1"><code>double(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#double-self-self-source-3" aria-label="Permalink to &quot;`double(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-79" tabindex="-1">Returns <a class="header-anchor" href="#returns-79" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="eval-self-self-source-3" tabindex="-1"><code>eval(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#eval-self-self-source-3" aria-label="Permalink to &quot;`eval(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808)&quot;">​</a></h4><p>Set the module in evaluation mode.</p><p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e. whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, etc.</p><p>This is equivalent with <code>self.train(False) &lt;tensorplay.nn.Module.train&gt;</code>.</p><p>See <code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.</p><h4 id="returns-80" tabindex="-1">Returns <a class="header-anchor" href="#returns-80" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="extra-repr-self-source-3" tabindex="-1"><code>extra_repr(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L97" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#extra-repr-self-source-3" aria-label="Permalink to &quot;`extra_repr(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L97)&quot;">​</a></h4><p>Return the extra representation of the module.</p><p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p><hr><h4 id="float-self-self-source-3" tabindex="-1"><code>float(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#float-self-self-source-3" aria-label="Permalink to &quot;`float(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-81" tabindex="-1">Returns <a class="header-anchor" href="#returns-81" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="forward-self-input-tensorplay-c-tensorbase-tensorplay-c-tensorbase-source-3" tabindex="-1"><code>forward(self, input: tensorplay._C.TensorBase) -&gt; tensorplay._C.TensorBase</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/instancenorm.py#L102" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#forward-self-input-tensorplay-c-tensorbase-tensorplay-c-tensorbase-source-3" aria-label="Permalink to &quot;`forward(self, input: tensorplay._C.TensorBase) -&gt; tensorplay._C.TensorBase` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/instancenorm.py#L102)&quot;">​</a></h4><p>Define the computation performed at every call.</p><p>Should be overridden by all subclasses.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code> instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.</p></div><hr><h4 id="get-buffer-self-target-str-tensor-source-3" tabindex="-1"><code>get_buffer(self, target: str) -&gt; &#39;Tensor&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-buffer-self-target-str-tensor-source-3" aria-label="Permalink to &quot;`get_buffer(self, target: str) -&gt; &#39;Tensor&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881)&quot;">​</a></h4><p>Return the buffer given by <code>target</code> if it exists, otherwise throw an error.</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-89" tabindex="-1">Args <a class="header-anchor" href="#args-89" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the buffer to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-82" tabindex="-1">Returns <a class="header-anchor" href="#returns-82" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.Tensor: The buffer referenced by ``target``
</code></pre><h4 id="raises-12" tabindex="-1">Raises <a class="header-anchor" href="#raises-12" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not a buffer</li></ul><hr><h4 id="get-extra-state-self-any-source-3" tabindex="-1"><code>get_extra_state(self) -&gt; Any</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-extra-state-self-any-source-3" aria-label="Permalink to &quot;`get_extra_state(self) -&gt; Any` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917)&quot;">​</a></h4><p>Return any extra state to include in the module&#39;s state_dict.</p><p>Implement this and a corresponding <code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module&#39;s <code>state_dict()</code>.</p><p>Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.</p><h4 id="returns-83" tabindex="-1">Returns <a class="header-anchor" href="#returns-83" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>object</strong>: Any extra state to store in the module&#39;s state_dict</li></ul><hr><h4 id="get-parameter-self-target-str-parameter-source-3" tabindex="-1"><code>get_parameter(self, target: str) -&gt; &#39;Parameter&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-parameter-self-target-str-parameter-source-3" aria-label="Permalink to &quot;`get_parameter(self, target: str) -&gt; &#39;Parameter&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845)&quot;">​</a></h4><p>Return the parameter given by <code>target</code> if it exists, otherwise throw an error.</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-90" tabindex="-1">Args <a class="header-anchor" href="#args-90" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the Parameter to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-84" tabindex="-1">Returns <a class="header-anchor" href="#returns-84" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Parameter: The Parameter referenced by ``target``
</code></pre><h4 id="raises-13" tabindex="-1">Raises <a class="header-anchor" href="#raises-13" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not an <code>nn.Parameter</code></li></ul><hr><h4 id="get-submodule-self-target-str-module-source-3" tabindex="-1"><code>get_submodule(self, target: str) -&gt; &#39;Module&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-submodule-self-target-str-module-source-3" aria-label="Permalink to &quot;`get_submodule(self, target: str) -&gt; &#39;Module&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699)&quot;">​</a></h4><p>Return the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> which has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(&quot;net_b.linear&quot;)</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(&quot;net_b.net_c.conv&quot;)</code>.</p><p>The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.</p><h4 id="args-91" tabindex="-1">Args <a class="header-anchor" href="#args-91" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li></ul><h4 id="returns-85" tabindex="-1">Returns <a class="header-anchor" href="#returns-85" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Module: The submodule referenced by ``target``
</code></pre><h4 id="raises-14" tabindex="-1">Raises <a class="header-anchor" href="#raises-14" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If at any point along the path resulting from the target string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="half-self-self-source-3" tabindex="-1"><code>half(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#half-self-self-source-3" aria-label="Permalink to &quot;`half(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-86" tabindex="-1">Returns <a class="header-anchor" href="#returns-86" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="has-uninitialized-params-self-tensorplay-nn-modules-lazy-lazyprotocol-source" tabindex="-1"><code>has_uninitialized_params(self: tensorplay.nn.modules.lazy._LazyProtocol)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/lazy.py#L236" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#has-uninitialized-params-self-tensorplay-nn-modules-lazy-lazyprotocol-source" aria-label="Permalink to &quot;`has_uninitialized_params(self: tensorplay.nn.modules.lazy._LazyProtocol)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/lazy.py#L236)&quot;">​</a></h4><p>Check if a module has parameters that are not initialized.</p><hr><h4 id="initialize-parameters-self-input-none-source" tabindex="-1"><code>initialize_parameters(self, input) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L247" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#initialize-parameters-self-input-none-source" aria-label="Permalink to &quot;`initialize_parameters(self, input) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L247)&quot;">​</a></h4><p>Initialize parameters according to the input batch properties.</p><p>This adds an interface to isolate parameter initialization from the forward pass when doing parameter shape inference.</p><hr><h4 id="load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source-3" tabindex="-1"><code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source-3" aria-label="Permalink to &quot;`load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436)&quot;">​</a></h4><p>Copy parameters and buffers from <code>state_dict</code> into this module and its descendants.</p><p>If <code>strict</code> is <code>True</code>, then the keys of <code>state_dict</code> must exactly match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function.</p><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>If <code>assign</code> is <code>True</code> the optimizer must be created after the call to <code>load_state_dict</code> unless <code>~tensorplay.__future__.get_swap_module_params_on_conversion</code> is <code>True</code>.</p></div><h4 id="args-92" tabindex="-1">Args <a class="header-anchor" href="#args-92" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state_dict</strong> (<code>dict</code>): a dict containing parameters and persistent buffers.</li><li><strong>strict</strong> (<code>bool, optional</code>): whether to strictly enforce that the keys in <code>state_dict</code> match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function. Default: <code>True</code></li><li><strong>assign</strong> (<code>bool, optional</code>): When set to <code>False</code>, the properties of the tensors in the current module are preserved whereas setting it to <code>True</code> preserves properties of the Tensors in the state dict. The only exception is the <code>requires_grad</code> field of <code>~tensorplay.nn.Parameter</code> for which the value from the module is preserved. Default: <code>False</code></li></ul><h4 id="returns-87" tabindex="-1">Returns <a class="header-anchor" href="#returns-87" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:
    * ``missing_keys`` is a list of str containing any keys that are expected
        by this module but missing from the provided ``state_dict``.
    * ``unexpected_keys`` is a list of str containing the keys that are not
        expected by this module but present in the provided ``state_dict``.
</code></pre><h4 id="note-9" tabindex="-1">Note <a class="header-anchor" href="#note-9" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>If a parameter or buffer is registered as ``None`` and its corresponding key
exists in `state_dict`, `load_state_dict` will raise a
``RuntimeError``.
</code></pre><hr><h4 id="modules-self-collections-abc-iterator-module-source-3" tabindex="-1"><code>modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#modules-self-collections-abc-iterator-module-source-3" aria-label="Permalink to &quot;`modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710)&quot;">​</a></h4><p>Return an iterator over all modules in the network.</p><h4 id="yields-26" tabindex="-1">Yields <a class="header-anchor" href="#yields-26" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a module in the network</li></ul><h4 id="note-10" tabindex="-1">Note <a class="header-anchor" href="#note-10" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-32" tabindex="-1">Example <a class="header-anchor" href="#example-32" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source-3" tabindex="-1"><code>named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source-3" aria-label="Permalink to &quot;`named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650)&quot;">​</a></h4><p>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p><h4 id="args-93" tabindex="-1">Args <a class="header-anchor" href="#args-93" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all buffer names.</li><li><strong>recurse</strong> (<code>bool, optional</code>): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Defaults to True.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated buffers in the result. Defaults to True.</li></ul><h4 id="yields-27" tabindex="-1">Yields <a class="header-anchor" href="#yields-27" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, tensorplay.Tensor): Tuple containing the name and buffer
</code></pre><h4 id="example-33" tabindex="-1">Example <a class="header-anchor" href="#example-33" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_buffers():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_var&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-children-self-collections-abc-iterator-tuple-str-module-source-3" tabindex="-1"><code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-children-self-collections-abc-iterator-tuple-str-module-source-3" aria-label="Permalink to &quot;`named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690)&quot;">​</a></h4><p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p><h4 id="yields-28" tabindex="-1">Yields <a class="header-anchor" href="#yields-28" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple containing a name and child module
</code></pre><h4 id="example-34" tabindex="-1">Example <a class="header-anchor" href="#example-34" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, module </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.named_children():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv4&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv5&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(module)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source-3" tabindex="-1"><code>named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source-3" aria-label="Permalink to &quot;`named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737)&quot;">​</a></h4><p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p><h4 id="args-94" tabindex="-1">Args <a class="header-anchor" href="#args-94" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>memo</strong>: a memo to store the set of modules already added to the result</li><li><strong>prefix</strong>: a prefix that will be added to the name of the module</li><li><strong>remove_duplicate</strong>: whether to remove the duplicated module instances in the result or not</li></ul><h4 id="yields-29" tabindex="-1">Yields <a class="header-anchor" href="#yields-29" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple of name and module
</code></pre><h4 id="note-11" tabindex="-1">Note <a class="header-anchor" href="#note-11" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-35" tabindex="-1">Example <a class="header-anchor" href="#example-35" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.named_modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;0&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source-3" tabindex="-1"><code>named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source-3" aria-label="Permalink to &quot;`named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595)&quot;">​</a></h4><p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p><h4 id="args-95" tabindex="-1">Args <a class="header-anchor" href="#args-95" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all parameter names.</li><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated parameters in the result. Defaults to True.</li></ul><h4 id="yields-30" tabindex="-1">Yields <a class="header-anchor" href="#yields-30" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Parameter): Tuple containing the name and parameter
</code></pre><h4 id="example-36" tabindex="-1">Example <a class="header-anchor" href="#example-36" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_parameters():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source-3" tabindex="-1"><code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source-3" aria-label="Permalink to &quot;`parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570)&quot;">​</a></h4><p>Return an iterator over module parameters.</p><p>This is typically passed to an optimizer.</p><h4 id="args-96" tabindex="-1">Args <a class="header-anchor" href="#args-96" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</li></ul><h4 id="yields-31" tabindex="-1">Yields <a class="header-anchor" href="#yields-31" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Parameter</strong>: module parameter</li></ul><h4 id="example-37" tabindex="-1">Example <a class="header-anchor" href="#example-37" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.parameters():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param), param.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source-3" tabindex="-1"><code>register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source-3" aria-label="Permalink to &quot;`register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>This function is deprecated in favor of <code>~tensorplay.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.</p><h4 id="returns-88" tabindex="-1">Returns <a class="header-anchor" href="#returns-88" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source-3" tabindex="-1"><code>register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source-3" aria-label="Permalink to &quot;`register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553)&quot;">​</a></h4><p>Add a buffer to the module.</p><p>This is typically used to register a buffer that should not be considered a model parameter. For example, BatchNorm&#39;s <code>running_mean</code> is not a parameter, but is part of the module&#39;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting <code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&#39;s <code>state_dict</code>.</p><p>Buffers can be accessed as attributes using given names.</p><h4 id="args-97" tabindex="-1">Args <a class="header-anchor" href="#args-97" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the buffer. The buffer can be accessed from this module using the given name</li><li><strong>tensor</strong> (<code>Tensor or None</code>): buffer to be registered. If <code>None</code>, then operations that run on buffers, such as <code>cuda</code>, are ignored. If <code>None</code>, the buffer is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li><li><strong>persistent</strong> (<code>bool</code>): whether the buffer is part of this module&#39;s <code>state_dict</code>.</li></ul><h4 id="example-38" tabindex="-1">Example <a class="header-anchor" href="#example-38" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.register_buffer(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_mean&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, tensorplay.zeros(num_features))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><hr><h4 id="register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source-3" tabindex="-1"><code>register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source-3" aria-label="Permalink to &quot;`register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592)&quot;">​</a></h4><p>Register a forward hook on the module.</p><p>The hook will be called every time after <code>forward</code> has computed an output.</p><p>If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after <code>forward</code> is called. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-98" tabindex="-1">Args <a class="header-anchor" href="#args-98" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If <code>True</code>, the provided <code>hook</code> will be fired before all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward</code> hooks registered with <code>register_module_forward_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If <code>True</code>, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>always_call</strong> (<code>bool</code>): If <code>True</code> the <code>hook</code> will be run regardless of whether an exception is raised while calling the Module.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-89" tabindex="-1">Returns <a class="header-anchor" href="#returns-89" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source-3" tabindex="-1"><code>register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source-3" aria-label="Permalink to &quot;`register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526)&quot;">​</a></h4><p>Register a forward pre-hook on the module.</p><p>The hook will be called every time before <code>forward</code> is invoked.</p><p>If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> of modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> and</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> kwargs</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-99" tabindex="-1">Args <a class="header-anchor" href="#args-99" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward_pre</code> hooks registered with <code>register_module_forward_pre_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If true, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-90" tabindex="-1">Returns <a class="header-anchor" href="#returns-90" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-3" tabindex="-1"><code>register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-3" aria-label="Permalink to &quot;`register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>The hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:</p><pre><code>1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.
2. If none of the module inputs require gradients, the hook will fire when the gradients are computed
   with respect to module outputs.
3. If none of the module outputs require gradients, then the hooks will not fire.
</code></pre><p>The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_input, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Tensor) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_input</code> and <code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of <code>grad_input</code> in subsequent computations. <code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in <code>grad_input</code> and <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> outputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-100" tabindex="-1">Args <a class="header-anchor" href="#args-100" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward</code> hooks registered with <code>register_module_full_backward_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-91" tabindex="-1">Returns <a class="header-anchor" href="#returns-91" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-3" tabindex="-1"><code>register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-3" aria-label="Permalink to &quot;`register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287)&quot;">​</a></h4><p>Register a backward pre-hook on the module.</p><p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tuple[Tensor] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of <code>grad_output</code> in subsequent computations. Entries in <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-101" tabindex="-1">Args <a class="header-anchor" href="#args-101" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward_pre</code> hooks registered with <code>register_module_full_backward_pre_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-92" tabindex="-1">Returns <a class="header-anchor" href="#returns-92" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-post-hook-self-hook-source-3" tabindex="-1"><code>register_load_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-post-hook-self-hook-source-3" aria-label="Permalink to &quot;`register_load_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226)&quot;">​</a></h4><p>Register a post-hook to be run after module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, incompatible_keys) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.</p><p>The given incompatible_keys can be modified inplace if needed.</p><p>Note that the checks performed when calling <code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.</p><h4 id="returns-93" tabindex="-1">Returns <a class="header-anchor" href="#returns-93" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-pre-hook-self-hook-source-3" tabindex="-1"><code>register_load_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-pre-hook-self-hook-source-3" aria-label="Permalink to &quot;`register_load_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214)&quot;">​</a></h4><p>Register a pre-hook to be run before module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # noqa: B950</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="arguments-3" tabindex="-1">Arguments <a class="header-anchor" href="#arguments-3" aria-label="Permalink to &quot;Arguments&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): Callable hook that will be invoked before loading the state dict.</li></ul><hr><h4 id="register-module-self-name-str-module-optional-forwardref-module-none-source-3" tabindex="-1"><code>register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-module-self-name-str-module-optional-forwardref-module-none-source-3" aria-label="Permalink to &quot;`register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695)&quot;">​</a></h4><p>Alias for <code>add_module</code>.</p><hr><h4 id="register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source-3" tabindex="-1"><code>register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source-3" aria-label="Permalink to &quot;`register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617)&quot;">​</a></h4><p>Add a parameter to the module.</p><p>The parameter can be accessed as an attribute using given name.</p><h4 id="args-102" tabindex="-1">Args <a class="header-anchor" href="#args-102" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the parameter. The parameter can be accessed from this module using the given name</li><li><strong>param</strong> (<code>Parameter or None</code>): parameter to be added to the module. If <code>None</code>, then operations that run on parameters, such as <code>cuda</code>, are ignored. If <code>None</code>, the parameter is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li></ul><hr><h4 id="register-state-dict-post-hook-self-hook-source-3" tabindex="-1"><code>register_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-post-hook-self-hook-source-3" aria-label="Permalink to &quot;`register_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017)&quot;">​</a></h4><p>Register a post-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can modify the <code>state_dict</code> inplace.</p><hr><h4 id="register-state-dict-pre-hook-self-hook-source-3" tabindex="-1"><code>register_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-pre-hook-self-hook-source-3" aria-label="Permalink to &quot;`register_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041)&quot;">​</a></h4><p>Register a pre-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, prefix, keep_vars) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p><hr><h4 id="requires-grad-self-requires-grad-bool-true-self-source-3" tabindex="-1"><code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#requires-grad-self-requires-grad-bool-true-self-source-3" aria-label="Permalink to &quot;`requires_grad_(self, requires_grad: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826)&quot;">​</a></h4><p>Change if autograd should record operations on parameters in this module.</p><p>This method sets the parameters&#39; <code>requires_grad</code> attributes in-place.</p><p>This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).</p><p>See <code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.</p><h4 id="args-103" tabindex="-1">Args <a class="header-anchor" href="#args-103" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>requires_grad</strong> (<code>bool</code>): whether autograd should record operations on parameters in this module. Default: <code>True</code>.</li></ul><h4 id="returns-94" tabindex="-1">Returns <a class="header-anchor" href="#returns-94" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="reset-parameters-self-none-source-3" tabindex="-1"><code>reset_parameters(self) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L243" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#reset-parameters-self-none-source-3" aria-label="Permalink to &quot;`reset_parameters(self) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L243)&quot;">​</a></h4><hr><h4 id="reset-running-stats-self-none-source-3" tabindex="-1"><code>reset_running_stats(self) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L80" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#reset-running-stats-self-none-source-3" aria-label="Permalink to &quot;`reset_running_stats(self) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L80)&quot;">​</a></h4><hr><h4 id="set-extra-state-self-state-any-none-source-3" tabindex="-1"><code>set_extra_state(self, state: Any) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-extra-state-self-state-any-none-source-3" aria-label="Permalink to &quot;`set_extra_state(self, state: Any) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938)&quot;">​</a></h4><p>Set extra state contained in the loaded <code>state_dict</code>.</p><p>This function is called from <code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding <code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.</p><h4 id="args-104" tabindex="-1">Args <a class="header-anchor" href="#args-104" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state</strong> (<code>dict</code>): Extra state from the <code>state_dict</code></li></ul><hr><h4 id="set-submodule-self-target-str-module-module-strict-bool-false-none-source-3" tabindex="-1"><code>set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-submodule-self-target-str-module-module-strict-bool-false-none-source-3" aria-label="Permalink to &quot;`set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764)&quot;">​</a></h4><p>Set the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>If <code>strict</code> is set to <code>False</code> (default), the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>strict</code> is set to <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule does not exist.</p></div><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(3, 3, 3) ) (linear): Linear(3, 3) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To override the <code>Conv2d</code> with a new submodule <code>Linear</code>, you could call <code>set_submodule(&quot;net_b.net_c.conv&quot;, nn.Linear(1, 1))</code> where <code>strict</code> could be <code>True</code> or <code>False</code></p><p>To add a new submodule <code>Conv2d</code> to the existing <code>net_b</code> module, you would call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1))</code>.</p><p>In the above if you set <code>strict=True</code> and call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1), strict=True)</code>, an AttributeError will be raised because <code>net_b</code> does not have a submodule named <code>conv</code>.</p><h4 id="args-105" tabindex="-1">Args <a class="header-anchor" href="#args-105" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li><li><strong>module</strong>: The module to set the submodule to.</li><li><strong>strict</strong>: If <code>False</code>, the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule doesn&#39;t already exist.</li></ul><h4 id="raises-15" tabindex="-1">Raises <a class="header-anchor" href="#raises-15" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>ValueError</strong>: If the <code>target</code> string is empty or if <code>module</code> is not an instance of <code>nn.Module</code>.</li><li><strong>AttributeError</strong>: If at any point along the path resulting from the <code>target</code> string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="share-memory-self-self-source-3" tabindex="-1"><code>share_memory(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#share-memory-self-self-source-3" aria-label="Permalink to &quot;`share_memory(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877)&quot;">​</a></h4><p>See <code>tensorplay.Tensor.share_memory_</code>.</p><hr><h4 id="state-dict-self-args-destination-none-prefix-keep-vars-false-source-3" tabindex="-1"><code>state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#state-dict-self-args-destination-none-prefix-keep-vars-false-source-3" aria-label="Permalink to &quot;`state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105)&quot;">​</a></h4><p>Return a dictionary containing references to the whole state of the module.</p><p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>The returned object is a shallow copy. It contains references to the module&#39;s parameters and buffers.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Currently <code>state_dict()</code> also accepts positional arguments for <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However, this is being deprecated and keyword arguments will be enforced in future releases.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Please avoid the use of argument <code>destination</code> as it is not designed for end-users.</p></div><h4 id="args-106" tabindex="-1">Args <a class="header-anchor" href="#args-106" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>destination</strong> (<code>dict, optional</code>): If provided, the state of module will be updated into the dict and the same object is returned. Otherwise, an <code>OrderedDict</code> will be created and returned.</li><li><strong>Default</strong>: <code>None</code>.</li><li><strong>prefix</strong> (<code>str, optional</code>): a prefix added to parameter and buffer names to compose the keys in state_dict. Default: <code>&#39;&#39;</code>.</li><li><strong>keep_vars</strong> (<code>bool, optional</code>): by default the <code>~tensorplay.Tensor</code> s returned in the state dict are detached from autograd. If it&#39;s set to <code>True</code>, detaching will not be performed.</li><li><strong>Default</strong>: <code>False</code>.</li></ul><h4 id="returns-95" tabindex="-1">Returns <a class="header-anchor" href="#returns-95" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>dict</strong>: a dictionary containing a whole state of the module</li></ul><h4 id="example-39" tabindex="-1">Example <a class="header-anchor" href="#example-39" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">module.state_dict().keys()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;weight&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><hr><h4 id="to-self-args-kwargs-source-3" tabindex="-1"><code>to(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-self-args-kwargs-source-3" aria-label="Permalink to &quot;`to(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151)&quot;">​</a></h4><p>Move and/or cast the parameters and buffers.</p><p>This can be called as</p><p>.. function:: to(device=None, dtype=None, non_blocking=False) :noindex:</p><p>.. function:: to(dtype, non_blocking=False) :noindex:</p><p>.. function:: to(tensor, non_blocking=False) :noindex:</p><p>.. function:: to(memory_format=tensorplay.channels_last) :noindex:</p><p>Its signature is similar to <code>tensorplay.Tensor.to</code>, but only accepts floating point or complex <code>dtype</code>\ s. In addition, this method will only cast the floating point or complex parameters and buffers to <code>dtype</code> (if given). The integral parameters and buffers will be moved <code>device</code>, if that is given, but with dtypes unchanged. When <code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.</p><p>See below for examples.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-107" tabindex="-1">Args <a class="header-anchor" href="#args-107" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): the desired device of the parameters and buffers in this module</li><li><strong>dtype</strong> (<code>tensorplay.dtype</code>): the desired floating point or complex dtype of the parameters and buffers in this module</li><li><strong>tensor</strong> (<code>tensorplay.Tensor</code>): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module</li><li><strong>memory_format</strong> (<code>tensorplay.memory_format</code>): the desired memory format for 4D parameters and buffers in this module (keyword only argument)</li></ul><h4 id="returns-96" tabindex="-1">Returns <a class="header-anchor" href="#returns-96" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="examples-6" tabindex="-1">Examples <a class="header-anchor" href="#examples-6" aria-label="Permalink to &quot;Examples&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +IGNORE_WANT(&quot;non-deterministic&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(tensorplay.double)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float64)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +REQUIRES(env:TENSORPLAY_DOCTEST_CUDA1)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">gpu1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cuda:1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(gpu1, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.half, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">non_blocking</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;cuda:1&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cpu </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cpu&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(cpu)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).to(tensorplay.cdouble)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3741</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,  </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2382</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5593</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.4443</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear(tensorplay.ones(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.cdouble))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br></div></div><hr><h4 id="to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source-3" tabindex="-1"><code>to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source-3" aria-label="Permalink to &quot;`to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119)&quot;">​</a></h4><p>Move the parameters and buffers to the specified device without copying storage.</p><h4 id="args-108" tabindex="-1">Args <a class="header-anchor" href="#args-108" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): The desired device of the parameters and buffers in this module.</li><li><strong>recurse</strong> (<code>bool</code>): Whether parameters and buffers of submodules should be recursively moved to the specified device.</li></ul><h4 id="returns-97" tabindex="-1">Returns <a class="header-anchor" href="#returns-97" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="train-self-mode-bool-true-self-source-3" tabindex="-1"><code>train(self, mode: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#train-self-mode-bool-true-self-source-3" aria-label="Permalink to &quot;`train(self, mode: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786)&quot;">​</a></h4><p>Set the module in training mode.</p><p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e., whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, etc.</p><h4 id="args-109" tabindex="-1">Args <a class="header-anchor" href="#args-109" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>mode</strong> (<code>bool</code>): whether to set training mode (<code>True</code>) or evaluation mode (<code>False</code>). Default: <code>True</code>.</li></ul><h4 id="returns-98" tabindex="-1">Returns <a class="header-anchor" href="#returns-98" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="type-self-dst-type-union-tensorplay-dtype-str-self-source-3" tabindex="-1"><code>type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#type-self-dst-type-union-tensorplay-dtype-str-self-source-3" aria-label="Permalink to &quot;`type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061)&quot;">​</a></h4><p>Casts all parameters and buffers to <code>dst_type</code>.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-110" tabindex="-1">Args <a class="header-anchor" href="#args-110" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>dst_type</strong> (<code>type or string</code>): the desired type</li></ul><h4 id="returns-99" tabindex="-1">Returns <a class="header-anchor" href="#returns-99" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="zero-grad-self-set-to-none-bool-true-none-source-3" tabindex="-1"><code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#zero-grad-self-set-to-none-bool-true-none-source-3" aria-label="Permalink to &quot;`zero_grad(self, set_to_none: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849)&quot;">​</a></h4><p>Reset gradients of all model parameters.</p><p>See similar function under <code>tensorplay.optim.Optimizer</code> for more context.</p><h4 id="args-111" tabindex="-1">Args <a class="header-anchor" href="#args-111" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>set_to_none</strong> (<code>bool</code>): instead of setting to zero, set the grads to None. See <code>tensorplay.optim.Optimizer.zero_grad</code> for details.</li></ul><hr></details><h3 id="class-lazyinstancenorm2d-source" tabindex="-1"><code>class LazyInstanceNorm2d</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/instancenorm.py#L318" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-lazyinstancenorm2d-source" aria-label="Permalink to &quot;`class LazyInstanceNorm2d` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/instancenorm.py#L318)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">LazyInstanceNorm2d(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">eps</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1e-05</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">momentum</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">affine</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">track_running_stats</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>_LazyNormBase</code>, <code>_InstanceNorm</code></p><p>A <code>tensorplay.nn.InstanceNorm2d</code> module with lazy initialization of the <code>num_features</code> argument.</p><p>The <code>num_features</code> argument of the <code>InstanceNorm2d</code> is inferred from the <code>input.size(1)</code>. The attributes that will be lazily initialized are <code>weight</code>, <code>bias</code>, <code>running_mean</code> and <code>running_var</code>.</p><p>Check the <code>tensorplay.nn.modules.lazy.LazyModuleMixin</code> for further documentation on lazy modules and their limitations.</p><h4 id="args-112" tabindex="-1">Args <a class="header-anchor" href="#args-112" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>num_features</strong>: <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.719ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 760 727" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi></math></mjx-assistive-mml></mjx-container> from an expected input of size <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="12.887ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 5696 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1277,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1721.7,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2481.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2926.3,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3814.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(4259,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5307,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>N</mi><mo>,</mo><mi>C</mi><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> or <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="9.872ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4363.3 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1149,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1593.7,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2481.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2926.3,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3974.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>C</mi><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container></li><li><strong>eps</strong>: a value added to the denominator for numerical stability. Default: 1e-5</li><li><strong>momentum</strong>: the value used for the running_mean and running_var computation. Default: 0.1</li><li><strong>affine</strong>: a boolean value that when set to <code>True</code>, this module has learnable affine parameters, initialized the same way as done for batch normalization.</li><li><strong>Default</strong>: <code>False</code>.</li><li><strong>track_running_stats</strong>: a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: <code>False</code></li></ul><h4 id="shape-4" tabindex="-1">Shape <a class="header-anchor" href="#shape-4" aria-label="Permalink to &quot;Shape&quot;">​</a></h4><ul><li>Input: <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="12.887ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 5696 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1277,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1721.7,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2481.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2926.3,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3814.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(4259,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5307,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>N</mi><mo>,</mo><mi>C</mi><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> or <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="9.872ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4363.3 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1149,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1593.7,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2481.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2926.3,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3974.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>C</mi><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container></li><li>Output: <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="12.887ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 5696 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1277,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1721.7,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2481.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2926.3,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3814.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(4259,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5307,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>N</mi><mo>,</mo><mi>C</mi><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> or <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="9.872ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4363.3 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1149,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1593.7,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2481.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2926.3,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3974.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>C</mi><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> (same shape as input)</li></ul><details><summary>Methods</summary><h4 id="init-self-eps-1e-05-momentum-0-1-affine-true-track-running-stats-true-device-none-dtype-none-none-source-1" tabindex="-1"><code>__init__(self, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L209" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-eps-1e-05-momentum-0-1-affine-true-track-running-stats-true-device-none-dtype-none-none-source-1" aria-label="Permalink to &quot;`__init__(self, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L209)&quot;">​</a></h4><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p><hr><h4 id="add-module-self-name-str-module-optional-forwardref-module-none-source-4" tabindex="-1"><code>add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#add-module-self-name-str-module-optional-forwardref-module-none-source-4" aria-label="Permalink to &quot;`add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667)&quot;">​</a></h4><p>Add a child module to the current module.</p><p>The module can be accessed as an attribute using the given name.</p><h4 id="args-113" tabindex="-1">Args <a class="header-anchor" href="#args-113" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the child module. The child module can be accessed from this module using the given name</li><li><strong>module</strong> (<code>Module</code>): child module to be added to the module.</li></ul><hr><h4 id="apply-self-fn-callable-forwardref-module-nonetype-self-source-4" tabindex="-1"><code>apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#apply-self-fn-callable-forwardref-module-nonetype-self-source-4" aria-label="Permalink to &quot;`apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990)&quot;">​</a></h4><p>Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self.</p><p>Typical use includes initializing the parameters of a model (see also <code>nn-init-doc</code>).</p><h4 id="args-114" tabindex="-1">Args <a class="header-anchor" href="#args-114" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>fn</strong> (``Module<code> -&gt; None</code>): function to be applied to each submodule</li></ul><h4 id="returns-100" tabindex="-1">Returns <a class="header-anchor" href="#returns-100" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="example-40" tabindex="-1">Example <a class="header-anchor" href="#example-40" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@tensorplay.no_grad</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> init_weights</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        m.weight.fill_(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m.weight)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net.apply(init_weights)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><hr><h4 id="bfloat16-self-self-source-4" tabindex="-1"><code>bfloat16(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#bfloat16-self-self-source-4" aria-label="Permalink to &quot;`bfloat16(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-101" tabindex="-1">Returns <a class="header-anchor" href="#returns-101" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source-4" tabindex="-1"><code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source-4" aria-label="Permalink to &quot;`buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627)&quot;">​</a></h4><p>Return an iterator over module buffers.</p><h4 id="args-115" tabindex="-1">Args <a class="header-anchor" href="#args-115" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.</li></ul><h4 id="yields-32" tabindex="-1">Yields <a class="header-anchor" href="#yields-32" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>tensorplay.Tensor: module buffer
</code></pre><h4 id="example-41" tabindex="-1">Example <a class="header-anchor" href="#example-41" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.buffers():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf), buf.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="children-self-collections-abc-iterator-module-source-4" tabindex="-1"><code>children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#children-self-collections-abc-iterator-module-source-4" aria-label="Permalink to &quot;`children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681)&quot;">​</a></h4><p>Return an iterator over immediate children modules.</p><h4 id="yields-33" tabindex="-1">Yields <a class="header-anchor" href="#yields-33" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a child module</li></ul><hr><h4 id="compile-self-args-kwargs-source-4" tabindex="-1"><code>compile(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#compile-self-args-kwargs-source-4" aria-label="Permalink to &quot;`compile(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944)&quot;">​</a></h4><p>Compile this Module&#39;s forward using <code>tensorplay.compile</code>.</p><p>This Module&#39;s <code>__call__</code> method is compiled and all arguments are passed as-is to <code>tensorplay.compile</code>.</p><p>See <code>tensorplay.compile</code> for details on the arguments for this function.</p><hr><h4 id="cpu-self-self-source-4" tabindex="-1"><code>cpu(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cpu-self-self-source-4" aria-label="Permalink to &quot;`cpu(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050)&quot;">​</a></h4><p>Move all model parameters and buffers to the CPU.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-102" tabindex="-1">Returns <a class="header-anchor" href="#returns-102" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source-4" tabindex="-1"><code>cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source-4" aria-label="Permalink to &quot;`cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031)&quot;">​</a></h4><p>Move all model parameters and buffers to the GPU.</p><p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on GPU while being optimized.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-116" tabindex="-1">Args <a class="header-anchor" href="#args-116" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>int, optional</code>): if specified, all parameters will be copied to that device</li></ul><h4 id="returns-103" tabindex="-1">Returns <a class="header-anchor" href="#returns-103" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="double-self-self-source-4" tabindex="-1"><code>double(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#double-self-self-source-4" aria-label="Permalink to &quot;`double(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-104" tabindex="-1">Returns <a class="header-anchor" href="#returns-104" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="eval-self-self-source-4" tabindex="-1"><code>eval(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#eval-self-self-source-4" aria-label="Permalink to &quot;`eval(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808)&quot;">​</a></h4><p>Set the module in evaluation mode.</p><p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e. whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, etc.</p><p>This is equivalent with <code>self.train(False) &lt;tensorplay.nn.Module.train&gt;</code>.</p><p>See <code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.</p><h4 id="returns-105" tabindex="-1">Returns <a class="header-anchor" href="#returns-105" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="extra-repr-self-source-4" tabindex="-1"><code>extra_repr(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L97" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#extra-repr-self-source-4" aria-label="Permalink to &quot;`extra_repr(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L97)&quot;">​</a></h4><p>Return the extra representation of the module.</p><p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p><hr><h4 id="float-self-self-source-4" tabindex="-1"><code>float(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#float-self-self-source-4" aria-label="Permalink to &quot;`float(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-106" tabindex="-1">Returns <a class="header-anchor" href="#returns-106" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="forward-self-input-tensorplay-c-tensorbase-tensorplay-c-tensorbase-source-4" tabindex="-1"><code>forward(self, input: tensorplay._C.TensorBase) -&gt; tensorplay._C.TensorBase</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/instancenorm.py#L102" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#forward-self-input-tensorplay-c-tensorbase-tensorplay-c-tensorbase-source-4" aria-label="Permalink to &quot;`forward(self, input: tensorplay._C.TensorBase) -&gt; tensorplay._C.TensorBase` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/instancenorm.py#L102)&quot;">​</a></h4><p>Define the computation performed at every call.</p><p>Should be overridden by all subclasses.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code> instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.</p></div><hr><h4 id="get-buffer-self-target-str-tensor-source-4" tabindex="-1"><code>get_buffer(self, target: str) -&gt; &#39;Tensor&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-buffer-self-target-str-tensor-source-4" aria-label="Permalink to &quot;`get_buffer(self, target: str) -&gt; &#39;Tensor&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881)&quot;">​</a></h4><p>Return the buffer given by <code>target</code> if it exists, otherwise throw an error.</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-117" tabindex="-1">Args <a class="header-anchor" href="#args-117" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the buffer to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-107" tabindex="-1">Returns <a class="header-anchor" href="#returns-107" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.Tensor: The buffer referenced by ``target``
</code></pre><h4 id="raises-16" tabindex="-1">Raises <a class="header-anchor" href="#raises-16" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not a buffer</li></ul><hr><h4 id="get-extra-state-self-any-source-4" tabindex="-1"><code>get_extra_state(self) -&gt; Any</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-extra-state-self-any-source-4" aria-label="Permalink to &quot;`get_extra_state(self) -&gt; Any` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917)&quot;">​</a></h4><p>Return any extra state to include in the module&#39;s state_dict.</p><p>Implement this and a corresponding <code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module&#39;s <code>state_dict()</code>.</p><p>Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.</p><h4 id="returns-108" tabindex="-1">Returns <a class="header-anchor" href="#returns-108" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>object</strong>: Any extra state to store in the module&#39;s state_dict</li></ul><hr><h4 id="get-parameter-self-target-str-parameter-source-4" tabindex="-1"><code>get_parameter(self, target: str) -&gt; &#39;Parameter&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-parameter-self-target-str-parameter-source-4" aria-label="Permalink to &quot;`get_parameter(self, target: str) -&gt; &#39;Parameter&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845)&quot;">​</a></h4><p>Return the parameter given by <code>target</code> if it exists, otherwise throw an error.</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-118" tabindex="-1">Args <a class="header-anchor" href="#args-118" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the Parameter to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-109" tabindex="-1">Returns <a class="header-anchor" href="#returns-109" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Parameter: The Parameter referenced by ``target``
</code></pre><h4 id="raises-17" tabindex="-1">Raises <a class="header-anchor" href="#raises-17" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not an <code>nn.Parameter</code></li></ul><hr><h4 id="get-submodule-self-target-str-module-source-4" tabindex="-1"><code>get_submodule(self, target: str) -&gt; &#39;Module&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-submodule-self-target-str-module-source-4" aria-label="Permalink to &quot;`get_submodule(self, target: str) -&gt; &#39;Module&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699)&quot;">​</a></h4><p>Return the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> which has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(&quot;net_b.linear&quot;)</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(&quot;net_b.net_c.conv&quot;)</code>.</p><p>The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.</p><h4 id="args-119" tabindex="-1">Args <a class="header-anchor" href="#args-119" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li></ul><h4 id="returns-110" tabindex="-1">Returns <a class="header-anchor" href="#returns-110" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Module: The submodule referenced by ``target``
</code></pre><h4 id="raises-18" tabindex="-1">Raises <a class="header-anchor" href="#raises-18" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If at any point along the path resulting from the target string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="half-self-self-source-4" tabindex="-1"><code>half(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#half-self-self-source-4" aria-label="Permalink to &quot;`half(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-111" tabindex="-1">Returns <a class="header-anchor" href="#returns-111" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="has-uninitialized-params-self-tensorplay-nn-modules-lazy-lazyprotocol-source-1" tabindex="-1"><code>has_uninitialized_params(self: tensorplay.nn.modules.lazy._LazyProtocol)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/lazy.py#L236" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#has-uninitialized-params-self-tensorplay-nn-modules-lazy-lazyprotocol-source-1" aria-label="Permalink to &quot;`has_uninitialized_params(self: tensorplay.nn.modules.lazy._LazyProtocol)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/lazy.py#L236)&quot;">​</a></h4><p>Check if a module has parameters that are not initialized.</p><hr><h4 id="initialize-parameters-self-input-none-source-1" tabindex="-1"><code>initialize_parameters(self, input) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L247" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#initialize-parameters-self-input-none-source-1" aria-label="Permalink to &quot;`initialize_parameters(self, input) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L247)&quot;">​</a></h4><p>Initialize parameters according to the input batch properties.</p><p>This adds an interface to isolate parameter initialization from the forward pass when doing parameter shape inference.</p><hr><h4 id="load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source-4" tabindex="-1"><code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source-4" aria-label="Permalink to &quot;`load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436)&quot;">​</a></h4><p>Copy parameters and buffers from <code>state_dict</code> into this module and its descendants.</p><p>If <code>strict</code> is <code>True</code>, then the keys of <code>state_dict</code> must exactly match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function.</p><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>If <code>assign</code> is <code>True</code> the optimizer must be created after the call to <code>load_state_dict</code> unless <code>~tensorplay.__future__.get_swap_module_params_on_conversion</code> is <code>True</code>.</p></div><h4 id="args-120" tabindex="-1">Args <a class="header-anchor" href="#args-120" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state_dict</strong> (<code>dict</code>): a dict containing parameters and persistent buffers.</li><li><strong>strict</strong> (<code>bool, optional</code>): whether to strictly enforce that the keys in <code>state_dict</code> match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function. Default: <code>True</code></li><li><strong>assign</strong> (<code>bool, optional</code>): When set to <code>False</code>, the properties of the tensors in the current module are preserved whereas setting it to <code>True</code> preserves properties of the Tensors in the state dict. The only exception is the <code>requires_grad</code> field of <code>~tensorplay.nn.Parameter</code> for which the value from the module is preserved. Default: <code>False</code></li></ul><h4 id="returns-112" tabindex="-1">Returns <a class="header-anchor" href="#returns-112" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:
    * ``missing_keys`` is a list of str containing any keys that are expected
        by this module but missing from the provided ``state_dict``.
    * ``unexpected_keys`` is a list of str containing the keys that are not
        expected by this module but present in the provided ``state_dict``.
</code></pre><h4 id="note-12" tabindex="-1">Note <a class="header-anchor" href="#note-12" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>If a parameter or buffer is registered as ``None`` and its corresponding key
exists in `state_dict`, `load_state_dict` will raise a
``RuntimeError``.
</code></pre><hr><h4 id="modules-self-collections-abc-iterator-module-source-4" tabindex="-1"><code>modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#modules-self-collections-abc-iterator-module-source-4" aria-label="Permalink to &quot;`modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710)&quot;">​</a></h4><p>Return an iterator over all modules in the network.</p><h4 id="yields-34" tabindex="-1">Yields <a class="header-anchor" href="#yields-34" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a module in the network</li></ul><h4 id="note-13" tabindex="-1">Note <a class="header-anchor" href="#note-13" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-42" tabindex="-1">Example <a class="header-anchor" href="#example-42" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source-4" tabindex="-1"><code>named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source-4" aria-label="Permalink to &quot;`named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650)&quot;">​</a></h4><p>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p><h4 id="args-121" tabindex="-1">Args <a class="header-anchor" href="#args-121" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all buffer names.</li><li><strong>recurse</strong> (<code>bool, optional</code>): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Defaults to True.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated buffers in the result. Defaults to True.</li></ul><h4 id="yields-35" tabindex="-1">Yields <a class="header-anchor" href="#yields-35" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, tensorplay.Tensor): Tuple containing the name and buffer
</code></pre><h4 id="example-43" tabindex="-1">Example <a class="header-anchor" href="#example-43" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_buffers():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_var&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-children-self-collections-abc-iterator-tuple-str-module-source-4" tabindex="-1"><code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-children-self-collections-abc-iterator-tuple-str-module-source-4" aria-label="Permalink to &quot;`named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690)&quot;">​</a></h4><p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p><h4 id="yields-36" tabindex="-1">Yields <a class="header-anchor" href="#yields-36" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple containing a name and child module
</code></pre><h4 id="example-44" tabindex="-1">Example <a class="header-anchor" href="#example-44" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, module </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.named_children():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv4&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv5&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(module)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source-4" tabindex="-1"><code>named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source-4" aria-label="Permalink to &quot;`named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737)&quot;">​</a></h4><p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p><h4 id="args-122" tabindex="-1">Args <a class="header-anchor" href="#args-122" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>memo</strong>: a memo to store the set of modules already added to the result</li><li><strong>prefix</strong>: a prefix that will be added to the name of the module</li><li><strong>remove_duplicate</strong>: whether to remove the duplicated module instances in the result or not</li></ul><h4 id="yields-37" tabindex="-1">Yields <a class="header-anchor" href="#yields-37" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple of name and module
</code></pre><h4 id="note-14" tabindex="-1">Note <a class="header-anchor" href="#note-14" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-45" tabindex="-1">Example <a class="header-anchor" href="#example-45" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.named_modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;0&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source-4" tabindex="-1"><code>named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source-4" aria-label="Permalink to &quot;`named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595)&quot;">​</a></h4><p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p><h4 id="args-123" tabindex="-1">Args <a class="header-anchor" href="#args-123" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all parameter names.</li><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated parameters in the result. Defaults to True.</li></ul><h4 id="yields-38" tabindex="-1">Yields <a class="header-anchor" href="#yields-38" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Parameter): Tuple containing the name and parameter
</code></pre><h4 id="example-46" tabindex="-1">Example <a class="header-anchor" href="#example-46" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_parameters():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source-4" tabindex="-1"><code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source-4" aria-label="Permalink to &quot;`parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570)&quot;">​</a></h4><p>Return an iterator over module parameters.</p><p>This is typically passed to an optimizer.</p><h4 id="args-124" tabindex="-1">Args <a class="header-anchor" href="#args-124" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</li></ul><h4 id="yields-39" tabindex="-1">Yields <a class="header-anchor" href="#yields-39" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Parameter</strong>: module parameter</li></ul><h4 id="example-47" tabindex="-1">Example <a class="header-anchor" href="#example-47" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.parameters():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param), param.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source-4" tabindex="-1"><code>register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source-4" aria-label="Permalink to &quot;`register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>This function is deprecated in favor of <code>~tensorplay.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.</p><h4 id="returns-113" tabindex="-1">Returns <a class="header-anchor" href="#returns-113" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source-4" tabindex="-1"><code>register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source-4" aria-label="Permalink to &quot;`register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553)&quot;">​</a></h4><p>Add a buffer to the module.</p><p>This is typically used to register a buffer that should not be considered a model parameter. For example, BatchNorm&#39;s <code>running_mean</code> is not a parameter, but is part of the module&#39;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting <code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&#39;s <code>state_dict</code>.</p><p>Buffers can be accessed as attributes using given names.</p><h4 id="args-125" tabindex="-1">Args <a class="header-anchor" href="#args-125" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the buffer. The buffer can be accessed from this module using the given name</li><li><strong>tensor</strong> (<code>Tensor or None</code>): buffer to be registered. If <code>None</code>, then operations that run on buffers, such as <code>cuda</code>, are ignored. If <code>None</code>, the buffer is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li><li><strong>persistent</strong> (<code>bool</code>): whether the buffer is part of this module&#39;s <code>state_dict</code>.</li></ul><h4 id="example-48" tabindex="-1">Example <a class="header-anchor" href="#example-48" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.register_buffer(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_mean&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, tensorplay.zeros(num_features))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><hr><h4 id="register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source-4" tabindex="-1"><code>register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source-4" aria-label="Permalink to &quot;`register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592)&quot;">​</a></h4><p>Register a forward hook on the module.</p><p>The hook will be called every time after <code>forward</code> has computed an output.</p><p>If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after <code>forward</code> is called. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-126" tabindex="-1">Args <a class="header-anchor" href="#args-126" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If <code>True</code>, the provided <code>hook</code> will be fired before all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward</code> hooks registered with <code>register_module_forward_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If <code>True</code>, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>always_call</strong> (<code>bool</code>): If <code>True</code> the <code>hook</code> will be run regardless of whether an exception is raised while calling the Module.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-114" tabindex="-1">Returns <a class="header-anchor" href="#returns-114" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source-4" tabindex="-1"><code>register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source-4" aria-label="Permalink to &quot;`register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526)&quot;">​</a></h4><p>Register a forward pre-hook on the module.</p><p>The hook will be called every time before <code>forward</code> is invoked.</p><p>If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> of modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> and</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> kwargs</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-127" tabindex="-1">Args <a class="header-anchor" href="#args-127" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward_pre</code> hooks registered with <code>register_module_forward_pre_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If true, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-115" tabindex="-1">Returns <a class="header-anchor" href="#returns-115" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-4" tabindex="-1"><code>register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-4" aria-label="Permalink to &quot;`register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>The hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:</p><pre><code>1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.
2. If none of the module inputs require gradients, the hook will fire when the gradients are computed
   with respect to module outputs.
3. If none of the module outputs require gradients, then the hooks will not fire.
</code></pre><p>The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_input, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Tensor) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_input</code> and <code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of <code>grad_input</code> in subsequent computations. <code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in <code>grad_input</code> and <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> outputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-128" tabindex="-1">Args <a class="header-anchor" href="#args-128" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward</code> hooks registered with <code>register_module_full_backward_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-116" tabindex="-1">Returns <a class="header-anchor" href="#returns-116" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-4" tabindex="-1"><code>register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-4" aria-label="Permalink to &quot;`register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287)&quot;">​</a></h4><p>Register a backward pre-hook on the module.</p><p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tuple[Tensor] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of <code>grad_output</code> in subsequent computations. Entries in <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-129" tabindex="-1">Args <a class="header-anchor" href="#args-129" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward_pre</code> hooks registered with <code>register_module_full_backward_pre_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-117" tabindex="-1">Returns <a class="header-anchor" href="#returns-117" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-post-hook-self-hook-source-4" tabindex="-1"><code>register_load_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-post-hook-self-hook-source-4" aria-label="Permalink to &quot;`register_load_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226)&quot;">​</a></h4><p>Register a post-hook to be run after module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, incompatible_keys) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.</p><p>The given incompatible_keys can be modified inplace if needed.</p><p>Note that the checks performed when calling <code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.</p><h4 id="returns-118" tabindex="-1">Returns <a class="header-anchor" href="#returns-118" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-pre-hook-self-hook-source-4" tabindex="-1"><code>register_load_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-pre-hook-self-hook-source-4" aria-label="Permalink to &quot;`register_load_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214)&quot;">​</a></h4><p>Register a pre-hook to be run before module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # noqa: B950</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="arguments-4" tabindex="-1">Arguments <a class="header-anchor" href="#arguments-4" aria-label="Permalink to &quot;Arguments&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): Callable hook that will be invoked before loading the state dict.</li></ul><hr><h4 id="register-module-self-name-str-module-optional-forwardref-module-none-source-4" tabindex="-1"><code>register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-module-self-name-str-module-optional-forwardref-module-none-source-4" aria-label="Permalink to &quot;`register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695)&quot;">​</a></h4><p>Alias for <code>add_module</code>.</p><hr><h4 id="register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source-4" tabindex="-1"><code>register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source-4" aria-label="Permalink to &quot;`register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617)&quot;">​</a></h4><p>Add a parameter to the module.</p><p>The parameter can be accessed as an attribute using given name.</p><h4 id="args-130" tabindex="-1">Args <a class="header-anchor" href="#args-130" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the parameter. The parameter can be accessed from this module using the given name</li><li><strong>param</strong> (<code>Parameter or None</code>): parameter to be added to the module. If <code>None</code>, then operations that run on parameters, such as <code>cuda</code>, are ignored. If <code>None</code>, the parameter is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li></ul><hr><h4 id="register-state-dict-post-hook-self-hook-source-4" tabindex="-1"><code>register_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-post-hook-self-hook-source-4" aria-label="Permalink to &quot;`register_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017)&quot;">​</a></h4><p>Register a post-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can modify the <code>state_dict</code> inplace.</p><hr><h4 id="register-state-dict-pre-hook-self-hook-source-4" tabindex="-1"><code>register_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-pre-hook-self-hook-source-4" aria-label="Permalink to &quot;`register_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041)&quot;">​</a></h4><p>Register a pre-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, prefix, keep_vars) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p><hr><h4 id="requires-grad-self-requires-grad-bool-true-self-source-4" tabindex="-1"><code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#requires-grad-self-requires-grad-bool-true-self-source-4" aria-label="Permalink to &quot;`requires_grad_(self, requires_grad: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826)&quot;">​</a></h4><p>Change if autograd should record operations on parameters in this module.</p><p>This method sets the parameters&#39; <code>requires_grad</code> attributes in-place.</p><p>This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).</p><p>See <code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.</p><h4 id="args-131" tabindex="-1">Args <a class="header-anchor" href="#args-131" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>requires_grad</strong> (<code>bool</code>): whether autograd should record operations on parameters in this module. Default: <code>True</code>.</li></ul><h4 id="returns-119" tabindex="-1">Returns <a class="header-anchor" href="#returns-119" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="reset-parameters-self-none-source-4" tabindex="-1"><code>reset_parameters(self) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L243" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#reset-parameters-self-none-source-4" aria-label="Permalink to &quot;`reset_parameters(self) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L243)&quot;">​</a></h4><hr><h4 id="reset-running-stats-self-none-source-4" tabindex="-1"><code>reset_running_stats(self) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L80" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#reset-running-stats-self-none-source-4" aria-label="Permalink to &quot;`reset_running_stats(self) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L80)&quot;">​</a></h4><hr><h4 id="set-extra-state-self-state-any-none-source-4" tabindex="-1"><code>set_extra_state(self, state: Any) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-extra-state-self-state-any-none-source-4" aria-label="Permalink to &quot;`set_extra_state(self, state: Any) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938)&quot;">​</a></h4><p>Set extra state contained in the loaded <code>state_dict</code>.</p><p>This function is called from <code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding <code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.</p><h4 id="args-132" tabindex="-1">Args <a class="header-anchor" href="#args-132" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state</strong> (<code>dict</code>): Extra state from the <code>state_dict</code></li></ul><hr><h4 id="set-submodule-self-target-str-module-module-strict-bool-false-none-source-4" tabindex="-1"><code>set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-submodule-self-target-str-module-module-strict-bool-false-none-source-4" aria-label="Permalink to &quot;`set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764)&quot;">​</a></h4><p>Set the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>If <code>strict</code> is set to <code>False</code> (default), the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>strict</code> is set to <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule does not exist.</p></div><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(3, 3, 3) ) (linear): Linear(3, 3) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To override the <code>Conv2d</code> with a new submodule <code>Linear</code>, you could call <code>set_submodule(&quot;net_b.net_c.conv&quot;, nn.Linear(1, 1))</code> where <code>strict</code> could be <code>True</code> or <code>False</code></p><p>To add a new submodule <code>Conv2d</code> to the existing <code>net_b</code> module, you would call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1))</code>.</p><p>In the above if you set <code>strict=True</code> and call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1), strict=True)</code>, an AttributeError will be raised because <code>net_b</code> does not have a submodule named <code>conv</code>.</p><h4 id="args-133" tabindex="-1">Args <a class="header-anchor" href="#args-133" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li><li><strong>module</strong>: The module to set the submodule to.</li><li><strong>strict</strong>: If <code>False</code>, the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule doesn&#39;t already exist.</li></ul><h4 id="raises-19" tabindex="-1">Raises <a class="header-anchor" href="#raises-19" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>ValueError</strong>: If the <code>target</code> string is empty or if <code>module</code> is not an instance of <code>nn.Module</code>.</li><li><strong>AttributeError</strong>: If at any point along the path resulting from the <code>target</code> string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="share-memory-self-self-source-4" tabindex="-1"><code>share_memory(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#share-memory-self-self-source-4" aria-label="Permalink to &quot;`share_memory(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877)&quot;">​</a></h4><p>See <code>tensorplay.Tensor.share_memory_</code>.</p><hr><h4 id="state-dict-self-args-destination-none-prefix-keep-vars-false-source-4" tabindex="-1"><code>state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#state-dict-self-args-destination-none-prefix-keep-vars-false-source-4" aria-label="Permalink to &quot;`state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105)&quot;">​</a></h4><p>Return a dictionary containing references to the whole state of the module.</p><p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>The returned object is a shallow copy. It contains references to the module&#39;s parameters and buffers.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Currently <code>state_dict()</code> also accepts positional arguments for <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However, this is being deprecated and keyword arguments will be enforced in future releases.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Please avoid the use of argument <code>destination</code> as it is not designed for end-users.</p></div><h4 id="args-134" tabindex="-1">Args <a class="header-anchor" href="#args-134" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>destination</strong> (<code>dict, optional</code>): If provided, the state of module will be updated into the dict and the same object is returned. Otherwise, an <code>OrderedDict</code> will be created and returned.</li><li><strong>Default</strong>: <code>None</code>.</li><li><strong>prefix</strong> (<code>str, optional</code>): a prefix added to parameter and buffer names to compose the keys in state_dict. Default: <code>&#39;&#39;</code>.</li><li><strong>keep_vars</strong> (<code>bool, optional</code>): by default the <code>~tensorplay.Tensor</code> s returned in the state dict are detached from autograd. If it&#39;s set to <code>True</code>, detaching will not be performed.</li><li><strong>Default</strong>: <code>False</code>.</li></ul><h4 id="returns-120" tabindex="-1">Returns <a class="header-anchor" href="#returns-120" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>dict</strong>: a dictionary containing a whole state of the module</li></ul><h4 id="example-49" tabindex="-1">Example <a class="header-anchor" href="#example-49" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">module.state_dict().keys()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;weight&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><hr><h4 id="to-self-args-kwargs-source-4" tabindex="-1"><code>to(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-self-args-kwargs-source-4" aria-label="Permalink to &quot;`to(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151)&quot;">​</a></h4><p>Move and/or cast the parameters and buffers.</p><p>This can be called as</p><p>.. function:: to(device=None, dtype=None, non_blocking=False) :noindex:</p><p>.. function:: to(dtype, non_blocking=False) :noindex:</p><p>.. function:: to(tensor, non_blocking=False) :noindex:</p><p>.. function:: to(memory_format=tensorplay.channels_last) :noindex:</p><p>Its signature is similar to <code>tensorplay.Tensor.to</code>, but only accepts floating point or complex <code>dtype</code>\ s. In addition, this method will only cast the floating point or complex parameters and buffers to <code>dtype</code> (if given). The integral parameters and buffers will be moved <code>device</code>, if that is given, but with dtypes unchanged. When <code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.</p><p>See below for examples.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-135" tabindex="-1">Args <a class="header-anchor" href="#args-135" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): the desired device of the parameters and buffers in this module</li><li><strong>dtype</strong> (<code>tensorplay.dtype</code>): the desired floating point or complex dtype of the parameters and buffers in this module</li><li><strong>tensor</strong> (<code>tensorplay.Tensor</code>): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module</li><li><strong>memory_format</strong> (<code>tensorplay.memory_format</code>): the desired memory format for 4D parameters and buffers in this module (keyword only argument)</li></ul><h4 id="returns-121" tabindex="-1">Returns <a class="header-anchor" href="#returns-121" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="examples-7" tabindex="-1">Examples <a class="header-anchor" href="#examples-7" aria-label="Permalink to &quot;Examples&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +IGNORE_WANT(&quot;non-deterministic&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(tensorplay.double)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float64)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +REQUIRES(env:TENSORPLAY_DOCTEST_CUDA1)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">gpu1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cuda:1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(gpu1, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.half, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">non_blocking</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;cuda:1&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cpu </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cpu&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(cpu)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).to(tensorplay.cdouble)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3741</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,  </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2382</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5593</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.4443</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear(tensorplay.ones(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.cdouble))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br></div></div><hr><h4 id="to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source-4" tabindex="-1"><code>to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source-4" aria-label="Permalink to &quot;`to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119)&quot;">​</a></h4><p>Move the parameters and buffers to the specified device without copying storage.</p><h4 id="args-136" tabindex="-1">Args <a class="header-anchor" href="#args-136" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): The desired device of the parameters and buffers in this module.</li><li><strong>recurse</strong> (<code>bool</code>): Whether parameters and buffers of submodules should be recursively moved to the specified device.</li></ul><h4 id="returns-122" tabindex="-1">Returns <a class="header-anchor" href="#returns-122" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="train-self-mode-bool-true-self-source-4" tabindex="-1"><code>train(self, mode: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#train-self-mode-bool-true-self-source-4" aria-label="Permalink to &quot;`train(self, mode: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786)&quot;">​</a></h4><p>Set the module in training mode.</p><p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e., whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, etc.</p><h4 id="args-137" tabindex="-1">Args <a class="header-anchor" href="#args-137" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>mode</strong> (<code>bool</code>): whether to set training mode (<code>True</code>) or evaluation mode (<code>False</code>). Default: <code>True</code>.</li></ul><h4 id="returns-123" tabindex="-1">Returns <a class="header-anchor" href="#returns-123" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="type-self-dst-type-union-tensorplay-dtype-str-self-source-4" tabindex="-1"><code>type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#type-self-dst-type-union-tensorplay-dtype-str-self-source-4" aria-label="Permalink to &quot;`type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061)&quot;">​</a></h4><p>Casts all parameters and buffers to <code>dst_type</code>.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-138" tabindex="-1">Args <a class="header-anchor" href="#args-138" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>dst_type</strong> (<code>type or string</code>): the desired type</li></ul><h4 id="returns-124" tabindex="-1">Returns <a class="header-anchor" href="#returns-124" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="zero-grad-self-set-to-none-bool-true-none-source-4" tabindex="-1"><code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#zero-grad-self-set-to-none-bool-true-none-source-4" aria-label="Permalink to &quot;`zero_grad(self, set_to_none: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849)&quot;">​</a></h4><p>Reset gradients of all model parameters.</p><p>See similar function under <code>tensorplay.optim.Optimizer</code> for more context.</p><h4 id="args-139" tabindex="-1">Args <a class="header-anchor" href="#args-139" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>set_to_none</strong> (<code>bool</code>): instead of setting to zero, set the grads to None. See <code>tensorplay.optim.Optimizer.zero_grad</code> for details.</li></ul><hr></details><h3 id="class-lazyinstancenorm3d-source" tabindex="-1"><code>class LazyInstanceNorm3d</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/instancenorm.py#L434" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-lazyinstancenorm3d-source" aria-label="Permalink to &quot;`class LazyInstanceNorm3d` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/instancenorm.py#L434)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">LazyInstanceNorm3d(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">eps</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1e-05</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">momentum</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">affine</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">track_running_stats</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>_LazyNormBase</code>, <code>_InstanceNorm</code></p><p>A <code>tensorplay.nn.InstanceNorm3d</code> module with lazy initialization of the <code>num_features</code> argument.</p><p>The <code>num_features</code> argument of the <code>InstanceNorm3d</code> is inferred from the <code>input.size(1)</code>. The attributes that will be lazily initialized are <code>weight</code>, <code>bias</code>, <code>running_mean</code> and <code>running_var</code>.</p><p>Check the <code>tensorplay.nn.modules.lazy.LazyModuleMixin</code> for further documentation on lazy modules and their limitations.</p><h4 id="args-140" tabindex="-1">Args <a class="header-anchor" href="#args-140" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>num_features</strong>: <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.719ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 760 727" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi></math></mjx-assistive-mml></mjx-container> from an expected input of size <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="15.766ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 6968.7 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1277,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1721.7,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2481.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2926.3,0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3754.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(4199,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5087,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(5531.7,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(6579.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>N</mi><mo>,</mo><mi>C</mi><mo>,</mo><mi>D</mi><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> or <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="12.751ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 5636 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1149,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1593.7,0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2421.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2866.3,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3754.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(4199,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5247,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>C</mi><mo>,</mo><mi>D</mi><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container></li><li><strong>eps</strong>: a value added to the denominator for numerical stability. Default: 1e-5</li><li><strong>momentum</strong>: the value used for the running_mean and running_var computation. Default: 0.1</li><li><strong>affine</strong>: a boolean value that when set to <code>True</code>, this module has learnable affine parameters, initialized the same way as done for batch normalization.</li><li><strong>Default</strong>: <code>False</code>.</li><li><strong>track_running_stats</strong>: a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: <code>False</code></li></ul><h4 id="shape-5" tabindex="-1">Shape <a class="header-anchor" href="#shape-5" aria-label="Permalink to &quot;Shape&quot;">​</a></h4><ul><li>Input: <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="15.766ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 6968.7 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1277,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1721.7,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2481.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2926.3,0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3754.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(4199,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5087,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(5531.7,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(6579.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>N</mi><mo>,</mo><mi>C</mi><mo>,</mo><mi>D</mi><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> or <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="12.751ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 5636 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1149,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1593.7,0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2421.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2866.3,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3754.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(4199,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5247,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>C</mi><mo>,</mo><mi>D</mi><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container></li><li>Output: <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="15.766ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 6968.7 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1277,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1721.7,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2481.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2926.3,0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3754.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(4199,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5087,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(5531.7,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(6579.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>N</mi><mo>,</mo><mi>C</mi><mo>,</mo><mi>D</mi><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> or <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="12.751ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 5636 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1149,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1593.7,0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2421.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2866.3,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3754.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(4199,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5247,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>C</mi><mo>,</mo><mi>D</mi><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> (same shape as input)</li></ul><details><summary>Methods</summary><h4 id="init-self-eps-1e-05-momentum-0-1-affine-true-track-running-stats-true-device-none-dtype-none-none-source-2" tabindex="-1"><code>__init__(self, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L209" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-eps-1e-05-momentum-0-1-affine-true-track-running-stats-true-device-none-dtype-none-none-source-2" aria-label="Permalink to &quot;`__init__(self, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L209)&quot;">​</a></h4><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p><hr><h4 id="add-module-self-name-str-module-optional-forwardref-module-none-source-5" tabindex="-1"><code>add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#add-module-self-name-str-module-optional-forwardref-module-none-source-5" aria-label="Permalink to &quot;`add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667)&quot;">​</a></h4><p>Add a child module to the current module.</p><p>The module can be accessed as an attribute using the given name.</p><h4 id="args-141" tabindex="-1">Args <a class="header-anchor" href="#args-141" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the child module. The child module can be accessed from this module using the given name</li><li><strong>module</strong> (<code>Module</code>): child module to be added to the module.</li></ul><hr><h4 id="apply-self-fn-callable-forwardref-module-nonetype-self-source-5" tabindex="-1"><code>apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#apply-self-fn-callable-forwardref-module-nonetype-self-source-5" aria-label="Permalink to &quot;`apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990)&quot;">​</a></h4><p>Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self.</p><p>Typical use includes initializing the parameters of a model (see also <code>nn-init-doc</code>).</p><h4 id="args-142" tabindex="-1">Args <a class="header-anchor" href="#args-142" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>fn</strong> (``Module<code> -&gt; None</code>): function to be applied to each submodule</li></ul><h4 id="returns-125" tabindex="-1">Returns <a class="header-anchor" href="#returns-125" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="example-50" tabindex="-1">Example <a class="header-anchor" href="#example-50" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@tensorplay.no_grad</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> init_weights</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        m.weight.fill_(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m.weight)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net.apply(init_weights)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><hr><h4 id="bfloat16-self-self-source-5" tabindex="-1"><code>bfloat16(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#bfloat16-self-self-source-5" aria-label="Permalink to &quot;`bfloat16(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-126" tabindex="-1">Returns <a class="header-anchor" href="#returns-126" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source-5" tabindex="-1"><code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source-5" aria-label="Permalink to &quot;`buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627)&quot;">​</a></h4><p>Return an iterator over module buffers.</p><h4 id="args-143" tabindex="-1">Args <a class="header-anchor" href="#args-143" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.</li></ul><h4 id="yields-40" tabindex="-1">Yields <a class="header-anchor" href="#yields-40" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>tensorplay.Tensor: module buffer
</code></pre><h4 id="example-51" tabindex="-1">Example <a class="header-anchor" href="#example-51" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.buffers():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf), buf.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="children-self-collections-abc-iterator-module-source-5" tabindex="-1"><code>children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#children-self-collections-abc-iterator-module-source-5" aria-label="Permalink to &quot;`children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681)&quot;">​</a></h4><p>Return an iterator over immediate children modules.</p><h4 id="yields-41" tabindex="-1">Yields <a class="header-anchor" href="#yields-41" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a child module</li></ul><hr><h4 id="compile-self-args-kwargs-source-5" tabindex="-1"><code>compile(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#compile-self-args-kwargs-source-5" aria-label="Permalink to &quot;`compile(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944)&quot;">​</a></h4><p>Compile this Module&#39;s forward using <code>tensorplay.compile</code>.</p><p>This Module&#39;s <code>__call__</code> method is compiled and all arguments are passed as-is to <code>tensorplay.compile</code>.</p><p>See <code>tensorplay.compile</code> for details on the arguments for this function.</p><hr><h4 id="cpu-self-self-source-5" tabindex="-1"><code>cpu(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cpu-self-self-source-5" aria-label="Permalink to &quot;`cpu(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050)&quot;">​</a></h4><p>Move all model parameters and buffers to the CPU.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-127" tabindex="-1">Returns <a class="header-anchor" href="#returns-127" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source-5" tabindex="-1"><code>cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source-5" aria-label="Permalink to &quot;`cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031)&quot;">​</a></h4><p>Move all model parameters and buffers to the GPU.</p><p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on GPU while being optimized.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-144" tabindex="-1">Args <a class="header-anchor" href="#args-144" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>int, optional</code>): if specified, all parameters will be copied to that device</li></ul><h4 id="returns-128" tabindex="-1">Returns <a class="header-anchor" href="#returns-128" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="double-self-self-source-5" tabindex="-1"><code>double(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#double-self-self-source-5" aria-label="Permalink to &quot;`double(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-129" tabindex="-1">Returns <a class="header-anchor" href="#returns-129" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="eval-self-self-source-5" tabindex="-1"><code>eval(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#eval-self-self-source-5" aria-label="Permalink to &quot;`eval(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808)&quot;">​</a></h4><p>Set the module in evaluation mode.</p><p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e. whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, etc.</p><p>This is equivalent with <code>self.train(False) &lt;tensorplay.nn.Module.train&gt;</code>.</p><p>See <code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.</p><h4 id="returns-130" tabindex="-1">Returns <a class="header-anchor" href="#returns-130" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="extra-repr-self-source-5" tabindex="-1"><code>extra_repr(self)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L97" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#extra-repr-self-source-5" aria-label="Permalink to &quot;`extra_repr(self)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L97)&quot;">​</a></h4><p>Return the extra representation of the module.</p><p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p><hr><h4 id="float-self-self-source-5" tabindex="-1"><code>float(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#float-self-self-source-5" aria-label="Permalink to &quot;`float(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-131" tabindex="-1">Returns <a class="header-anchor" href="#returns-131" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="forward-self-input-tensorplay-c-tensorbase-tensorplay-c-tensorbase-source-5" tabindex="-1"><code>forward(self, input: tensorplay._C.TensorBase) -&gt; tensorplay._C.TensorBase</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/instancenorm.py#L102" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#forward-self-input-tensorplay-c-tensorbase-tensorplay-c-tensorbase-source-5" aria-label="Permalink to &quot;`forward(self, input: tensorplay._C.TensorBase) -&gt; tensorplay._C.TensorBase` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/instancenorm.py#L102)&quot;">​</a></h4><p>Define the computation performed at every call.</p><p>Should be overridden by all subclasses.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code> instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.</p></div><hr><h4 id="get-buffer-self-target-str-tensor-source-5" tabindex="-1"><code>get_buffer(self, target: str) -&gt; &#39;Tensor&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-buffer-self-target-str-tensor-source-5" aria-label="Permalink to &quot;`get_buffer(self, target: str) -&gt; &#39;Tensor&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881)&quot;">​</a></h4><p>Return the buffer given by <code>target</code> if it exists, otherwise throw an error.</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-145" tabindex="-1">Args <a class="header-anchor" href="#args-145" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the buffer to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-132" tabindex="-1">Returns <a class="header-anchor" href="#returns-132" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.Tensor: The buffer referenced by ``target``
</code></pre><h4 id="raises-20" tabindex="-1">Raises <a class="header-anchor" href="#raises-20" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not a buffer</li></ul><hr><h4 id="get-extra-state-self-any-source-5" tabindex="-1"><code>get_extra_state(self) -&gt; Any</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-extra-state-self-any-source-5" aria-label="Permalink to &quot;`get_extra_state(self) -&gt; Any` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917)&quot;">​</a></h4><p>Return any extra state to include in the module&#39;s state_dict.</p><p>Implement this and a corresponding <code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module&#39;s <code>state_dict()</code>.</p><p>Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.</p><h4 id="returns-133" tabindex="-1">Returns <a class="header-anchor" href="#returns-133" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>object</strong>: Any extra state to store in the module&#39;s state_dict</li></ul><hr><h4 id="get-parameter-self-target-str-parameter-source-5" tabindex="-1"><code>get_parameter(self, target: str) -&gt; &#39;Parameter&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-parameter-self-target-str-parameter-source-5" aria-label="Permalink to &quot;`get_parameter(self, target: str) -&gt; &#39;Parameter&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845)&quot;">​</a></h4><p>Return the parameter given by <code>target</code> if it exists, otherwise throw an error.</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-146" tabindex="-1">Args <a class="header-anchor" href="#args-146" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the Parameter to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-134" tabindex="-1">Returns <a class="header-anchor" href="#returns-134" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Parameter: The Parameter referenced by ``target``
</code></pre><h4 id="raises-21" tabindex="-1">Raises <a class="header-anchor" href="#raises-21" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not an <code>nn.Parameter</code></li></ul><hr><h4 id="get-submodule-self-target-str-module-source-5" tabindex="-1"><code>get_submodule(self, target: str) -&gt; &#39;Module&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-submodule-self-target-str-module-source-5" aria-label="Permalink to &quot;`get_submodule(self, target: str) -&gt; &#39;Module&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699)&quot;">​</a></h4><p>Return the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> which has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(&quot;net_b.linear&quot;)</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(&quot;net_b.net_c.conv&quot;)</code>.</p><p>The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.</p><h4 id="args-147" tabindex="-1">Args <a class="header-anchor" href="#args-147" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li></ul><h4 id="returns-135" tabindex="-1">Returns <a class="header-anchor" href="#returns-135" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Module: The submodule referenced by ``target``
</code></pre><h4 id="raises-22" tabindex="-1">Raises <a class="header-anchor" href="#raises-22" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If at any point along the path resulting from the target string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="half-self-self-source-5" tabindex="-1"><code>half(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#half-self-self-source-5" aria-label="Permalink to &quot;`half(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-136" tabindex="-1">Returns <a class="header-anchor" href="#returns-136" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="has-uninitialized-params-self-tensorplay-nn-modules-lazy-lazyprotocol-source-2" tabindex="-1"><code>has_uninitialized_params(self: tensorplay.nn.modules.lazy._LazyProtocol)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/lazy.py#L236" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#has-uninitialized-params-self-tensorplay-nn-modules-lazy-lazyprotocol-source-2" aria-label="Permalink to &quot;`has_uninitialized_params(self: tensorplay.nn.modules.lazy._LazyProtocol)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/lazy.py#L236)&quot;">​</a></h4><p>Check if a module has parameters that are not initialized.</p><hr><h4 id="initialize-parameters-self-input-none-source-2" tabindex="-1"><code>initialize_parameters(self, input) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L247" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#initialize-parameters-self-input-none-source-2" aria-label="Permalink to &quot;`initialize_parameters(self, input) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L247)&quot;">​</a></h4><p>Initialize parameters according to the input batch properties.</p><p>This adds an interface to isolate parameter initialization from the forward pass when doing parameter shape inference.</p><hr><h4 id="load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source-5" tabindex="-1"><code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source-5" aria-label="Permalink to &quot;`load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436)&quot;">​</a></h4><p>Copy parameters and buffers from <code>state_dict</code> into this module and its descendants.</p><p>If <code>strict</code> is <code>True</code>, then the keys of <code>state_dict</code> must exactly match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function.</p><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>If <code>assign</code> is <code>True</code> the optimizer must be created after the call to <code>load_state_dict</code> unless <code>~tensorplay.__future__.get_swap_module_params_on_conversion</code> is <code>True</code>.</p></div><h4 id="args-148" tabindex="-1">Args <a class="header-anchor" href="#args-148" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state_dict</strong> (<code>dict</code>): a dict containing parameters and persistent buffers.</li><li><strong>strict</strong> (<code>bool, optional</code>): whether to strictly enforce that the keys in <code>state_dict</code> match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function. Default: <code>True</code></li><li><strong>assign</strong> (<code>bool, optional</code>): When set to <code>False</code>, the properties of the tensors in the current module are preserved whereas setting it to <code>True</code> preserves properties of the Tensors in the state dict. The only exception is the <code>requires_grad</code> field of <code>~tensorplay.nn.Parameter</code> for which the value from the module is preserved. Default: <code>False</code></li></ul><h4 id="returns-137" tabindex="-1">Returns <a class="header-anchor" href="#returns-137" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:
    * ``missing_keys`` is a list of str containing any keys that are expected
        by this module but missing from the provided ``state_dict``.
    * ``unexpected_keys`` is a list of str containing the keys that are not
        expected by this module but present in the provided ``state_dict``.
</code></pre><h4 id="note-15" tabindex="-1">Note <a class="header-anchor" href="#note-15" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>If a parameter or buffer is registered as ``None`` and its corresponding key
exists in `state_dict`, `load_state_dict` will raise a
``RuntimeError``.
</code></pre><hr><h4 id="modules-self-collections-abc-iterator-module-source-5" tabindex="-1"><code>modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#modules-self-collections-abc-iterator-module-source-5" aria-label="Permalink to &quot;`modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710)&quot;">​</a></h4><p>Return an iterator over all modules in the network.</p><h4 id="yields-42" tabindex="-1">Yields <a class="header-anchor" href="#yields-42" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a module in the network</li></ul><h4 id="note-16" tabindex="-1">Note <a class="header-anchor" href="#note-16" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-52" tabindex="-1">Example <a class="header-anchor" href="#example-52" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source-5" tabindex="-1"><code>named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source-5" aria-label="Permalink to &quot;`named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650)&quot;">​</a></h4><p>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p><h4 id="args-149" tabindex="-1">Args <a class="header-anchor" href="#args-149" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all buffer names.</li><li><strong>recurse</strong> (<code>bool, optional</code>): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Defaults to True.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated buffers in the result. Defaults to True.</li></ul><h4 id="yields-43" tabindex="-1">Yields <a class="header-anchor" href="#yields-43" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, tensorplay.Tensor): Tuple containing the name and buffer
</code></pre><h4 id="example-53" tabindex="-1">Example <a class="header-anchor" href="#example-53" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_buffers():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_var&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-children-self-collections-abc-iterator-tuple-str-module-source-5" tabindex="-1"><code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-children-self-collections-abc-iterator-tuple-str-module-source-5" aria-label="Permalink to &quot;`named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690)&quot;">​</a></h4><p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p><h4 id="yields-44" tabindex="-1">Yields <a class="header-anchor" href="#yields-44" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple containing a name and child module
</code></pre><h4 id="example-54" tabindex="-1">Example <a class="header-anchor" href="#example-54" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, module </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.named_children():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv4&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv5&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(module)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source-5" tabindex="-1"><code>named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source-5" aria-label="Permalink to &quot;`named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737)&quot;">​</a></h4><p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p><h4 id="args-150" tabindex="-1">Args <a class="header-anchor" href="#args-150" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>memo</strong>: a memo to store the set of modules already added to the result</li><li><strong>prefix</strong>: a prefix that will be added to the name of the module</li><li><strong>remove_duplicate</strong>: whether to remove the duplicated module instances in the result or not</li></ul><h4 id="yields-45" tabindex="-1">Yields <a class="header-anchor" href="#yields-45" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple of name and module
</code></pre><h4 id="note-17" tabindex="-1">Note <a class="header-anchor" href="#note-17" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-55" tabindex="-1">Example <a class="header-anchor" href="#example-55" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.named_modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;0&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source-5" tabindex="-1"><code>named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source-5" aria-label="Permalink to &quot;`named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595)&quot;">​</a></h4><p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p><h4 id="args-151" tabindex="-1">Args <a class="header-anchor" href="#args-151" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all parameter names.</li><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated parameters in the result. Defaults to True.</li></ul><h4 id="yields-46" tabindex="-1">Yields <a class="header-anchor" href="#yields-46" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Parameter): Tuple containing the name and parameter
</code></pre><h4 id="example-56" tabindex="-1">Example <a class="header-anchor" href="#example-56" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_parameters():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source-5" tabindex="-1"><code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source-5" aria-label="Permalink to &quot;`parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570)&quot;">​</a></h4><p>Return an iterator over module parameters.</p><p>This is typically passed to an optimizer.</p><h4 id="args-152" tabindex="-1">Args <a class="header-anchor" href="#args-152" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</li></ul><h4 id="yields-47" tabindex="-1">Yields <a class="header-anchor" href="#yields-47" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Parameter</strong>: module parameter</li></ul><h4 id="example-57" tabindex="-1">Example <a class="header-anchor" href="#example-57" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.parameters():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param), param.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source-5" tabindex="-1"><code>register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source-5" aria-label="Permalink to &quot;`register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>This function is deprecated in favor of <code>~tensorplay.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.</p><h4 id="returns-138" tabindex="-1">Returns <a class="header-anchor" href="#returns-138" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source-5" tabindex="-1"><code>register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source-5" aria-label="Permalink to &quot;`register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553)&quot;">​</a></h4><p>Add a buffer to the module.</p><p>This is typically used to register a buffer that should not be considered a model parameter. For example, BatchNorm&#39;s <code>running_mean</code> is not a parameter, but is part of the module&#39;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting <code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&#39;s <code>state_dict</code>.</p><p>Buffers can be accessed as attributes using given names.</p><h4 id="args-153" tabindex="-1">Args <a class="header-anchor" href="#args-153" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the buffer. The buffer can be accessed from this module using the given name</li><li><strong>tensor</strong> (<code>Tensor or None</code>): buffer to be registered. If <code>None</code>, then operations that run on buffers, such as <code>cuda</code>, are ignored. If <code>None</code>, the buffer is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li><li><strong>persistent</strong> (<code>bool</code>): whether the buffer is part of this module&#39;s <code>state_dict</code>.</li></ul><h4 id="example-58" tabindex="-1">Example <a class="header-anchor" href="#example-58" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.register_buffer(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_mean&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, tensorplay.zeros(num_features))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><hr><h4 id="register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source-5" tabindex="-1"><code>register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source-5" aria-label="Permalink to &quot;`register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592)&quot;">​</a></h4><p>Register a forward hook on the module.</p><p>The hook will be called every time after <code>forward</code> has computed an output.</p><p>If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after <code>forward</code> is called. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-154" tabindex="-1">Args <a class="header-anchor" href="#args-154" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If <code>True</code>, the provided <code>hook</code> will be fired before all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward</code> hooks registered with <code>register_module_forward_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If <code>True</code>, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>always_call</strong> (<code>bool</code>): If <code>True</code> the <code>hook</code> will be run regardless of whether an exception is raised while calling the Module.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-139" tabindex="-1">Returns <a class="header-anchor" href="#returns-139" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source-5" tabindex="-1"><code>register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source-5" aria-label="Permalink to &quot;`register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526)&quot;">​</a></h4><p>Register a forward pre-hook on the module.</p><p>The hook will be called every time before <code>forward</code> is invoked.</p><p>If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> of modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> and</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> kwargs</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-155" tabindex="-1">Args <a class="header-anchor" href="#args-155" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward_pre</code> hooks registered with <code>register_module_forward_pre_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If true, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-140" tabindex="-1">Returns <a class="header-anchor" href="#returns-140" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-5" tabindex="-1"><code>register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-5" aria-label="Permalink to &quot;`register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>The hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:</p><pre><code>1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.
2. If none of the module inputs require gradients, the hook will fire when the gradients are computed
   with respect to module outputs.
3. If none of the module outputs require gradients, then the hooks will not fire.
</code></pre><p>The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_input, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Tensor) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_input</code> and <code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of <code>grad_input</code> in subsequent computations. <code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in <code>grad_input</code> and <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> outputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-156" tabindex="-1">Args <a class="header-anchor" href="#args-156" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward</code> hooks registered with <code>register_module_full_backward_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-141" tabindex="-1">Returns <a class="header-anchor" href="#returns-141" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-5" tabindex="-1"><code>register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-5" aria-label="Permalink to &quot;`register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287)&quot;">​</a></h4><p>Register a backward pre-hook on the module.</p><p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tuple[Tensor] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of <code>grad_output</code> in subsequent computations. Entries in <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-157" tabindex="-1">Args <a class="header-anchor" href="#args-157" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward_pre</code> hooks registered with <code>register_module_full_backward_pre_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-142" tabindex="-1">Returns <a class="header-anchor" href="#returns-142" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-post-hook-self-hook-source-5" tabindex="-1"><code>register_load_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-post-hook-self-hook-source-5" aria-label="Permalink to &quot;`register_load_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226)&quot;">​</a></h4><p>Register a post-hook to be run after module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, incompatible_keys) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.</p><p>The given incompatible_keys can be modified inplace if needed.</p><p>Note that the checks performed when calling <code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.</p><h4 id="returns-143" tabindex="-1">Returns <a class="header-anchor" href="#returns-143" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-pre-hook-self-hook-source-5" tabindex="-1"><code>register_load_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-pre-hook-self-hook-source-5" aria-label="Permalink to &quot;`register_load_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214)&quot;">​</a></h4><p>Register a pre-hook to be run before module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # noqa: B950</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="arguments-5" tabindex="-1">Arguments <a class="header-anchor" href="#arguments-5" aria-label="Permalink to &quot;Arguments&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): Callable hook that will be invoked before loading the state dict.</li></ul><hr><h4 id="register-module-self-name-str-module-optional-forwardref-module-none-source-5" tabindex="-1"><code>register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-module-self-name-str-module-optional-forwardref-module-none-source-5" aria-label="Permalink to &quot;`register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695)&quot;">​</a></h4><p>Alias for <code>add_module</code>.</p><hr><h4 id="register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source-5" tabindex="-1"><code>register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source-5" aria-label="Permalink to &quot;`register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617)&quot;">​</a></h4><p>Add a parameter to the module.</p><p>The parameter can be accessed as an attribute using given name.</p><h4 id="args-158" tabindex="-1">Args <a class="header-anchor" href="#args-158" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the parameter. The parameter can be accessed from this module using the given name</li><li><strong>param</strong> (<code>Parameter or None</code>): parameter to be added to the module. If <code>None</code>, then operations that run on parameters, such as <code>cuda</code>, are ignored. If <code>None</code>, the parameter is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li></ul><hr><h4 id="register-state-dict-post-hook-self-hook-source-5" tabindex="-1"><code>register_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-post-hook-self-hook-source-5" aria-label="Permalink to &quot;`register_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017)&quot;">​</a></h4><p>Register a post-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can modify the <code>state_dict</code> inplace.</p><hr><h4 id="register-state-dict-pre-hook-self-hook-source-5" tabindex="-1"><code>register_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-pre-hook-self-hook-source-5" aria-label="Permalink to &quot;`register_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041)&quot;">​</a></h4><p>Register a pre-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, prefix, keep_vars) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p><hr><h4 id="requires-grad-self-requires-grad-bool-true-self-source-5" tabindex="-1"><code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#requires-grad-self-requires-grad-bool-true-self-source-5" aria-label="Permalink to &quot;`requires_grad_(self, requires_grad: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826)&quot;">​</a></h4><p>Change if autograd should record operations on parameters in this module.</p><p>This method sets the parameters&#39; <code>requires_grad</code> attributes in-place.</p><p>This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).</p><p>See <code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.</p><h4 id="args-159" tabindex="-1">Args <a class="header-anchor" href="#args-159" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>requires_grad</strong> (<code>bool</code>): whether autograd should record operations on parameters in this module. Default: <code>True</code>.</li></ul><h4 id="returns-144" tabindex="-1">Returns <a class="header-anchor" href="#returns-144" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="reset-parameters-self-none-source-5" tabindex="-1"><code>reset_parameters(self) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L243" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#reset-parameters-self-none-source-5" aria-label="Permalink to &quot;`reset_parameters(self) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L243)&quot;">​</a></h4><hr><h4 id="reset-running-stats-self-none-source-5" tabindex="-1"><code>reset_running_stats(self) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L80" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#reset-running-stats-self-none-source-5" aria-label="Permalink to &quot;`reset_running_stats(self) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/batchnorm.py#L80)&quot;">​</a></h4><hr><h4 id="set-extra-state-self-state-any-none-source-5" tabindex="-1"><code>set_extra_state(self, state: Any) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-extra-state-self-state-any-none-source-5" aria-label="Permalink to &quot;`set_extra_state(self, state: Any) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938)&quot;">​</a></h4><p>Set extra state contained in the loaded <code>state_dict</code>.</p><p>This function is called from <code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding <code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.</p><h4 id="args-160" tabindex="-1">Args <a class="header-anchor" href="#args-160" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state</strong> (<code>dict</code>): Extra state from the <code>state_dict</code></li></ul><hr><h4 id="set-submodule-self-target-str-module-module-strict-bool-false-none-source-5" tabindex="-1"><code>set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-submodule-self-target-str-module-module-strict-bool-false-none-source-5" aria-label="Permalink to &quot;`set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764)&quot;">​</a></h4><p>Set the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>If <code>strict</code> is set to <code>False</code> (default), the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>strict</code> is set to <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule does not exist.</p></div><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(3, 3, 3) ) (linear): Linear(3, 3) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To override the <code>Conv2d</code> with a new submodule <code>Linear</code>, you could call <code>set_submodule(&quot;net_b.net_c.conv&quot;, nn.Linear(1, 1))</code> where <code>strict</code> could be <code>True</code> or <code>False</code></p><p>To add a new submodule <code>Conv2d</code> to the existing <code>net_b</code> module, you would call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1))</code>.</p><p>In the above if you set <code>strict=True</code> and call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1), strict=True)</code>, an AttributeError will be raised because <code>net_b</code> does not have a submodule named <code>conv</code>.</p><h4 id="args-161" tabindex="-1">Args <a class="header-anchor" href="#args-161" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li><li><strong>module</strong>: The module to set the submodule to.</li><li><strong>strict</strong>: If <code>False</code>, the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule doesn&#39;t already exist.</li></ul><h4 id="raises-23" tabindex="-1">Raises <a class="header-anchor" href="#raises-23" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>ValueError</strong>: If the <code>target</code> string is empty or if <code>module</code> is not an instance of <code>nn.Module</code>.</li><li><strong>AttributeError</strong>: If at any point along the path resulting from the <code>target</code> string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="share-memory-self-self-source-5" tabindex="-1"><code>share_memory(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#share-memory-self-self-source-5" aria-label="Permalink to &quot;`share_memory(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877)&quot;">​</a></h4><p>See <code>tensorplay.Tensor.share_memory_</code>.</p><hr><h4 id="state-dict-self-args-destination-none-prefix-keep-vars-false-source-5" tabindex="-1"><code>state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#state-dict-self-args-destination-none-prefix-keep-vars-false-source-5" aria-label="Permalink to &quot;`state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105)&quot;">​</a></h4><p>Return a dictionary containing references to the whole state of the module.</p><p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>The returned object is a shallow copy. It contains references to the module&#39;s parameters and buffers.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Currently <code>state_dict()</code> also accepts positional arguments for <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However, this is being deprecated and keyword arguments will be enforced in future releases.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Please avoid the use of argument <code>destination</code> as it is not designed for end-users.</p></div><h4 id="args-162" tabindex="-1">Args <a class="header-anchor" href="#args-162" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>destination</strong> (<code>dict, optional</code>): If provided, the state of module will be updated into the dict and the same object is returned. Otherwise, an <code>OrderedDict</code> will be created and returned.</li><li><strong>Default</strong>: <code>None</code>.</li><li><strong>prefix</strong> (<code>str, optional</code>): a prefix added to parameter and buffer names to compose the keys in state_dict. Default: <code>&#39;&#39;</code>.</li><li><strong>keep_vars</strong> (<code>bool, optional</code>): by default the <code>~tensorplay.Tensor</code> s returned in the state dict are detached from autograd. If it&#39;s set to <code>True</code>, detaching will not be performed.</li><li><strong>Default</strong>: <code>False</code>.</li></ul><h4 id="returns-145" tabindex="-1">Returns <a class="header-anchor" href="#returns-145" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>dict</strong>: a dictionary containing a whole state of the module</li></ul><h4 id="example-59" tabindex="-1">Example <a class="header-anchor" href="#example-59" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">module.state_dict().keys()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;weight&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><hr><h4 id="to-self-args-kwargs-source-5" tabindex="-1"><code>to(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-self-args-kwargs-source-5" aria-label="Permalink to &quot;`to(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151)&quot;">​</a></h4><p>Move and/or cast the parameters and buffers.</p><p>This can be called as</p><p>.. function:: to(device=None, dtype=None, non_blocking=False) :noindex:</p><p>.. function:: to(dtype, non_blocking=False) :noindex:</p><p>.. function:: to(tensor, non_blocking=False) :noindex:</p><p>.. function:: to(memory_format=tensorplay.channels_last) :noindex:</p><p>Its signature is similar to <code>tensorplay.Tensor.to</code>, but only accepts floating point or complex <code>dtype</code>\ s. In addition, this method will only cast the floating point or complex parameters and buffers to <code>dtype</code> (if given). The integral parameters and buffers will be moved <code>device</code>, if that is given, but with dtypes unchanged. When <code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.</p><p>See below for examples.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-163" tabindex="-1">Args <a class="header-anchor" href="#args-163" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): the desired device of the parameters and buffers in this module</li><li><strong>dtype</strong> (<code>tensorplay.dtype</code>): the desired floating point or complex dtype of the parameters and buffers in this module</li><li><strong>tensor</strong> (<code>tensorplay.Tensor</code>): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module</li><li><strong>memory_format</strong> (<code>tensorplay.memory_format</code>): the desired memory format for 4D parameters and buffers in this module (keyword only argument)</li></ul><h4 id="returns-146" tabindex="-1">Returns <a class="header-anchor" href="#returns-146" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="examples-8" tabindex="-1">Examples <a class="header-anchor" href="#examples-8" aria-label="Permalink to &quot;Examples&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +IGNORE_WANT(&quot;non-deterministic&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(tensorplay.double)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float64)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +REQUIRES(env:TENSORPLAY_DOCTEST_CUDA1)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">gpu1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cuda:1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(gpu1, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.half, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">non_blocking</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;cuda:1&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cpu </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cpu&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(cpu)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).to(tensorplay.cdouble)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3741</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,  </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2382</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5593</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.4443</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear(tensorplay.ones(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.cdouble))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br></div></div><hr><h4 id="to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source-5" tabindex="-1"><code>to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source-5" aria-label="Permalink to &quot;`to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119)&quot;">​</a></h4><p>Move the parameters and buffers to the specified device without copying storage.</p><h4 id="args-164" tabindex="-1">Args <a class="header-anchor" href="#args-164" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): The desired device of the parameters and buffers in this module.</li><li><strong>recurse</strong> (<code>bool</code>): Whether parameters and buffers of submodules should be recursively moved to the specified device.</li></ul><h4 id="returns-147" tabindex="-1">Returns <a class="header-anchor" href="#returns-147" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="train-self-mode-bool-true-self-source-5" tabindex="-1"><code>train(self, mode: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#train-self-mode-bool-true-self-source-5" aria-label="Permalink to &quot;`train(self, mode: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786)&quot;">​</a></h4><p>Set the module in training mode.</p><p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e., whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, etc.</p><h4 id="args-165" tabindex="-1">Args <a class="header-anchor" href="#args-165" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>mode</strong> (<code>bool</code>): whether to set training mode (<code>True</code>) or evaluation mode (<code>False</code>). Default: <code>True</code>.</li></ul><h4 id="returns-148" tabindex="-1">Returns <a class="header-anchor" href="#returns-148" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="type-self-dst-type-union-tensorplay-dtype-str-self-source-5" tabindex="-1"><code>type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#type-self-dst-type-union-tensorplay-dtype-str-self-source-5" aria-label="Permalink to &quot;`type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061)&quot;">​</a></h4><p>Casts all parameters and buffers to <code>dst_type</code>.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-166" tabindex="-1">Args <a class="header-anchor" href="#args-166" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>dst_type</strong> (<code>type or string</code>): the desired type</li></ul><h4 id="returns-149" tabindex="-1">Returns <a class="header-anchor" href="#returns-149" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="zero-grad-self-set-to-none-bool-true-none-source-5" tabindex="-1"><code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#zero-grad-self-set-to-none-bool-true-none-source-5" aria-label="Permalink to &quot;`zero_grad(self, set_to_none: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849)&quot;">​</a></h4><p>Reset gradients of all model parameters.</p><p>See similar function under <code>tensorplay.optim.Optimizer</code> for more context.</p><h4 id="args-167" tabindex="-1">Args <a class="header-anchor" href="#args-167" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>set_to_none</strong> (<code>bool</code>): instead of setting to zero, set the grads to None. See <code>tensorplay.optim.Optimizer.zero_grad</code> for details.</li></ul><hr></details></div></div></main><footer class="VPDocFooter" data-v-39a288b8 data-v-e257564d><!--[--><!--]--><!----><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-e257564d><span class="visually-hidden" id="doc-footer-aria-label" data-v-e257564d>Pager</span><div class="pager" data-v-e257564d><!----></div><div class="pager" data-v-e257564d><a class="VPLink link pager-link next" href="/zh/api/" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>Next page</span><span class="title" data-v-e257564d>概览</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-5d98c3a5 data-v-e315a0ad><div class="container" data-v-e315a0ad><p class="message" data-v-e315a0ad>基于 Apache 2.0 许可发布。</p><p class="copyright" data-v-e315a0ad>版权所有 © 2025 zlx。保留所有权利。</p></div></footer><!--[--><a href="https://deepwiki.com/bluemoon-o2/TensorPlay" target="_blank" rel="noopener noreferrer" class="deepwiki-floating-badge" title="View on DeepWiki" data-v-5bf2a798><div class="badge-content" data-v-5bf2a798><span class="badge-icon" data-v-5bf2a798>📚</span><span class="badge-text" data-v-5bf2a798>DeepWiki</span></div></a><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"about_index.md\":\"CIIINTJo\",\"api__c.md\":\"CqosplcF\",\"api__reduction.md\":\"4CT0Th-w\",\"api_activation.md\":\"D6ga1LTB\",\"api_adagrad.md\":\"DLbERoZa\",\"api_adam.md\":\"RHJKi90L\",\"api_adamw.md\":\"CwVxav7q\",\"api_alexnet.md\":\"MpdCIREt\",\"api_amp.md\":\"DwxpVi7g\",\"api_audio.md\":\"Bh-isKk9\",\"api_autocast_mode.md\":\"Upx07om-\",\"api_autograd.md\":\"CT9D3RMp\",\"api_backend.md\":\"B-rtwINW\",\"api_backends.md\":\"CZwYzPim\",\"api_batchnorm.md\":\"DiZgGllz\",\"api_collate.md\":\"D7C1FWND\",\"api_common_types.md\":\"D___UsnV\",\"api_comparison.md\":\"CreVyI7m\",\"api_container.md\":\"DHHM_Bnu\",\"api_conv.md\":\"DVE_N5Pv\",\"api_cpp.md\":\"BzQeMOx5\",\"api_cpu.md\":\"CRrVscpA\",\"api_cuda.md\":\"CfDgBLV4\",\"api_data.md\":\"X1JkS3Ea\",\"api_dataloader.md\":\"C7Xq7oGq\",\"api_dataset.md\":\"B_1fKkbj\",\"api_datasets.md\":\"X5IH5o-c\",\"api_dropout.md\":\"4oOM6zi3\",\"api_flatten.md\":\"CazrUAyd\",\"api_folder.md\":\"iJO9XhTd\",\"api_function.md\":\"Cifia0I5\",\"api_functional.md\":\"vzxgI2qA\",\"api_grad_mode.md\":\"pynV7unl\",\"api_grad_scaler.md\":\"B2e1EVkK\",\"api_hooks.md\":\"oWg4LFLb\",\"api_hub.md\":\"LAmAFqTQ\",\"api_index.md\":\"7k9z3nEC\",\"api_init.md\":\"CRGsoW-4\",\"api_instancenorm.md\":\"DLI9QpOZ\",\"api_io.md\":\"B4QJ8i4z\",\"api_lazy.md\":\"CncGuZCr\",\"api_linear.md\":\"oksodNM4\",\"api_loss.md\":\"j_BqCfCJ\",\"api_lr_scheduler.md\":\"X8HZdXpB\",\"api_mkl.md\":\"BjC9gPeB\",\"api_mkldnn.md\":\"vxGeiDLe\",\"api_mnist.md\":\"y5npcclL\",\"api_models.md\":\"SRUf7VVL\",\"api_module.md\":\"BwrS1O16\",\"api_modules.md\":\"QlSgpybu\",\"api_multiprocessing.md\":\"DQUtvEWY\",\"api_nn.md\":\"COPAhDKB\",\"api_normalization.md\":\"ByrPVG7H\",\"api_onnx.md\":\"SEclrq_a\",\"api_openmp.md\":\"BT-Xb62x\",\"api_ops.md\":\"BQq94cRn\",\"api_optim.md\":\"Dn-RoM6r\",\"api_optimizer.md\":\"C2B0c4Z4\",\"api_parameter.md\":\"BKqpCoGr\",\"api_pooling.md\":\"D0thcWXT\",\"api_primitives.md\":\"D9obA82D\",\"api_rmsprop.md\":\"5QmwGE08\",\"api_sampler.md\":\"FxRjpOCf\",\"api_serialization.md\":\"DHRYYFog\",\"api_sgd.md\":\"C_yFSfpK\",\"api_sparse.md\":\"JEGqpwR5\",\"api_stax.md\":\"FE2KuGtx\",\"api_tensorplay.md\":\"CKZuHRE1\",\"api_transforms.md\":\"2p8MgsrV\",\"api_triton.md\":\"Cjx-7JMC\",\"api_types.md\":\"CXoOl-31\",\"api_utils.md\":\"D3F7mqqh\",\"api_vision.md\":\"DLc3G25z\",\"api_viz.md\":\"B2hlZ2Gl\",\"api_worker.md\":\"B5lclh1L\",\"blog_index.md\":\"D21wAZT6\",\"blog_posts_deep-dive-p10-dispatcher.md\":\"Ca8YFPud\",\"blog_posts_tensorplay-architecture-design.md\":\"B8x6QRfn\",\"blog_posts_tpx-autograd-decoupling.md\":\"D7yIFGv8\",\"changelog.md\":\"CfqvsU-D\",\"community_index.md\":\"DsvJKdP0\",\"contributing.md\":\"reV6tBNP\",\"ecosystem_index.md\":\"C5wl-ECg\",\"guide_architecture.md\":\"dtwAcikV\",\"guide_getting-started.md\":\"DA8shq5l\",\"guide_install.md\":\"CFhD7ThC\",\"guide_quickstart.md\":\"DmWxuskA\",\"guide_resources.md\":\"CvwzxFzL\",\"guide_tutorials.md\":\"DIGxDztk\",\"guide_tutorials_cnn-classification.md\":\"BW6UVjdS\",\"guide_tutorials_custom-autograd.md\":\"CisYHV9c\",\"guide_tutorials_linear-regression.md\":\"CE8n7LhM\",\"guide_what-is-tensorplay.md\":\"EVupmN6e\",\"index.md\":\"BLDTU91p\",\"join_index.md\":\"gMCm4TUT\",\"privacy_index.md\":\"CJIqUJMr\",\"subscribe_index.md\":\"B4TN1i3y\",\"zh_about_index.md\":\"-3pTOuqZ\",\"zh_api__c.md\":\"MPrSxP_X\",\"zh_api__reduction.md\":\"qV6ouOlq\",\"zh_api_activation.md\":\"DhmY2P-X\",\"zh_api_adagrad.md\":\"DBlRXNHF\",\"zh_api_adam.md\":\"C0nw-mF4\",\"zh_api_adamw.md\":\"D-lYkaTv\",\"zh_api_alexnet.md\":\"BWVWH3KT\",\"zh_api_amp.md\":\"DmfeKcbR\",\"zh_api_audio.md\":\"DV8jfVkL\",\"zh_api_autocast_mode.md\":\"CA5EsxSP\",\"zh_api_autograd.md\":\"D2gfxv4i\",\"zh_api_backend.md\":\"D7d0LjCA\",\"zh_api_backends.md\":\"DI7dZt-1\",\"zh_api_batchnorm.md\":\"QaaaODQC\",\"zh_api_collate.md\":\"CkgzFnj7\",\"zh_api_common_types.md\":\"Bi-t-p8_\",\"zh_api_comparison.md\":\"8mWeXUIV\",\"zh_api_container.md\":\"3WPRuOa5\",\"zh_api_conv.md\":\"FT4rQ48Y\",\"zh_api_cpp.md\":\"Dg31fkWU\",\"zh_api_cuda.md\":\"Ftjr_r5j\",\"zh_api_data.md\":\"DXafuWdu\",\"zh_api_dataloader.md\":\"9lTMqcrA\",\"zh_api_dataset.md\":\"BjaAZvTL\",\"zh_api_datasets.md\":\"rRe-_lwa\",\"zh_api_dropout.md\":\"DicyjWrW\",\"zh_api_flatten.md\":\"CiGze1E1\",\"zh_api_folder.md\":\"C0ip8jxS\",\"zh_api_function.md\":\"CC0XVXv7\",\"zh_api_functional.md\":\"CKRoagcS\",\"zh_api_grad_mode.md\":\"BAdK9C3A\",\"zh_api_grad_scaler.md\":\"ayjcYf_m\",\"zh_api_hooks.md\":\"lgFFJBfG\",\"zh_api_hub.md\":\"BU3mLuXZ\",\"zh_api_index.md\":\"XELyvdx1\",\"zh_api_init.md\":\"BpuF-xTb\",\"zh_api_instancenorm.md\":\"Bx8asyFc\",\"zh_api_io.md\":\"CKfyH_yr\",\"zh_api_lazy.md\":\"Dh-43yXH\",\"zh_api_linear.md\":\"DAJwSbeo\",\"zh_api_loss.md\":\"BtTzGqB8\",\"zh_api_lr_scheduler.md\":\"Bx57Gcb3\",\"zh_api_mkl.md\":\"BTQ2a__w\",\"zh_api_mkldnn.md\":\"CXkzUfQN\",\"zh_api_mnist.md\":\"DRdX7jO1\",\"zh_api_models.md\":\"FzTgqXB3\",\"zh_api_module.md\":\"yXevpwKU\",\"zh_api_modules.md\":\"4fFY20ZD\",\"zh_api_multiprocessing.md\":\"um9au9b1\",\"zh_api_nn.md\":\"CplqfocR\",\"zh_api_normalization.md\":\"hobNBqmi\",\"zh_api_onnx.md\":\"Ddz87Msq\",\"zh_api_openmp.md\":\"i1AKxeou\",\"zh_api_ops.md\":\"9Jj9Wr54\",\"zh_api_optim.md\":\"Dw0vGXeU\",\"zh_api_optimizer.md\":\"BqVotpYJ\",\"zh_api_parameter.md\":\"C3WaBDRJ\",\"zh_api_pooling.md\":\"KQjmVa32\",\"zh_api_primitives.md\":\"asKXZhlA\",\"zh_api_rmsprop.md\":\"CZe0TE5A\",\"zh_api_sampler.md\":\"B5rPkeW0\",\"zh_api_serialization.md\":\"DkXFDqSw\",\"zh_api_sgd.md\":\"sm0U3Vw6\",\"zh_api_sparse.md\":\"CejENDG3\",\"zh_api_stax.md\":\"CGuqno6N\",\"zh_api_tensorplay.md\":\"CNtf_oqw\",\"zh_api_transforms.md\":\"zevP9TO2\",\"zh_api_types.md\":\"DU2x97wA\",\"zh_api_utils.md\":\"DPEFHCAQ\",\"zh_api_vision.md\":\"DLR640Rq\",\"zh_api_viz.md\":\"B512LDgq\",\"zh_api_worker.md\":\"BUorbrDl\",\"zh_blog_index.md\":\"DVTzOKJL\",\"zh_blog_posts_deep-dive-p10-dispatcher.md\":\"DC8lY5o1\",\"zh_blog_posts_tensorplay-architecture-design.md\":\"CSq3TUdr\",\"zh_blog_posts_tpx-autograd-decoupling.md\":\"DLZzwXhu\",\"zh_changelog.md\":\"6PPzigGg\",\"zh_community_index.md\":\"EwEOzVj_\",\"zh_contributing.md\":\"CGGMcTSp\",\"zh_ecosystem_index.md\":\"yUKOvCqU\",\"zh_guide_architecture.md\":\"DCtlxIRq\",\"zh_guide_getting-started.md\":\"BKIhLfTl\",\"zh_guide_install.md\":\"D4h-Og8N\",\"zh_guide_quickstart.md\":\"CDQ5vuiK\",\"zh_guide_resources.md\":\"C7rFLh8M\",\"zh_guide_tutorials.md\":\"ZnE-wF46\",\"zh_guide_tutorials_cnn-classification.md\":\"B4Ud8y-D\",\"zh_guide_tutorials_custom-autograd.md\":\"BYkwBgzh\",\"zh_guide_tutorials_linear-regression.md\":\"BnHJUJXt\",\"zh_guide_what-is-tensorplay.md\":\"DqKe5Ta0\",\"zh_index.md\":\"CZTb8-i1\",\"zh_join_index.md\":\"CiJpMZ-N\",\"zh_privacy_index.md\":\"sYKLed_z\",\"zh_subscribe_index.md\":\"B7p3pDqA\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"TensorPlay\",\"description\":\"A transparent, educational, and PyTorch-compatible deep learning framework.\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"logo\":\"/images/logo-0.png\",\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/bluemoon-o2/tensorplay\"}],\"search\":{\"provider\":\"local\",\"options\":{\"locales\":{\"zh\":{\"translations\":{\"button\":{\"buttonText\":\"搜索文档\",\"buttonAriaLabel\":\"搜索文档\"},\"modal\":{\"noResultsText\":\"无法找到相关结果\",\"resetButtonTitle\":\"清除查询条件\",\"footer\":{\"selectText\":\"选择\",\"navigateText\":\"切换\",\"closeText\":\"关闭\"}}}}}}}},\"locales\":{\"root\":{\"label\":\"English\",\"lang\":\"en\",\"description\":\"A simple deep learning framework designed for educational purposes and small-scale experiments.\",\"themeConfig\":{\"nav\":[{\"text\":\"Learn\",\"items\":[{\"text\":\"Getting Started\",\"link\":\"/guide/getting-started\"},{\"text\":\"Tutorials\",\"link\":\"/guide/tutorials\"}]},{\"text\":\"Community\",\"link\":\"/community/\"},{\"text\":\"Projects\",\"link\":\"/ecosystem/\"},{\"text\":\"Docs\",\"link\":\"/api/\"},{\"text\":\"Blog & News\",\"link\":\"/blog/\"},{\"text\":\"About\",\"link\":\"/about/\"},{\"text\":\"JOIN\",\"link\":\"/join/\"}],\"sidebar\":{\"/guide/\":[{\"text\":\"Introduction\",\"items\":[{\"text\":\"What is TensorPlay?\",\"link\":\"/guide/what-is-tensorplay\"},{\"text\":\"Getting Started\",\"link\":\"/guide/getting-started\"},{\"text\":\"Installation\",\"link\":\"/guide/install\"},{\"text\":\"Quickstart\",\"link\":\"/guide/quickstart\"},{\"text\":\"Architecture\",\"link\":\"/guide/architecture\"}]},{\"text\":\"Guides & Resources\",\"items\":[{\"text\":\"Tutorials\",\"link\":\"/guide/tutorials\",\"items\":[{\"text\":\"Image Classification\",\"link\":\"/guide/tutorials/cnn-classification\"},{\"text\":\"Linear Regression\",\"link\":\"/guide/tutorials/linear-regression\"},{\"text\":\"Custom Autograd\",\"link\":\"/guide/tutorials/custom-autograd\"}]},{\"text\":\"Resources\",\"link\":\"/guide/resources\"},{\"text\":\"API Reference\",\"link\":\"/api/\"}]}],\"/api/\":[{\"text\":\"Core API\",\"items\":[{\"text\":\"Overview\",\"link\":\"/api/\"},{\"text\":\"tensorplay\",\"link\":\"/api/tensorplay\"},{\"text\":\"autograd\",\"link\":\"/api/autograd\"},{\"text\":\"functional\",\"link\":\"/api/functional\"},{\"text\":\"optim\",\"link\":\"/api/optim\"},{\"text\":\"cuda\",\"link\":\"/api/cuda\"}]},{\"text\":\"Neural Networks (nn)\",\"collapsed\":false,\"items\":[{\"text\":\"nn\",\"link\":\"/api/nn\"},{\"text\":\"Modules\",\"link\":\"/api/modules\"},{\"text\":\"Linear Layers\",\"link\":\"/api/linear\"},{\"text\":\"Convolution Layers\",\"link\":\"/api/conv\"},{\"text\":\"Pooling Layers\",\"link\":\"/api/pooling\"},{\"text\":\"Activation Functions\",\"link\":\"/api/activation\"},{\"text\":\"Normalization Layers\",\"link\":\"/api/normalization\"},{\"text\":\"Loss Functions\",\"link\":\"/api/loss\"},{\"text\":\"Dropout Layers\",\"link\":\"/api/dropout\"},{\"text\":\"Container\",\"link\":\"/api/container\"}]}]},\"footer\":{\"message\":\"Released under the Apache 2.0 License.\",\"copyright\":\"Copyright © 2025 zlx. All rights reserved.\"}}},\"zh\":{\"label\":\"简体中文\",\"lang\":\"zh\",\"link\":\"/zh/\",\"description\":\"一个适合学习者、兼容 PyTorch 的深度学习框架。\",\"themeConfig\":{\"nav\":[{\"text\":\"学习\",\"items\":[{\"text\":\"快速开始\",\"link\":\"/zh/guide/getting-started\"},{\"text\":\"教程\",\"link\":\"/zh/guide/tutorials\"}]},{\"text\":\"社区\",\"link\":\"/zh/community/\"},{\"text\":\"项目\",\"link\":\"/zh/ecosystem/\"},{\"text\":\"文档\",\"link\":\"/zh/api/\"},{\"text\":\"博客与新闻\",\"link\":\"/zh/blog/\"},{\"text\":\"关于\",\"link\":\"/zh/about/\"},{\"text\":\"加入我们\",\"link\":\"/zh/join/\"}],\"sidebar\":{\"/zh/guide/\":[{\"text\":\"介绍\",\"items\":[{\"text\":\"什么是 TensorPlay?\",\"link\":\"/zh/guide/what-is-tensorplay\"},{\"text\":\"快速开始\",\"link\":\"/zh/guide/getting-started\"},{\"text\":\"安装\",\"link\":\"/zh/guide/install\"},{\"text\":\"快速入门\",\"link\":\"/zh/guide/quickstart\"},{\"text\":\"架构设计\",\"link\":\"/zh/guide/architecture\"}]},{\"text\":\"指南与资源\",\"items\":[{\"text\":\"教程\",\"link\":\"/zh/guide/tutorials\",\"items\":[{\"text\":\"图像分类\",\"link\":\"/zh/guide/tutorials/cnn-classification\"},{\"text\":\"线性回归\",\"link\":\"/zh/guide/tutorials/linear-regression\"},{\"text\":\"自定义自动微分\",\"link\":\"/zh/guide/tutorials/custom-autograd\"}]},{\"text\":\"资源\",\"link\":\"/zh/guide/resources\"},{\"text\":\"API 参考\",\"link\":\"/zh/api/\"}]}],\"/zh/api/\":[{\"text\":\"核心 API\",\"items\":[{\"text\":\"概览\",\"link\":\"/zh/api/\"},{\"text\":\"tensorplay\",\"link\":\"/zh/api/tensorplay\"},{\"text\":\"autograd\",\"link\":\"/zh/api/autograd\"},{\"text\":\"functional\",\"link\":\"/zh/api/functional\"},{\"text\":\"optim\",\"link\":\"/zh/api/optim\"},{\"text\":\"cuda\",\"link\":\"/zh/api/cuda\"}]},{\"text\":\"神经网络 (nn)\",\"collapsed\":false,\"items\":[{\"text\":\"nn\",\"link\":\"/zh/api/nn\"},{\"text\":\"Modules\",\"link\":\"/zh/api/modules\"},{\"text\":\"Linear Layers\",\"link\":\"/zh/api/linear\"},{\"text\":\"Convolution Layers\",\"link\":\"/zh/api/conv\"},{\"text\":\"Pooling Layers\",\"link\":\"/zh/api/pooling\"},{\"text\":\"Activation Functions\",\"link\":\"/zh/api/activation\"},{\"text\":\"Normalization Layers\",\"link\":\"/zh/api/normalization\"},{\"text\":\"Loss Functions\",\"link\":\"/zh/api/loss\"},{\"text\":\"Dropout Layers\",\"link\":\"/zh/api/dropout\"},{\"text\":\"Container\",\"link\":\"/zh/api/container\"}]}]},\"footer\":{\"message\":\"基于 Apache 2.0 许可发布。\",\"copyright\":\"版权所有 © 2025 zlx。保留所有权利。\"}}}},\"scrollOffset\":134,\"cleanUrls\":true}");</script>
    
  </body>
</html>