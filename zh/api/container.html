<!DOCTYPE html>
<html lang="zh" dir="ltr">
  <head>
    <meta charset="utf-8">
    
    <title>tensorplay.nn.modules.container | TensorPlay</title>
    <meta name="description" content="一个适合学习者、兼容 PyTorch 的深度学习框架。">
    <meta name="generator" content="VitePress v1.6.4">
    <link rel="preload stylesheet" href="/assets/style.B8dWbnM-.css" as="style">
    <link rel="preload stylesheet" href="/vp-icons.css" as="style">
    
    <script type="module" src="/assets/app.Dd0Bb3z9.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/theme.DpmSWF5K.js">
    <link rel="modulepreload" href="/assets/chunks/framework.DWXU4yX9.js">
    <link rel="modulepreload" href="/assets/zh_api_container.md.ecpuGL9c.lean.js">
    <link rel="icon" type="image/png" href="/images/logo-0.png">
    <link rel="apple-touch-icon" href="/images/logo-0.png">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <meta name="keywords" content="deep learning, machine learning, ai framework, pytorch compatible, educational, c++, python, tensorplay, 深度学习, 机器学习, 自动微分">
    <meta name="author" content="TensorPlay Team">
    <meta property="og:type" content="website">
    <meta property="og:title" content="TensorPlay | Transparent &amp; Compatible AI Framework">
    <meta property="og:description" content="A transparent, educational, and PyTorch-compatible deep learning framework for the modern AI era.">
    <meta property="og:site_name" content="TensorPlay">
    <meta property="og:url" content="https://www.tensorplay.cn">
    <meta property="og:image" content="https://www.tensorplay.cn/images/logo-1.png">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="TensorPlay | Transparent &amp; Compatible AI Framework">
    <meta name="twitter:description" content="A transparent, educational, and PyTorch-compatible deep learning framework for the modern AI era.">
    <meta name="twitter:image" content="https://www.tensorplay.cn/images/logo-1.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@300;400;500;600;700&amp;display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-5d98c3a5><!--[--><canvas id="cyberpunk-particles" data-v-36680640></canvas><!----><!--]--><!--[--><span tabindex="-1" data-v-0b0ada53></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-0b0ada53>Skip to content</a><!--]--><!----><header class="VPNav" data-v-5d98c3a5 data-v-ae24b3ad><div class="VPNavBar" data-v-ae24b3ad data-v-6aa21345><div class="wrapper" data-v-6aa21345><div class="container" data-v-6aa21345><div class="title" data-v-6aa21345><div class="VPNavBarTitle has-sidebar" data-v-6aa21345 data-v-1168a8e4><a class="title" href="/zh/" data-v-1168a8e4><!--[--><!--]--><!--[--><img class="VPImage logo" src="/images/logo-0.png" alt data-v-8426fc1a><!--]--><span data-v-1168a8e4>TensorPlay</span><!--[--><!--]--></a></div></div><div class="content" data-v-6aa21345><div class="content-body" data-v-6aa21345><!--[--><!--]--><div class="VPNavBarSearch search" data-v-6aa21345><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="搜索文档"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">搜索文档</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-6aa21345 data-v-dc692963><span id="main-nav-aria-label" class="visually-hidden" data-v-dc692963> Main Navigation </span><!--[--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-dc692963 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-cf11d7a2><span class="text" data-v-cf11d7a2><!----><span data-v-cf11d7a2>学习</span><span class="vpi-chevron-down text-icon" data-v-cf11d7a2></span></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><div class="items" data-v-b98bc113><!--[--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/zh/guide/getting-started.html" data-v-35975db6><!--[--><span data-v-35975db6>快速开始</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/zh/guide/tutorials.html" data-v-35975db6><!--[--><span data-v-35975db6>教程</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/community/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>社区</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/ecosystem/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>项目</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/api/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>文档</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/blog/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>博客与新闻</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/about/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>关于</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/join/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>加入我们</span><!--]--></a><!--]--><!--]--></nav><div class="VPFlyout VPNavBarTranslations translations" data-v-6aa21345 data-v-88af2de4 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="Change language" data-v-cf11d7a2><span class="text" data-v-cf11d7a2><span class="vpi-languages option-icon" data-v-cf11d7a2></span><!----><span class="vpi-chevron-down text-icon" data-v-cf11d7a2></span></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><!----><!--[--><!--[--><div class="items" data-v-88af2de4><p class="title" data-v-88af2de4>简体中文</p><!--[--><div class="VPMenuLink" data-v-88af2de4 data-v-35975db6><a class="VPLink link" href="/api/container.html" data-v-35975db6><!--[--><span data-v-35975db6>English</span><!--]--></a></div><!--]--></div><!--]--><!--]--></div></div></div><div class="VPNavBarAppearance appearance" data-v-6aa21345 data-v-6c893767><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-6c893767 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-6aa21345 data-v-0394ad82 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/bluemoon-o2/tensorplay" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-6aa21345 data-v-bb2aa2f0 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-cf11d7a2><span class="vpi-more-horizontal icon" data-v-cf11d7a2></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><!----><!--[--><!--[--><div class="group translations" data-v-bb2aa2f0><p class="trans-title" data-v-bb2aa2f0>简体中文</p><!--[--><div class="VPMenuLink" data-v-bb2aa2f0 data-v-35975db6><a class="VPLink link" href="/api/container.html" data-v-35975db6><!--[--><span data-v-35975db6>English</span><!--]--></a></div><!--]--></div><div class="group" data-v-bb2aa2f0><div class="item appearance" data-v-bb2aa2f0><p class="label" data-v-bb2aa2f0>Appearance</p><div class="appearance-action" data-v-bb2aa2f0><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-bb2aa2f0 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div></div></div><div class="group" data-v-bb2aa2f0><div class="item social-links" data-v-bb2aa2f0><div class="VPSocialLinks social-links-list" data-v-bb2aa2f0 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/bluemoon-o2/tensorplay" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-6aa21345 data-v-e5dd9c1c><span class="container" data-v-e5dd9c1c><span class="top" data-v-e5dd9c1c></span><span class="middle" data-v-e5dd9c1c></span><span class="bottom" data-v-e5dd9c1c></span></span></button></div></div></div></div><div class="divider" data-v-6aa21345><div class="divider-line" data-v-6aa21345></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-5d98c3a5 data-v-a6f0e41e><div class="container" data-v-a6f0e41e><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-a6f0e41e><span class="vpi-align-left menu-icon" data-v-a6f0e41e></span><span class="menu-text" data-v-a6f0e41e>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-a6f0e41e data-v-8a42e2b4><button data-v-8a42e2b4>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-5d98c3a5 data-v-319d5ca6><div class="curtain" data-v-319d5ca6></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-319d5ca6><span class="visually-hidden" id="sidebar-aria-label" data-v-319d5ca6> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-c40bc020><section class="VPSidebarItem level-0" data-v-c40bc020 data-v-b3fd67f8><div class="item" role="button" tabindex="0" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><h2 class="text" data-v-b3fd67f8>核心 API</h2><!----></div><div class="items" data-v-b3fd67f8><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>概览</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/tensorplay.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>tensorplay</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/autograd.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>autograd</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/functional.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>functional</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/optim.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>optim</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/cuda.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>cuda</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-c40bc020><section class="VPSidebarItem level-0 collapsible has-active" data-v-c40bc020 data-v-b3fd67f8><div class="item" role="button" tabindex="0" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><h2 class="text" data-v-b3fd67f8>神经网络 (nn)</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-b3fd67f8><span class="vpi-chevron-right caret-icon" data-v-b3fd67f8></span></div></div><div class="items" data-v-b3fd67f8><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/nn.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>nn</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/modules.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Modules</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/linear.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Linear Layers</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/conv.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Convolution Layers</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/pooling.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Pooling Layers</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/activation.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Activation Functions</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/normalization.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Normalization Layers</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/loss.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Loss Functions</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/dropout.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Dropout Layers</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/container.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Container</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-5d98c3a5 data-v-1428d186><div class="VPDoc has-sidebar has-aside" data-v-1428d186 data-v-39a288b8><!--[--><!--]--><div class="container" data-v-39a288b8><div class="aside" data-v-39a288b8><div class="aside-curtain" data-v-39a288b8></div><div class="aside-container" data-v-39a288b8><div class="aside-content" data-v-39a288b8><div class="VPDocAside" data-v-39a288b8 data-v-3f215769><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-3f215769 data-v-a5bbad30><div class="content" data-v-a5bbad30><div class="outline-marker" data-v-a5bbad30></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-a5bbad30>On this page</div><ul class="VPDocOutlineItem root" data-v-a5bbad30 data-v-b933a997><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-3f215769></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-39a288b8><div class="content-container" data-v-39a288b8><!--[--><!--[--><!--[--><!----><!--]--><!--]--><!--]--><main class="main" data-v-39a288b8><div style="position:relative;" class="vp-doc _zh_api_container" data-v-39a288b8><div><div class="warning custom-block"><p class="custom-block-title">WARNING</p><p>该页面尚未翻译。 以下内容为英文原版。</p></div><h1 id="tensorplay-nn-modules-container" tabindex="-1">tensorplay.nn.modules.container <a class="header-anchor" href="#tensorplay-nn-modules-container" aria-label="Permalink to &quot;tensorplay.nn.modules.container&quot;">​</a></h1><h2 id="classes" tabindex="-1">Classes <a class="header-anchor" href="#classes" aria-label="Permalink to &quot;Classes&quot;">​</a></h2><h3 id="class-moduledict-source" tabindex="-1"><code>class ModuleDict</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L473" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-moduledict-source" aria-label="Permalink to &quot;`class ModuleDict` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L473)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">ModuleDict(modules: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Optional[Mapping[str, Module]]&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;None&#39;</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>Module</code></p><p>Holds submodules in a dictionary.</p><p><code>~tensorplay.nn.ModuleDict</code> can be indexed like a regular Python dictionary, but modules it contains are properly registered, and will be visible by all <code>~tensorplay.nn.Module</code> methods.</p><p><code>~tensorplay.nn.ModuleDict</code> is an <strong>ordered</strong> dictionary that respects</p><ul><li><p>the order of insertion, and</p></li><li><p>in <code>~tensorplay.nn.ModuleDict.update</code>, the order of the merged <code>OrderedDict</code>, <code>dict</code> (started from Python 3.6) or another <code>~tensorplay.nn.ModuleDict</code> (the argument to <code>~tensorplay.nn.ModuleDict.update</code>).</p></li></ul><p>Note that <code>~tensorplay.nn.ModuleDict.update</code> with other unordered mapping types (e.g., Python&#39;s plain <code>dict</code> before Python version 3.6) does not preserve the order of the merged mapping.</p><h4 id="args" tabindex="-1">Args <a class="header-anchor" href="#args" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>modules</strong> (<code>iterable, optional</code>): a mapping (dictionary) of (string: module) or an iterable of key-value pairs of type (string, module)</li></ul><h4 id="example" tabindex="-1">Example <a class="header-anchor" href="#example" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">class</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> MyModule</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">nn</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">Module</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> __init__</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self) -&gt; </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    super</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">().</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">__init__</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.choices </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.ModuleDict(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        {</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;conv&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: nn.Conv2d(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;pool&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: nn.MaxPool2d(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)}</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    )</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.activations </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.ModuleDict(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;lrelu&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, nn.LeakyReLU()], [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;prelu&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, nn.PReLU()]]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    )</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> forward</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, x, choice, act):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.choices[choice](x)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.activations[act](x)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><details><summary>Methods</summary><h4 id="init-self-modules-optional-mapping-str-module-none-none-source" tabindex="-1"><code>__init__(self, modules: &#39;Optional[Mapping[str, Module]]&#39; = None) -&gt; &#39;None&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L517" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-modules-optional-mapping-str-module-none-none-source" aria-label="Permalink to &quot;`__init__(self, modules: &#39;Optional[Mapping[str, Module]]&#39; = None) -&gt; &#39;None&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L517)&quot;">​</a></h4><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p><hr><h4 id="add-module-self-name-str-module-optional-forwardref-module-none-source" tabindex="-1"><code>add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#add-module-self-name-str-module-optional-forwardref-module-none-source" aria-label="Permalink to &quot;`add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667)&quot;">​</a></h4><p>Add a child module to the current module.</p><p>The module can be accessed as an attribute using the given name.</p><h4 id="args-1" tabindex="-1">Args <a class="header-anchor" href="#args-1" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the child module. The child module can be accessed from this module using the given name</li><li><strong>module</strong> (<code>Module</code>): child module to be added to the module.</li></ul><hr><h4 id="apply-self-fn-callable-forwardref-module-nonetype-self-source" tabindex="-1"><code>apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#apply-self-fn-callable-forwardref-module-nonetype-self-source" aria-label="Permalink to &quot;`apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990)&quot;">​</a></h4><p>Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self.</p><p>Typical use includes initializing the parameters of a model (see also <code>nn-init-doc</code>).</p><h4 id="args-2" tabindex="-1">Args <a class="header-anchor" href="#args-2" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>fn</strong> (``Module<code> -&gt; None</code>): function to be applied to each submodule</li></ul><h4 id="returns" tabindex="-1">Returns <a class="header-anchor" href="#returns" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="example-1" tabindex="-1">Example <a class="header-anchor" href="#example-1" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@tensorplay.no_grad</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> init_weights</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        m.weight.fill_(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m.weight)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net.apply(init_weights)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><hr><h4 id="bfloat16-self-self-source" tabindex="-1"><code>bfloat16(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#bfloat16-self-self-source" aria-label="Permalink to &quot;`bfloat16(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-1" tabindex="-1">Returns <a class="header-anchor" href="#returns-1" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source" tabindex="-1"><code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source" aria-label="Permalink to &quot;`buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627)&quot;">​</a></h4><p>Return an iterator over module buffers.</p><h4 id="args-3" tabindex="-1">Args <a class="header-anchor" href="#args-3" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.</li></ul><h4 id="yields" tabindex="-1">Yields <a class="header-anchor" href="#yields" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>tensorplay.Tensor: module buffer
</code></pre><h4 id="example-2" tabindex="-1">Example <a class="header-anchor" href="#example-2" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.buffers():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf), buf.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="children-self-collections-abc-iterator-module-source" tabindex="-1"><code>children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#children-self-collections-abc-iterator-module-source" aria-label="Permalink to &quot;`children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681)&quot;">​</a></h4><p>Return an iterator over immediate children modules.</p><h4 id="yields-1" tabindex="-1">Yields <a class="header-anchor" href="#yields-1" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a child module</li></ul><hr><h4 id="clear-self-none-source" tabindex="-1"><code>clear(self) -&gt; &#39;None&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L544" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#clear-self-none-source" aria-label="Permalink to &quot;`clear(self) -&gt; &#39;None&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L544)&quot;">​</a></h4><p>Remove all items from the ModuleDict.</p><hr><h4 id="compile-self-args-kwargs-source" tabindex="-1"><code>compile(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#compile-self-args-kwargs-source" aria-label="Permalink to &quot;`compile(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944)&quot;">​</a></h4><p>Compile this Module&#39;s forward using <code>tensorplay.compile</code>.</p><p>This Module&#39;s <code>__call__</code> method is compiled and all arguments are passed as-is to <code>tensorplay.compile</code>.</p><p>See <code>tensorplay.compile</code> for details on the arguments for this function.</p><hr><h4 id="cpu-self-self-source" tabindex="-1"><code>cpu(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cpu-self-self-source" aria-label="Permalink to &quot;`cpu(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050)&quot;">​</a></h4><p>Move all model parameters and buffers to the CPU.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-2" tabindex="-1">Returns <a class="header-anchor" href="#returns-2" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source" tabindex="-1"><code>cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source" aria-label="Permalink to &quot;`cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031)&quot;">​</a></h4><p>Move all model parameters and buffers to the GPU.</p><p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on GPU while being optimized.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-4" tabindex="-1">Args <a class="header-anchor" href="#args-4" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>int, optional</code>): if specified, all parameters will be copied to that device</li></ul><h4 id="returns-3" tabindex="-1">Returns <a class="header-anchor" href="#returns-3" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="double-self-self-source" tabindex="-1"><code>double(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#double-self-self-source" aria-label="Permalink to &quot;`double(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-4" tabindex="-1">Returns <a class="header-anchor" href="#returns-4" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="eval-self-self-source" tabindex="-1"><code>eval(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#eval-self-self-source" aria-label="Permalink to &quot;`eval(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808)&quot;">​</a></h4><p>Set the module in evaluation mode.</p><p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e. whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, etc.</p><p>This is equivalent with <code>self.train(False) &lt;tensorplay.nn.Module.train&gt;</code>.</p><p>See <code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.</p><h4 id="returns-5" tabindex="-1">Returns <a class="header-anchor" href="#returns-5" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="extra-repr-self-str-source" tabindex="-1"><code>extra_repr(self) -&gt; str</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2884" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#extra-repr-self-str-source" aria-label="Permalink to &quot;`extra_repr(self) -&gt; str` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2884)&quot;">​</a></h4><p>Return the extra representation of the module.</p><p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p><hr><h4 id="float-self-self-source" tabindex="-1"><code>float(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#float-self-self-source" aria-label="Permalink to &quot;`float(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-6" tabindex="-1">Returns <a class="header-anchor" href="#returns-6" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="forward-self-input-any-none-source" tabindex="-1"><code>forward(self, *input: Any) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L388" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#forward-self-input-any-none-source" aria-label="Permalink to &quot;`forward(self, *input: Any) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L388)&quot;">​</a></h4><p>Define the computation performed at every call.</p><p>Should be overridden by all subclasses.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code> instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.</p></div><hr><h4 id="get-buffer-self-target-str-tensor-source" tabindex="-1"><code>get_buffer(self, target: str) -&gt; &#39;Tensor&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-buffer-self-target-str-tensor-source" aria-label="Permalink to &quot;`get_buffer(self, target: str) -&gt; &#39;Tensor&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881)&quot;">​</a></h4><p>Return the buffer given by <code>target</code> if it exists, otherwise throw an error.</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-5" tabindex="-1">Args <a class="header-anchor" href="#args-5" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the buffer to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-7" tabindex="-1">Returns <a class="header-anchor" href="#returns-7" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.Tensor: The buffer referenced by ``target``
</code></pre><h4 id="raises" tabindex="-1">Raises <a class="header-anchor" href="#raises" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not a buffer</li></ul><hr><h4 id="get-extra-state-self-any-source" tabindex="-1"><code>get_extra_state(self) -&gt; Any</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-extra-state-self-any-source" aria-label="Permalink to &quot;`get_extra_state(self) -&gt; Any` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917)&quot;">​</a></h4><p>Return any extra state to include in the module&#39;s state_dict.</p><p>Implement this and a corresponding <code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module&#39;s <code>state_dict()</code>.</p><p>Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.</p><h4 id="returns-8" tabindex="-1">Returns <a class="header-anchor" href="#returns-8" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>object</strong>: Any extra state to store in the module&#39;s state_dict</li></ul><hr><h4 id="get-parameter-self-target-str-parameter-source" tabindex="-1"><code>get_parameter(self, target: str) -&gt; &#39;Parameter&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-parameter-self-target-str-parameter-source" aria-label="Permalink to &quot;`get_parameter(self, target: str) -&gt; &#39;Parameter&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845)&quot;">​</a></h4><p>Return the parameter given by <code>target</code> if it exists, otherwise throw an error.</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-6" tabindex="-1">Args <a class="header-anchor" href="#args-6" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the Parameter to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-9" tabindex="-1">Returns <a class="header-anchor" href="#returns-9" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Parameter: The Parameter referenced by ``target``
</code></pre><h4 id="raises-1" tabindex="-1">Raises <a class="header-anchor" href="#raises-1" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not an <code>nn.Parameter</code></li></ul><hr><h4 id="get-submodule-self-target-str-module-source" tabindex="-1"><code>get_submodule(self, target: str) -&gt; &#39;Module&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-submodule-self-target-str-module-source" aria-label="Permalink to &quot;`get_submodule(self, target: str) -&gt; &#39;Module&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699)&quot;">​</a></h4><p>Return the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> which has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(&quot;net_b.linear&quot;)</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(&quot;net_b.net_c.conv&quot;)</code>.</p><p>The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.</p><h4 id="args-7" tabindex="-1">Args <a class="header-anchor" href="#args-7" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li></ul><h4 id="returns-10" tabindex="-1">Returns <a class="header-anchor" href="#returns-10" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Module: The submodule referenced by ``target``
</code></pre><h4 id="raises-2" tabindex="-1">Raises <a class="header-anchor" href="#raises-2" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If at any point along the path resulting from the target string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="half-self-self-source" tabindex="-1"><code>half(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#half-self-self-source" aria-label="Permalink to &quot;`half(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-11" tabindex="-1">Returns <a class="header-anchor" href="#returns-11" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="items-self-container-abcs-itemsview-str-module-source" tabindex="-1"><code>items(self) -&gt; &#39;container_abcs.ItemsView[str, Module]&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L564" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#items-self-container-abcs-itemsview-str-module-source" aria-label="Permalink to &quot;`items(self) -&gt; &#39;container_abcs.ItemsView[str, Module]&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L564)&quot;">​</a></h4><p>Return an iterable of the ModuleDict key/value pairs.</p><hr><h4 id="keys-self-container-abcs-keysview-str-source" tabindex="-1"><code>keys(self) -&gt; &#39;container_abcs.KeysView[str]&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L559" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#keys-self-container-abcs-keysview-str-source" aria-label="Permalink to &quot;`keys(self) -&gt; &#39;container_abcs.KeysView[str]&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L559)&quot;">​</a></h4><p>Return an iterable of the ModuleDict keys.</p><hr><h4 id="load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source" tabindex="-1"><code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source" aria-label="Permalink to &quot;`load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436)&quot;">​</a></h4><p>Copy parameters and buffers from <code>state_dict</code> into this module and its descendants.</p><p>If <code>strict</code> is <code>True</code>, then the keys of <code>state_dict</code> must exactly match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function.</p><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>If <code>assign</code> is <code>True</code> the optimizer must be created after the call to <code>load_state_dict</code> unless <code>~tensorplay.__future__.get_swap_module_params_on_conversion</code> is <code>True</code>.</p></div><h4 id="args-8" tabindex="-1">Args <a class="header-anchor" href="#args-8" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state_dict</strong> (<code>dict</code>): a dict containing parameters and persistent buffers.</li><li><strong>strict</strong> (<code>bool, optional</code>): whether to strictly enforce that the keys in <code>state_dict</code> match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function. Default: <code>True</code></li><li><strong>assign</strong> (<code>bool, optional</code>): When set to <code>False</code>, the properties of the tensors in the current module are preserved whereas setting it to <code>True</code> preserves properties of the Tensors in the state dict. The only exception is the <code>requires_grad</code> field of <code>~tensorplay.nn.Parameter</code> for which the value from the module is preserved. Default: <code>False</code></li></ul><h4 id="returns-12" tabindex="-1">Returns <a class="header-anchor" href="#returns-12" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:
    * ``missing_keys`` is a list of str containing any keys that are expected
        by this module but missing from the provided ``state_dict``.
    * ``unexpected_keys`` is a list of str containing the keys that are not
        expected by this module but present in the provided ``state_dict``.
</code></pre><h4 id="note" tabindex="-1">Note <a class="header-anchor" href="#note" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>If a parameter or buffer is registered as ``None`` and its corresponding key
exists in `state_dict`, `load_state_dict` will raise a
``RuntimeError``.
</code></pre><hr><h4 id="modules-self-collections-abc-iterator-module-source" tabindex="-1"><code>modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#modules-self-collections-abc-iterator-module-source" aria-label="Permalink to &quot;`modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710)&quot;">​</a></h4><p>Return an iterator over all modules in the network.</p><h4 id="yields-2" tabindex="-1">Yields <a class="header-anchor" href="#yields-2" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a module in the network</li></ul><h4 id="note-1" tabindex="-1">Note <a class="header-anchor" href="#note-1" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-3" tabindex="-1">Example <a class="header-anchor" href="#example-3" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source" tabindex="-1"><code>named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source" aria-label="Permalink to &quot;`named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650)&quot;">​</a></h4><p>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p><h4 id="args-9" tabindex="-1">Args <a class="header-anchor" href="#args-9" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all buffer names.</li><li><strong>recurse</strong> (<code>bool, optional</code>): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Defaults to True.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated buffers in the result. Defaults to True.</li></ul><h4 id="yields-3" tabindex="-1">Yields <a class="header-anchor" href="#yields-3" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, tensorplay.Tensor): Tuple containing the name and buffer
</code></pre><h4 id="example-4" tabindex="-1">Example <a class="header-anchor" href="#example-4" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_buffers():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_var&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-children-self-collections-abc-iterator-tuple-str-module-source" tabindex="-1"><code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-children-self-collections-abc-iterator-tuple-str-module-source" aria-label="Permalink to &quot;`named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690)&quot;">​</a></h4><p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p><h4 id="yields-4" tabindex="-1">Yields <a class="header-anchor" href="#yields-4" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple containing a name and child module
</code></pre><h4 id="example-5" tabindex="-1">Example <a class="header-anchor" href="#example-5" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, module </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.named_children():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv4&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv5&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(module)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source" tabindex="-1"><code>named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source" aria-label="Permalink to &quot;`named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737)&quot;">​</a></h4><p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p><h4 id="args-10" tabindex="-1">Args <a class="header-anchor" href="#args-10" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>memo</strong>: a memo to store the set of modules already added to the result</li><li><strong>prefix</strong>: a prefix that will be added to the name of the module</li><li><strong>remove_duplicate</strong>: whether to remove the duplicated module instances in the result or not</li></ul><h4 id="yields-5" tabindex="-1">Yields <a class="header-anchor" href="#yields-5" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple of name and module
</code></pre><h4 id="note-2" tabindex="-1">Note <a class="header-anchor" href="#note-2" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-6" tabindex="-1">Example <a class="header-anchor" href="#example-6" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.named_modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;0&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source" tabindex="-1"><code>named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source" aria-label="Permalink to &quot;`named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595)&quot;">​</a></h4><p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p><h4 id="args-11" tabindex="-1">Args <a class="header-anchor" href="#args-11" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all parameter names.</li><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated parameters in the result. Defaults to True.</li></ul><h4 id="yields-6" tabindex="-1">Yields <a class="header-anchor" href="#yields-6" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Parameter): Tuple containing the name and parameter
</code></pre><h4 id="example-7" tabindex="-1">Example <a class="header-anchor" href="#example-7" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_parameters():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source" tabindex="-1"><code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source" aria-label="Permalink to &quot;`parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570)&quot;">​</a></h4><p>Return an iterator over module parameters.</p><p>This is typically passed to an optimizer.</p><h4 id="args-12" tabindex="-1">Args <a class="header-anchor" href="#args-12" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</li></ul><h4 id="yields-7" tabindex="-1">Yields <a class="header-anchor" href="#yields-7" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Parameter</strong>: module parameter</li></ul><h4 id="example-8" tabindex="-1">Example <a class="header-anchor" href="#example-8" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.parameters():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param), param.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="pop-self-key-str-module-source" tabindex="-1"><code>pop(self, key: &#39;str&#39;) -&gt; &#39;Module&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L548" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#pop-self-key-str-module-source" aria-label="Permalink to &quot;`pop(self, key: &#39;str&#39;) -&gt; &#39;Module&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L548)&quot;">​</a></h4><p>Remove key from the ModuleDict and return its module.</p><h4 id="args-13" tabindex="-1">Args <a class="header-anchor" href="#args-13" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>key</strong> (<code>str</code>): key to pop from the ModuleDict</li></ul><hr><h4 id="register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source" tabindex="-1"><code>register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source" aria-label="Permalink to &quot;`register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>This function is deprecated in favor of <code>~tensorplay.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.</p><h4 id="returns-13" tabindex="-1">Returns <a class="header-anchor" href="#returns-13" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source" tabindex="-1"><code>register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source" aria-label="Permalink to &quot;`register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553)&quot;">​</a></h4><p>Add a buffer to the module.</p><p>This is typically used to register a buffer that should not be considered a model parameter. For example, BatchNorm&#39;s <code>running_mean</code> is not a parameter, but is part of the module&#39;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting <code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&#39;s <code>state_dict</code>.</p><p>Buffers can be accessed as attributes using given names.</p><h4 id="args-14" tabindex="-1">Args <a class="header-anchor" href="#args-14" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the buffer. The buffer can be accessed from this module using the given name</li><li><strong>tensor</strong> (<code>Tensor or None</code>): buffer to be registered. If <code>None</code>, then operations that run on buffers, such as <code>cuda</code>, are ignored. If <code>None</code>, the buffer is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li><li><strong>persistent</strong> (<code>bool</code>): whether the buffer is part of this module&#39;s <code>state_dict</code>.</li></ul><h4 id="example-9" tabindex="-1">Example <a class="header-anchor" href="#example-9" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.register_buffer(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_mean&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, tensorplay.zeros(num_features))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><hr><h4 id="register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source" tabindex="-1"><code>register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source" aria-label="Permalink to &quot;`register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592)&quot;">​</a></h4><p>Register a forward hook on the module.</p><p>The hook will be called every time after <code>forward</code> has computed an output.</p><p>If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after <code>forward</code> is called. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-15" tabindex="-1">Args <a class="header-anchor" href="#args-15" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If <code>True</code>, the provided <code>hook</code> will be fired before all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward</code> hooks registered with <code>register_module_forward_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If <code>True</code>, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>always_call</strong> (<code>bool</code>): If <code>True</code> the <code>hook</code> will be run regardless of whether an exception is raised while calling the Module.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-14" tabindex="-1">Returns <a class="header-anchor" href="#returns-14" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source" tabindex="-1"><code>register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source" aria-label="Permalink to &quot;`register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526)&quot;">​</a></h4><p>Register a forward pre-hook on the module.</p><p>The hook will be called every time before <code>forward</code> is invoked.</p><p>If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> of modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> and</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> kwargs</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-16" tabindex="-1">Args <a class="header-anchor" href="#args-16" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward_pre</code> hooks registered with <code>register_module_forward_pre_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If true, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-15" tabindex="-1">Returns <a class="header-anchor" href="#returns-15" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source" tabindex="-1"><code>register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source" aria-label="Permalink to &quot;`register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>The hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:</p><pre><code>1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.
2. If none of the module inputs require gradients, the hook will fire when the gradients are computed
   with respect to module outputs.
3. If none of the module outputs require gradients, then the hooks will not fire.
</code></pre><p>The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_input, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Tensor) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_input</code> and <code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of <code>grad_input</code> in subsequent computations. <code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in <code>grad_input</code> and <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> outputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-17" tabindex="-1">Args <a class="header-anchor" href="#args-17" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward</code> hooks registered with <code>register_module_full_backward_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-16" tabindex="-1">Returns <a class="header-anchor" href="#returns-16" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source" tabindex="-1"><code>register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source" aria-label="Permalink to &quot;`register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287)&quot;">​</a></h4><p>Register a backward pre-hook on the module.</p><p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tuple[Tensor] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of <code>grad_output</code> in subsequent computations. Entries in <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-18" tabindex="-1">Args <a class="header-anchor" href="#args-18" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward_pre</code> hooks registered with <code>register_module_full_backward_pre_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-17" tabindex="-1">Returns <a class="header-anchor" href="#returns-17" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-post-hook-self-hook-source" tabindex="-1"><code>register_load_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-post-hook-self-hook-source" aria-label="Permalink to &quot;`register_load_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226)&quot;">​</a></h4><p>Register a post-hook to be run after module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, incompatible_keys) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.</p><p>The given incompatible_keys can be modified inplace if needed.</p><p>Note that the checks performed when calling <code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.</p><h4 id="returns-18" tabindex="-1">Returns <a class="header-anchor" href="#returns-18" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-pre-hook-self-hook-source" tabindex="-1"><code>register_load_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-pre-hook-self-hook-source" aria-label="Permalink to &quot;`register_load_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214)&quot;">​</a></h4><p>Register a pre-hook to be run before module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # noqa: B950</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="arguments" tabindex="-1">Arguments <a class="header-anchor" href="#arguments" aria-label="Permalink to &quot;Arguments&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): Callable hook that will be invoked before loading the state dict.</li></ul><hr><h4 id="register-module-self-name-str-module-optional-forwardref-module-none-source" tabindex="-1"><code>register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-module-self-name-str-module-optional-forwardref-module-none-source" aria-label="Permalink to &quot;`register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695)&quot;">​</a></h4><p>Alias for <code>add_module</code>.</p><hr><h4 id="register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source" tabindex="-1"><code>register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source" aria-label="Permalink to &quot;`register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617)&quot;">​</a></h4><p>Add a parameter to the module.</p><p>The parameter can be accessed as an attribute using given name.</p><h4 id="args-19" tabindex="-1">Args <a class="header-anchor" href="#args-19" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the parameter. The parameter can be accessed from this module using the given name</li><li><strong>param</strong> (<code>Parameter or None</code>): parameter to be added to the module. If <code>None</code>, then operations that run on parameters, such as <code>cuda</code>, are ignored. If <code>None</code>, the parameter is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li></ul><hr><h4 id="register-state-dict-post-hook-self-hook-source" tabindex="-1"><code>register_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-post-hook-self-hook-source" aria-label="Permalink to &quot;`register_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017)&quot;">​</a></h4><p>Register a post-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can modify the <code>state_dict</code> inplace.</p><hr><h4 id="register-state-dict-pre-hook-self-hook-source" tabindex="-1"><code>register_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-pre-hook-self-hook-source" aria-label="Permalink to &quot;`register_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041)&quot;">​</a></h4><p>Register a pre-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, prefix, keep_vars) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p><hr><h4 id="requires-grad-self-requires-grad-bool-true-self-source" tabindex="-1"><code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#requires-grad-self-requires-grad-bool-true-self-source" aria-label="Permalink to &quot;`requires_grad_(self, requires_grad: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826)&quot;">​</a></h4><p>Change if autograd should record operations on parameters in this module.</p><p>This method sets the parameters&#39; <code>requires_grad</code> attributes in-place.</p><p>This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).</p><p>See <code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.</p><h4 id="args-20" tabindex="-1">Args <a class="header-anchor" href="#args-20" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>requires_grad</strong> (<code>bool</code>): whether autograd should record operations on parameters in this module. Default: <code>True</code>.</li></ul><h4 id="returns-19" tabindex="-1">Returns <a class="header-anchor" href="#returns-19" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="set-extra-state-self-state-any-none-source" tabindex="-1"><code>set_extra_state(self, state: Any) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-extra-state-self-state-any-none-source" aria-label="Permalink to &quot;`set_extra_state(self, state: Any) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938)&quot;">​</a></h4><p>Set extra state contained in the loaded <code>state_dict</code>.</p><p>This function is called from <code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding <code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.</p><h4 id="args-21" tabindex="-1">Args <a class="header-anchor" href="#args-21" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state</strong> (<code>dict</code>): Extra state from the <code>state_dict</code></li></ul><hr><h4 id="set-submodule-self-target-str-module-module-strict-bool-false-none-source" tabindex="-1"><code>set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-submodule-self-target-str-module-module-strict-bool-false-none-source" aria-label="Permalink to &quot;`set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764)&quot;">​</a></h4><p>Set the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>If <code>strict</code> is set to <code>False</code> (default), the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>strict</code> is set to <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule does not exist.</p></div><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(3, 3, 3) ) (linear): Linear(3, 3) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To override the <code>Conv2d</code> with a new submodule <code>Linear</code>, you could call <code>set_submodule(&quot;net_b.net_c.conv&quot;, nn.Linear(1, 1))</code> where <code>strict</code> could be <code>True</code> or <code>False</code></p><p>To add a new submodule <code>Conv2d</code> to the existing <code>net_b</code> module, you would call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1))</code>.</p><p>In the above if you set <code>strict=True</code> and call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1), strict=True)</code>, an AttributeError will be raised because <code>net_b</code> does not have a submodule named <code>conv</code>.</p><h4 id="args-22" tabindex="-1">Args <a class="header-anchor" href="#args-22" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li><li><strong>module</strong>: The module to set the submodule to.</li><li><strong>strict</strong>: If <code>False</code>, the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule doesn&#39;t already exist.</li></ul><h4 id="raises-3" tabindex="-1">Raises <a class="header-anchor" href="#raises-3" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>ValueError</strong>: If the <code>target</code> string is empty or if <code>module</code> is not an instance of <code>nn.Module</code>.</li><li><strong>AttributeError</strong>: If at any point along the path resulting from the <code>target</code> string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="share-memory-self-self-source" tabindex="-1"><code>share_memory(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#share-memory-self-self-source" aria-label="Permalink to &quot;`share_memory(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877)&quot;">​</a></h4><p>See <code>tensorplay.Tensor.share_memory_</code>.</p><hr><h4 id="state-dict-self-args-destination-none-prefix-keep-vars-false-source" tabindex="-1"><code>state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#state-dict-self-args-destination-none-prefix-keep-vars-false-source" aria-label="Permalink to &quot;`state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105)&quot;">​</a></h4><p>Return a dictionary containing references to the whole state of the module.</p><p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>The returned object is a shallow copy. It contains references to the module&#39;s parameters and buffers.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Currently <code>state_dict()</code> also accepts positional arguments for <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However, this is being deprecated and keyword arguments will be enforced in future releases.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Please avoid the use of argument <code>destination</code> as it is not designed for end-users.</p></div><h4 id="args-23" tabindex="-1">Args <a class="header-anchor" href="#args-23" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>destination</strong> (<code>dict, optional</code>): If provided, the state of module will be updated into the dict and the same object is returned. Otherwise, an <code>OrderedDict</code> will be created and returned.</li><li><strong>Default</strong>: <code>None</code>.</li><li><strong>prefix</strong> (<code>str, optional</code>): a prefix added to parameter and buffer names to compose the keys in state_dict. Default: <code>&#39;&#39;</code>.</li><li><strong>keep_vars</strong> (<code>bool, optional</code>): by default the <code>~tensorplay.Tensor</code> s returned in the state dict are detached from autograd. If it&#39;s set to <code>True</code>, detaching will not be performed.</li><li><strong>Default</strong>: <code>False</code>.</li></ul><h4 id="returns-20" tabindex="-1">Returns <a class="header-anchor" href="#returns-20" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>dict</strong>: a dictionary containing a whole state of the module</li></ul><h4 id="example-10" tabindex="-1">Example <a class="header-anchor" href="#example-10" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">module.state_dict().keys()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;weight&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><hr><h4 id="to-self-args-kwargs-source" tabindex="-1"><code>to(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-self-args-kwargs-source" aria-label="Permalink to &quot;`to(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151)&quot;">​</a></h4><p>Move and/or cast the parameters and buffers.</p><p>This can be called as</p><p>.. function:: to(device=None, dtype=None, non_blocking=False) :noindex:</p><p>.. function:: to(dtype, non_blocking=False) :noindex:</p><p>.. function:: to(tensor, non_blocking=False) :noindex:</p><p>.. function:: to(memory_format=tensorplay.channels_last) :noindex:</p><p>Its signature is similar to <code>tensorplay.Tensor.to</code>, but only accepts floating point or complex <code>dtype</code>\ s. In addition, this method will only cast the floating point or complex parameters and buffers to <code>dtype</code> (if given). The integral parameters and buffers will be moved <code>device</code>, if that is given, but with dtypes unchanged. When <code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.</p><p>See below for examples.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-24" tabindex="-1">Args <a class="header-anchor" href="#args-24" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): the desired device of the parameters and buffers in this module</li><li><strong>dtype</strong> (<code>tensorplay.dtype</code>): the desired floating point or complex dtype of the parameters and buffers in this module</li><li><strong>tensor</strong> (<code>tensorplay.Tensor</code>): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module</li><li><strong>memory_format</strong> (<code>tensorplay.memory_format</code>): the desired memory format for 4D parameters and buffers in this module (keyword only argument)</li></ul><h4 id="returns-21" tabindex="-1">Returns <a class="header-anchor" href="#returns-21" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="examples" tabindex="-1">Examples <a class="header-anchor" href="#examples" aria-label="Permalink to &quot;Examples&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +IGNORE_WANT(&quot;non-deterministic&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(tensorplay.double)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float64)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +REQUIRES(env:TENSORPLAY_DOCTEST_CUDA1)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">gpu1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cuda:1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(gpu1, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.half, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">non_blocking</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;cuda:1&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cpu </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cpu&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(cpu)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).to(tensorplay.cdouble)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3741</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,  </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2382</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5593</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.4443</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear(tensorplay.ones(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.cdouble))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br></div></div><hr><h4 id="to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source" tabindex="-1"><code>to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source" aria-label="Permalink to &quot;`to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119)&quot;">​</a></h4><p>Move the parameters and buffers to the specified device without copying storage.</p><h4 id="args-25" tabindex="-1">Args <a class="header-anchor" href="#args-25" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): The desired device of the parameters and buffers in this module.</li><li><strong>recurse</strong> (<code>bool</code>): Whether parameters and buffers of submodules should be recursively moved to the specified device.</li></ul><h4 id="returns-22" tabindex="-1">Returns <a class="header-anchor" href="#returns-22" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="train-self-mode-bool-true-self-source" tabindex="-1"><code>train(self, mode: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#train-self-mode-bool-true-self-source" aria-label="Permalink to &quot;`train(self, mode: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786)&quot;">​</a></h4><p>Set the module in training mode.</p><p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e., whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, etc.</p><h4 id="args-26" tabindex="-1">Args <a class="header-anchor" href="#args-26" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>mode</strong> (<code>bool</code>): whether to set training mode (<code>True</code>) or evaluation mode (<code>False</code>). Default: <code>True</code>.</li></ul><h4 id="returns-23" tabindex="-1">Returns <a class="header-anchor" href="#returns-23" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="type-self-dst-type-union-tensorplay-dtype-str-self-source" tabindex="-1"><code>type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#type-self-dst-type-union-tensorplay-dtype-str-self-source" aria-label="Permalink to &quot;`type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061)&quot;">​</a></h4><p>Casts all parameters and buffers to <code>dst_type</code>.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-27" tabindex="-1">Args <a class="header-anchor" href="#args-27" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>dst_type</strong> (<code>type or string</code>): the desired type</li></ul><h4 id="returns-24" tabindex="-1">Returns <a class="header-anchor" href="#returns-24" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="update-self-modules-mapping-str-module-none-source" tabindex="-1"><code>update(self, modules: &#39;Mapping[str, Module]&#39;) -&gt; &#39;None&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L573" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#update-self-modules-mapping-str-module-none-source" aria-label="Permalink to &quot;`update(self, modules: &#39;Mapping[str, Module]&#39;) -&gt; &#39;None&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L573)&quot;">​</a></h4><p>Update the <code>~tensorplay.nn.ModuleDict</code> with key-value pairs from a mapping, overwriting existing keys.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>If <code>modules</code> is an <code>OrderedDict</code>, a <code>~tensorplay.nn.ModuleDict</code>, or an iterable of key-value pairs, the order of new elements in it is preserved.</p></div><h4 id="args-28" tabindex="-1">Args <a class="header-anchor" href="#args-28" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>modules</strong> (<code>iterable</code>): a mapping (dictionary) from string to <code>~tensorplay.nn.Module</code>, or an iterable of key-value pairs of type (string, <code>~tensorplay.nn.Module</code>)</li></ul><hr><h4 id="values-self-container-abcs-valuesview-module-source" tabindex="-1"><code>values(self) -&gt; &#39;container_abcs.ValuesView[Module]&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L569" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#values-self-container-abcs-valuesview-module-source" aria-label="Permalink to &quot;`values(self) -&gt; &#39;container_abcs.ValuesView[Module]&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L569)&quot;">​</a></h4><p>Return an iterable of the ModuleDict values.</p><hr><h4 id="zero-grad-self-set-to-none-bool-true-none-source" tabindex="-1"><code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#zero-grad-self-set-to-none-bool-true-none-source" aria-label="Permalink to &quot;`zero_grad(self, set_to_none: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849)&quot;">​</a></h4><p>Reset gradients of all model parameters.</p><p>See similar function under <code>tensorplay.optim.Optimizer</code> for more context.</p><h4 id="args-29" tabindex="-1">Args <a class="header-anchor" href="#args-29" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>set_to_none</strong> (<code>bool</code>): instead of setting to zero, set the grads to None. See <code>tensorplay.optim.Optimizer.zero_grad</code> for details.</li></ul><hr></details><h3 id="class-modulelist-source" tabindex="-1"><code>class ModuleList</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L309" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-modulelist-source" aria-label="Permalink to &quot;`class ModuleList` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L309)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">ModuleList(modules: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Optional[Iterable[Module]]&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;None&#39;</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>Module</code></p><p>Holds submodules in a list.</p><p><code>~tensorplay.nn.ModuleList</code> can be indexed like a regular Python list, but modules it contains are properly registered, and will be visible by all <code>~tensorplay.nn.Module</code> methods.</p><h4 id="args-30" tabindex="-1">Args <a class="header-anchor" href="#args-30" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>modules</strong> (<code>iterable, optional</code>): an iterable of modules to add</li></ul><h4 id="example-11" tabindex="-1">Example <a class="header-anchor" href="#example-11" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">class</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> MyModule</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">nn</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">Module</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> __init__</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self) -&gt; </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    super</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">().</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">__init__</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.ModuleList([nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> i </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> range</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)])</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> forward</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, x):</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # ModuleList can act as an iterable, or be indexed using ints</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> i, l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.linear):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.linear[i </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">//</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">](x) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> l(x)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><details><summary>Methods</summary><h4 id="init-self-modules-optional-iterable-module-none-none-source" tabindex="-1"><code>__init__(self, modules: &#39;Optional[Iterable[Module]]&#39; = None) -&gt; &#39;None&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L335" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-modules-optional-iterable-module-none-none-source" aria-label="Permalink to &quot;`__init__(self, modules: &#39;Optional[Iterable[Module]]&#39; = None) -&gt; &#39;None&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L335)&quot;">​</a></h4><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p><hr><h4 id="add-module-self-name-str-module-optional-forwardref-module-none-source-1" tabindex="-1"><code>add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#add-module-self-name-str-module-optional-forwardref-module-none-source-1" aria-label="Permalink to &quot;`add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667)&quot;">​</a></h4><p>Add a child module to the current module.</p><p>The module can be accessed as an attribute using the given name.</p><h4 id="args-31" tabindex="-1">Args <a class="header-anchor" href="#args-31" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the child module. The child module can be accessed from this module using the given name</li><li><strong>module</strong> (<code>Module</code>): child module to be added to the module.</li></ul><hr><h4 id="append-self-module-module-self-source" tabindex="-1"><code>append(self, module: &#39;Module&#39;) -&gt; &#39;Self&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L442" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#append-self-module-module-self-source" aria-label="Permalink to &quot;`append(self, module: &#39;Module&#39;) -&gt; &#39;Self&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L442)&quot;">​</a></h4><p>Append a given module to the end of the list.</p><h4 id="args-32" tabindex="-1">Args <a class="header-anchor" href="#args-32" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>module</strong> (<code>nn.Module</code>): module to append</li></ul><hr><h4 id="apply-self-fn-callable-forwardref-module-nonetype-self-source-1" tabindex="-1"><code>apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#apply-self-fn-callable-forwardref-module-nonetype-self-source-1" aria-label="Permalink to &quot;`apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990)&quot;">​</a></h4><p>Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self.</p><p>Typical use includes initializing the parameters of a model (see also <code>nn-init-doc</code>).</p><h4 id="args-33" tabindex="-1">Args <a class="header-anchor" href="#args-33" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>fn</strong> (``Module<code> -&gt; None</code>): function to be applied to each submodule</li></ul><h4 id="returns-25" tabindex="-1">Returns <a class="header-anchor" href="#returns-25" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="example-12" tabindex="-1">Example <a class="header-anchor" href="#example-12" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@tensorplay.no_grad</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> init_weights</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        m.weight.fill_(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m.weight)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net.apply(init_weights)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><hr><h4 id="bfloat16-self-self-source-1" tabindex="-1"><code>bfloat16(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#bfloat16-self-self-source-1" aria-label="Permalink to &quot;`bfloat16(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-26" tabindex="-1">Returns <a class="header-anchor" href="#returns-26" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source-1" tabindex="-1"><code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source-1" aria-label="Permalink to &quot;`buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627)&quot;">​</a></h4><p>Return an iterator over module buffers.</p><h4 id="args-34" tabindex="-1">Args <a class="header-anchor" href="#args-34" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.</li></ul><h4 id="yields-8" tabindex="-1">Yields <a class="header-anchor" href="#yields-8" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>tensorplay.Tensor: module buffer
</code></pre><h4 id="example-13" tabindex="-1">Example <a class="header-anchor" href="#example-13" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.buffers():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf), buf.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="children-self-collections-abc-iterator-module-source-1" tabindex="-1"><code>children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#children-self-collections-abc-iterator-module-source-1" aria-label="Permalink to &quot;`children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681)&quot;">​</a></h4><p>Return an iterator over immediate children modules.</p><h4 id="yields-9" tabindex="-1">Yields <a class="header-anchor" href="#yields-9" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a child module</li></ul><hr><h4 id="compile-self-args-kwargs-source-1" tabindex="-1"><code>compile(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#compile-self-args-kwargs-source-1" aria-label="Permalink to &quot;`compile(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944)&quot;">​</a></h4><p>Compile this Module&#39;s forward using <code>tensorplay.compile</code>.</p><p>This Module&#39;s <code>__call__</code> method is compiled and all arguments are passed as-is to <code>tensorplay.compile</code>.</p><p>See <code>tensorplay.compile</code> for details on the arguments for this function.</p><hr><h4 id="cpu-self-self-source-1" tabindex="-1"><code>cpu(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cpu-self-self-source-1" aria-label="Permalink to &quot;`cpu(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050)&quot;">​</a></h4><p>Move all model parameters and buffers to the CPU.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-27" tabindex="-1">Returns <a class="header-anchor" href="#returns-27" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source-1" tabindex="-1"><code>cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source-1" aria-label="Permalink to &quot;`cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031)&quot;">​</a></h4><p>Move all model parameters and buffers to the GPU.</p><p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on GPU while being optimized.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-35" tabindex="-1">Args <a class="header-anchor" href="#args-35" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>int, optional</code>): if specified, all parameters will be copied to that device</li></ul><h4 id="returns-28" tabindex="-1">Returns <a class="header-anchor" href="#returns-28" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="double-self-self-source-1" tabindex="-1"><code>double(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#double-self-self-source-1" aria-label="Permalink to &quot;`double(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-29" tabindex="-1">Returns <a class="header-anchor" href="#returns-29" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="eval-self-self-source-1" tabindex="-1"><code>eval(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#eval-self-self-source-1" aria-label="Permalink to &quot;`eval(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808)&quot;">​</a></h4><p>Set the module in evaluation mode.</p><p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e. whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, etc.</p><p>This is equivalent with <code>self.train(False) &lt;tensorplay.nn.Module.train&gt;</code>.</p><p>See <code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.</p><h4 id="returns-30" tabindex="-1">Returns <a class="header-anchor" href="#returns-30" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="extend-self-modules-iterable-module-self-source" tabindex="-1"><code>extend(self, modules: &#39;Iterable[Module]&#39;) -&gt; &#39;Self&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L456" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#extend-self-modules-iterable-module-self-source" aria-label="Permalink to &quot;`extend(self, modules: &#39;Iterable[Module]&#39;) -&gt; &#39;Self&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L456)&quot;">​</a></h4><p>Append modules from a Python iterable to the end of the list.</p><h4 id="args-36" tabindex="-1">Args <a class="header-anchor" href="#args-36" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>modules</strong> (<code>iterable</code>): iterable of modules to append</li></ul><hr><h4 id="extra-repr-self-str-source-1" tabindex="-1"><code>extra_repr(self) -&gt; str</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2884" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#extra-repr-self-str-source-1" aria-label="Permalink to &quot;`extra_repr(self) -&gt; str` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2884)&quot;">​</a></h4><p>Return the extra representation of the module.</p><p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p><hr><h4 id="float-self-self-source-1" tabindex="-1"><code>float(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#float-self-self-source-1" aria-label="Permalink to &quot;`float(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-31" tabindex="-1">Returns <a class="header-anchor" href="#returns-31" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="forward-self-input-any-none-source-1" tabindex="-1"><code>forward(self, *input: Any) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L388" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#forward-self-input-any-none-source-1" aria-label="Permalink to &quot;`forward(self, *input: Any) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L388)&quot;">​</a></h4><p>Define the computation performed at every call.</p><p>Should be overridden by all subclasses.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code> instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.</p></div><hr><h4 id="get-buffer-self-target-str-tensor-source-1" tabindex="-1"><code>get_buffer(self, target: str) -&gt; &#39;Tensor&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-buffer-self-target-str-tensor-source-1" aria-label="Permalink to &quot;`get_buffer(self, target: str) -&gt; &#39;Tensor&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881)&quot;">​</a></h4><p>Return the buffer given by <code>target</code> if it exists, otherwise throw an error.</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-37" tabindex="-1">Args <a class="header-anchor" href="#args-37" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the buffer to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-32" tabindex="-1">Returns <a class="header-anchor" href="#returns-32" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.Tensor: The buffer referenced by ``target``
</code></pre><h4 id="raises-4" tabindex="-1">Raises <a class="header-anchor" href="#raises-4" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not a buffer</li></ul><hr><h4 id="get-extra-state-self-any-source-1" tabindex="-1"><code>get_extra_state(self) -&gt; Any</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-extra-state-self-any-source-1" aria-label="Permalink to &quot;`get_extra_state(self) -&gt; Any` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917)&quot;">​</a></h4><p>Return any extra state to include in the module&#39;s state_dict.</p><p>Implement this and a corresponding <code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module&#39;s <code>state_dict()</code>.</p><p>Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.</p><h4 id="returns-33" tabindex="-1">Returns <a class="header-anchor" href="#returns-33" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>object</strong>: Any extra state to store in the module&#39;s state_dict</li></ul><hr><h4 id="get-parameter-self-target-str-parameter-source-1" tabindex="-1"><code>get_parameter(self, target: str) -&gt; &#39;Parameter&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-parameter-self-target-str-parameter-source-1" aria-label="Permalink to &quot;`get_parameter(self, target: str) -&gt; &#39;Parameter&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845)&quot;">​</a></h4><p>Return the parameter given by <code>target</code> if it exists, otherwise throw an error.</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-38" tabindex="-1">Args <a class="header-anchor" href="#args-38" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the Parameter to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-34" tabindex="-1">Returns <a class="header-anchor" href="#returns-34" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Parameter: The Parameter referenced by ``target``
</code></pre><h4 id="raises-5" tabindex="-1">Raises <a class="header-anchor" href="#raises-5" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not an <code>nn.Parameter</code></li></ul><hr><h4 id="get-submodule-self-target-str-module-source-1" tabindex="-1"><code>get_submodule(self, target: str) -&gt; &#39;Module&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-submodule-self-target-str-module-source-1" aria-label="Permalink to &quot;`get_submodule(self, target: str) -&gt; &#39;Module&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699)&quot;">​</a></h4><p>Return the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> which has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(&quot;net_b.linear&quot;)</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(&quot;net_b.net_c.conv&quot;)</code>.</p><p>The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.</p><h4 id="args-39" tabindex="-1">Args <a class="header-anchor" href="#args-39" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li></ul><h4 id="returns-35" tabindex="-1">Returns <a class="header-anchor" href="#returns-35" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Module: The submodule referenced by ``target``
</code></pre><h4 id="raises-6" tabindex="-1">Raises <a class="header-anchor" href="#raises-6" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If at any point along the path resulting from the target string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="half-self-self-source-1" tabindex="-1"><code>half(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#half-self-self-source-1" aria-label="Permalink to &quot;`half(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-36" tabindex="-1">Returns <a class="header-anchor" href="#returns-36" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="insert-self-index-int-module-module-none-source" tabindex="-1"><code>insert(self, index: &#39;int&#39;, module: &#39;Module&#39;) -&gt; &#39;None&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L431" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#insert-self-index-int-module-module-none-source" aria-label="Permalink to &quot;`insert(self, index: &#39;int&#39;, module: &#39;Module&#39;) -&gt; &#39;None&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L431)&quot;">​</a></h4><p>Insert a given module before a given index in the list.</p><h4 id="args-40" tabindex="-1">Args <a class="header-anchor" href="#args-40" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>index</strong> (<code>int</code>): index to insert.</li><li><strong>module</strong> (<code>nn.Module</code>): module to insert</li></ul><hr><h4 id="load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source-1" tabindex="-1"><code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source-1" aria-label="Permalink to &quot;`load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436)&quot;">​</a></h4><p>Copy parameters and buffers from <code>state_dict</code> into this module and its descendants.</p><p>If <code>strict</code> is <code>True</code>, then the keys of <code>state_dict</code> must exactly match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function.</p><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>If <code>assign</code> is <code>True</code> the optimizer must be created after the call to <code>load_state_dict</code> unless <code>~tensorplay.__future__.get_swap_module_params_on_conversion</code> is <code>True</code>.</p></div><h4 id="args-41" tabindex="-1">Args <a class="header-anchor" href="#args-41" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state_dict</strong> (<code>dict</code>): a dict containing parameters and persistent buffers.</li><li><strong>strict</strong> (<code>bool, optional</code>): whether to strictly enforce that the keys in <code>state_dict</code> match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function. Default: <code>True</code></li><li><strong>assign</strong> (<code>bool, optional</code>): When set to <code>False</code>, the properties of the tensors in the current module are preserved whereas setting it to <code>True</code> preserves properties of the Tensors in the state dict. The only exception is the <code>requires_grad</code> field of <code>~tensorplay.nn.Parameter</code> for which the value from the module is preserved. Default: <code>False</code></li></ul><h4 id="returns-37" tabindex="-1">Returns <a class="header-anchor" href="#returns-37" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:
    * ``missing_keys`` is a list of str containing any keys that are expected
        by this module but missing from the provided ``state_dict``.
    * ``unexpected_keys`` is a list of str containing the keys that are not
        expected by this module but present in the provided ``state_dict``.
</code></pre><h4 id="note-3" tabindex="-1">Note <a class="header-anchor" href="#note-3" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>If a parameter or buffer is registered as ``None`` and its corresponding key
exists in `state_dict`, `load_state_dict` will raise a
``RuntimeError``.
</code></pre><hr><h4 id="modules-self-collections-abc-iterator-module-source-1" tabindex="-1"><code>modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#modules-self-collections-abc-iterator-module-source-1" aria-label="Permalink to &quot;`modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710)&quot;">​</a></h4><p>Return an iterator over all modules in the network.</p><h4 id="yields-10" tabindex="-1">Yields <a class="header-anchor" href="#yields-10" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a module in the network</li></ul><h4 id="note-4" tabindex="-1">Note <a class="header-anchor" href="#note-4" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-14" tabindex="-1">Example <a class="header-anchor" href="#example-14" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source-1" tabindex="-1"><code>named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source-1" aria-label="Permalink to &quot;`named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650)&quot;">​</a></h4><p>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p><h4 id="args-42" tabindex="-1">Args <a class="header-anchor" href="#args-42" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all buffer names.</li><li><strong>recurse</strong> (<code>bool, optional</code>): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Defaults to True.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated buffers in the result. Defaults to True.</li></ul><h4 id="yields-11" tabindex="-1">Yields <a class="header-anchor" href="#yields-11" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, tensorplay.Tensor): Tuple containing the name and buffer
</code></pre><h4 id="example-15" tabindex="-1">Example <a class="header-anchor" href="#example-15" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_buffers():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_var&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-children-self-collections-abc-iterator-tuple-str-module-source-1" tabindex="-1"><code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-children-self-collections-abc-iterator-tuple-str-module-source-1" aria-label="Permalink to &quot;`named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690)&quot;">​</a></h4><p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p><h4 id="yields-12" tabindex="-1">Yields <a class="header-anchor" href="#yields-12" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple containing a name and child module
</code></pre><h4 id="example-16" tabindex="-1">Example <a class="header-anchor" href="#example-16" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, module </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.named_children():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv4&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv5&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(module)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source-1" tabindex="-1"><code>named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source-1" aria-label="Permalink to &quot;`named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737)&quot;">​</a></h4><p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p><h4 id="args-43" tabindex="-1">Args <a class="header-anchor" href="#args-43" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>memo</strong>: a memo to store the set of modules already added to the result</li><li><strong>prefix</strong>: a prefix that will be added to the name of the module</li><li><strong>remove_duplicate</strong>: whether to remove the duplicated module instances in the result or not</li></ul><h4 id="yields-13" tabindex="-1">Yields <a class="header-anchor" href="#yields-13" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple of name and module
</code></pre><h4 id="note-5" tabindex="-1">Note <a class="header-anchor" href="#note-5" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-17" tabindex="-1">Example <a class="header-anchor" href="#example-17" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.named_modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;0&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source-1" tabindex="-1"><code>named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source-1" aria-label="Permalink to &quot;`named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595)&quot;">​</a></h4><p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p><h4 id="args-44" tabindex="-1">Args <a class="header-anchor" href="#args-44" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all parameter names.</li><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated parameters in the result. Defaults to True.</li></ul><h4 id="yields-14" tabindex="-1">Yields <a class="header-anchor" href="#yields-14" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Parameter): Tuple containing the name and parameter
</code></pre><h4 id="example-18" tabindex="-1">Example <a class="header-anchor" href="#example-18" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_parameters():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source-1" tabindex="-1"><code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source-1" aria-label="Permalink to &quot;`parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570)&quot;">​</a></h4><p>Return an iterator over module parameters.</p><p>This is typically passed to an optimizer.</p><h4 id="args-45" tabindex="-1">Args <a class="header-anchor" href="#args-45" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</li></ul><h4 id="yields-15" tabindex="-1">Yields <a class="header-anchor" href="#yields-15" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Parameter</strong>: module parameter</li></ul><h4 id="example-19" tabindex="-1">Example <a class="header-anchor" href="#example-19" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.parameters():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param), param.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="pop-self-key-union-int-slice-module-source" tabindex="-1"><code>pop(self, key: &#39;Union[int, slice]&#39;) -&gt; &#39;Module&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L451" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#pop-self-key-union-int-slice-module-source" aria-label="Permalink to &quot;`pop(self, key: &#39;Union[int, slice]&#39;) -&gt; &#39;Module&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L451)&quot;">​</a></h4><hr><h4 id="register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source-1" tabindex="-1"><code>register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source-1" aria-label="Permalink to &quot;`register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>This function is deprecated in favor of <code>~tensorplay.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.</p><h4 id="returns-38" tabindex="-1">Returns <a class="header-anchor" href="#returns-38" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source-1" tabindex="-1"><code>register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source-1" aria-label="Permalink to &quot;`register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553)&quot;">​</a></h4><p>Add a buffer to the module.</p><p>This is typically used to register a buffer that should not be considered a model parameter. For example, BatchNorm&#39;s <code>running_mean</code> is not a parameter, but is part of the module&#39;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting <code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&#39;s <code>state_dict</code>.</p><p>Buffers can be accessed as attributes using given names.</p><h4 id="args-46" tabindex="-1">Args <a class="header-anchor" href="#args-46" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the buffer. The buffer can be accessed from this module using the given name</li><li><strong>tensor</strong> (<code>Tensor or None</code>): buffer to be registered. If <code>None</code>, then operations that run on buffers, such as <code>cuda</code>, are ignored. If <code>None</code>, the buffer is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li><li><strong>persistent</strong> (<code>bool</code>): whether the buffer is part of this module&#39;s <code>state_dict</code>.</li></ul><h4 id="example-20" tabindex="-1">Example <a class="header-anchor" href="#example-20" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.register_buffer(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_mean&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, tensorplay.zeros(num_features))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><hr><h4 id="register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source-1" tabindex="-1"><code>register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source-1" aria-label="Permalink to &quot;`register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592)&quot;">​</a></h4><p>Register a forward hook on the module.</p><p>The hook will be called every time after <code>forward</code> has computed an output.</p><p>If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after <code>forward</code> is called. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-47" tabindex="-1">Args <a class="header-anchor" href="#args-47" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If <code>True</code>, the provided <code>hook</code> will be fired before all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward</code> hooks registered with <code>register_module_forward_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If <code>True</code>, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>always_call</strong> (<code>bool</code>): If <code>True</code> the <code>hook</code> will be run regardless of whether an exception is raised while calling the Module.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-39" tabindex="-1">Returns <a class="header-anchor" href="#returns-39" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source-1" tabindex="-1"><code>register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source-1" aria-label="Permalink to &quot;`register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526)&quot;">​</a></h4><p>Register a forward pre-hook on the module.</p><p>The hook will be called every time before <code>forward</code> is invoked.</p><p>If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> of modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> and</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> kwargs</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-48" tabindex="-1">Args <a class="header-anchor" href="#args-48" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward_pre</code> hooks registered with <code>register_module_forward_pre_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If true, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-40" tabindex="-1">Returns <a class="header-anchor" href="#returns-40" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-1" tabindex="-1"><code>register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-1" aria-label="Permalink to &quot;`register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>The hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:</p><pre><code>1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.
2. If none of the module inputs require gradients, the hook will fire when the gradients are computed
   with respect to module outputs.
3. If none of the module outputs require gradients, then the hooks will not fire.
</code></pre><p>The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_input, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Tensor) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_input</code> and <code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of <code>grad_input</code> in subsequent computations. <code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in <code>grad_input</code> and <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> outputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-49" tabindex="-1">Args <a class="header-anchor" href="#args-49" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward</code> hooks registered with <code>register_module_full_backward_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-41" tabindex="-1">Returns <a class="header-anchor" href="#returns-41" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-1" tabindex="-1"><code>register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-1" aria-label="Permalink to &quot;`register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287)&quot;">​</a></h4><p>Register a backward pre-hook on the module.</p><p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tuple[Tensor] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of <code>grad_output</code> in subsequent computations. Entries in <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-50" tabindex="-1">Args <a class="header-anchor" href="#args-50" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward_pre</code> hooks registered with <code>register_module_full_backward_pre_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-42" tabindex="-1">Returns <a class="header-anchor" href="#returns-42" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-post-hook-self-hook-source-1" tabindex="-1"><code>register_load_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-post-hook-self-hook-source-1" aria-label="Permalink to &quot;`register_load_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226)&quot;">​</a></h4><p>Register a post-hook to be run after module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, incompatible_keys) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.</p><p>The given incompatible_keys can be modified inplace if needed.</p><p>Note that the checks performed when calling <code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.</p><h4 id="returns-43" tabindex="-1">Returns <a class="header-anchor" href="#returns-43" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-pre-hook-self-hook-source-1" tabindex="-1"><code>register_load_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-pre-hook-self-hook-source-1" aria-label="Permalink to &quot;`register_load_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214)&quot;">​</a></h4><p>Register a pre-hook to be run before module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # noqa: B950</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="arguments-1" tabindex="-1">Arguments <a class="header-anchor" href="#arguments-1" aria-label="Permalink to &quot;Arguments&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): Callable hook that will be invoked before loading the state dict.</li></ul><hr><h4 id="register-module-self-name-str-module-optional-forwardref-module-none-source-1" tabindex="-1"><code>register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-module-self-name-str-module-optional-forwardref-module-none-source-1" aria-label="Permalink to &quot;`register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695)&quot;">​</a></h4><p>Alias for <code>add_module</code>.</p><hr><h4 id="register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source-1" tabindex="-1"><code>register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source-1" aria-label="Permalink to &quot;`register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617)&quot;">​</a></h4><p>Add a parameter to the module.</p><p>The parameter can be accessed as an attribute using given name.</p><h4 id="args-51" tabindex="-1">Args <a class="header-anchor" href="#args-51" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the parameter. The parameter can be accessed from this module using the given name</li><li><strong>param</strong> (<code>Parameter or None</code>): parameter to be added to the module. If <code>None</code>, then operations that run on parameters, such as <code>cuda</code>, are ignored. If <code>None</code>, the parameter is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li></ul><hr><h4 id="register-state-dict-post-hook-self-hook-source-1" tabindex="-1"><code>register_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-post-hook-self-hook-source-1" aria-label="Permalink to &quot;`register_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017)&quot;">​</a></h4><p>Register a post-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can modify the <code>state_dict</code> inplace.</p><hr><h4 id="register-state-dict-pre-hook-self-hook-source-1" tabindex="-1"><code>register_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-pre-hook-self-hook-source-1" aria-label="Permalink to &quot;`register_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041)&quot;">​</a></h4><p>Register a pre-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, prefix, keep_vars) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p><hr><h4 id="requires-grad-self-requires-grad-bool-true-self-source-1" tabindex="-1"><code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#requires-grad-self-requires-grad-bool-true-self-source-1" aria-label="Permalink to &quot;`requires_grad_(self, requires_grad: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826)&quot;">​</a></h4><p>Change if autograd should record operations on parameters in this module.</p><p>This method sets the parameters&#39; <code>requires_grad</code> attributes in-place.</p><p>This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).</p><p>See <code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.</p><h4 id="args-52" tabindex="-1">Args <a class="header-anchor" href="#args-52" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>requires_grad</strong> (<code>bool</code>): whether autograd should record operations on parameters in this module. Default: <code>True</code>.</li></ul><h4 id="returns-44" tabindex="-1">Returns <a class="header-anchor" href="#returns-44" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="set-extra-state-self-state-any-none-source-1" tabindex="-1"><code>set_extra_state(self, state: Any) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-extra-state-self-state-any-none-source-1" aria-label="Permalink to &quot;`set_extra_state(self, state: Any) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938)&quot;">​</a></h4><p>Set extra state contained in the loaded <code>state_dict</code>.</p><p>This function is called from <code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding <code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.</p><h4 id="args-53" tabindex="-1">Args <a class="header-anchor" href="#args-53" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state</strong> (<code>dict</code>): Extra state from the <code>state_dict</code></li></ul><hr><h4 id="set-submodule-self-target-str-module-module-strict-bool-false-none-source-1" tabindex="-1"><code>set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-submodule-self-target-str-module-module-strict-bool-false-none-source-1" aria-label="Permalink to &quot;`set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764)&quot;">​</a></h4><p>Set the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>If <code>strict</code> is set to <code>False</code> (default), the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>strict</code> is set to <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule does not exist.</p></div><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(3, 3, 3) ) (linear): Linear(3, 3) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To override the <code>Conv2d</code> with a new submodule <code>Linear</code>, you could call <code>set_submodule(&quot;net_b.net_c.conv&quot;, nn.Linear(1, 1))</code> where <code>strict</code> could be <code>True</code> or <code>False</code></p><p>To add a new submodule <code>Conv2d</code> to the existing <code>net_b</code> module, you would call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1))</code>.</p><p>In the above if you set <code>strict=True</code> and call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1), strict=True)</code>, an AttributeError will be raised because <code>net_b</code> does not have a submodule named <code>conv</code>.</p><h4 id="args-54" tabindex="-1">Args <a class="header-anchor" href="#args-54" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li><li><strong>module</strong>: The module to set the submodule to.</li><li><strong>strict</strong>: If <code>False</code>, the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule doesn&#39;t already exist.</li></ul><h4 id="raises-7" tabindex="-1">Raises <a class="header-anchor" href="#raises-7" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>ValueError</strong>: If the <code>target</code> string is empty or if <code>module</code> is not an instance of <code>nn.Module</code>.</li><li><strong>AttributeError</strong>: If at any point along the path resulting from the <code>target</code> string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="share-memory-self-self-source-1" tabindex="-1"><code>share_memory(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#share-memory-self-self-source-1" aria-label="Permalink to &quot;`share_memory(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877)&quot;">​</a></h4><p>See <code>tensorplay.Tensor.share_memory_</code>.</p><hr><h4 id="state-dict-self-args-destination-none-prefix-keep-vars-false-source-1" tabindex="-1"><code>state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#state-dict-self-args-destination-none-prefix-keep-vars-false-source-1" aria-label="Permalink to &quot;`state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105)&quot;">​</a></h4><p>Return a dictionary containing references to the whole state of the module.</p><p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>The returned object is a shallow copy. It contains references to the module&#39;s parameters and buffers.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Currently <code>state_dict()</code> also accepts positional arguments for <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However, this is being deprecated and keyword arguments will be enforced in future releases.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Please avoid the use of argument <code>destination</code> as it is not designed for end-users.</p></div><h4 id="args-55" tabindex="-1">Args <a class="header-anchor" href="#args-55" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>destination</strong> (<code>dict, optional</code>): If provided, the state of module will be updated into the dict and the same object is returned. Otherwise, an <code>OrderedDict</code> will be created and returned.</li><li><strong>Default</strong>: <code>None</code>.</li><li><strong>prefix</strong> (<code>str, optional</code>): a prefix added to parameter and buffer names to compose the keys in state_dict. Default: <code>&#39;&#39;</code>.</li><li><strong>keep_vars</strong> (<code>bool, optional</code>): by default the <code>~tensorplay.Tensor</code> s returned in the state dict are detached from autograd. If it&#39;s set to <code>True</code>, detaching will not be performed.</li><li><strong>Default</strong>: <code>False</code>.</li></ul><h4 id="returns-45" tabindex="-1">Returns <a class="header-anchor" href="#returns-45" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>dict</strong>: a dictionary containing a whole state of the module</li></ul><h4 id="example-21" tabindex="-1">Example <a class="header-anchor" href="#example-21" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">module.state_dict().keys()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;weight&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><hr><h4 id="to-self-args-kwargs-source-1" tabindex="-1"><code>to(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-self-args-kwargs-source-1" aria-label="Permalink to &quot;`to(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151)&quot;">​</a></h4><p>Move and/or cast the parameters and buffers.</p><p>This can be called as</p><p>.. function:: to(device=None, dtype=None, non_blocking=False) :noindex:</p><p>.. function:: to(dtype, non_blocking=False) :noindex:</p><p>.. function:: to(tensor, non_blocking=False) :noindex:</p><p>.. function:: to(memory_format=tensorplay.channels_last) :noindex:</p><p>Its signature is similar to <code>tensorplay.Tensor.to</code>, but only accepts floating point or complex <code>dtype</code>\ s. In addition, this method will only cast the floating point or complex parameters and buffers to <code>dtype</code> (if given). The integral parameters and buffers will be moved <code>device</code>, if that is given, but with dtypes unchanged. When <code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.</p><p>See below for examples.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-56" tabindex="-1">Args <a class="header-anchor" href="#args-56" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): the desired device of the parameters and buffers in this module</li><li><strong>dtype</strong> (<code>tensorplay.dtype</code>): the desired floating point or complex dtype of the parameters and buffers in this module</li><li><strong>tensor</strong> (<code>tensorplay.Tensor</code>): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module</li><li><strong>memory_format</strong> (<code>tensorplay.memory_format</code>): the desired memory format for 4D parameters and buffers in this module (keyword only argument)</li></ul><h4 id="returns-46" tabindex="-1">Returns <a class="header-anchor" href="#returns-46" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="examples-1" tabindex="-1">Examples <a class="header-anchor" href="#examples-1" aria-label="Permalink to &quot;Examples&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +IGNORE_WANT(&quot;non-deterministic&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(tensorplay.double)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float64)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +REQUIRES(env:TENSORPLAY_DOCTEST_CUDA1)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">gpu1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cuda:1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(gpu1, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.half, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">non_blocking</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;cuda:1&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cpu </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cpu&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(cpu)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).to(tensorplay.cdouble)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3741</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,  </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2382</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5593</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.4443</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear(tensorplay.ones(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.cdouble))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br></div></div><hr><h4 id="to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source-1" tabindex="-1"><code>to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source-1" aria-label="Permalink to &quot;`to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119)&quot;">​</a></h4><p>Move the parameters and buffers to the specified device without copying storage.</p><h4 id="args-57" tabindex="-1">Args <a class="header-anchor" href="#args-57" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): The desired device of the parameters and buffers in this module.</li><li><strong>recurse</strong> (<code>bool</code>): Whether parameters and buffers of submodules should be recursively moved to the specified device.</li></ul><h4 id="returns-47" tabindex="-1">Returns <a class="header-anchor" href="#returns-47" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="train-self-mode-bool-true-self-source-1" tabindex="-1"><code>train(self, mode: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#train-self-mode-bool-true-self-source-1" aria-label="Permalink to &quot;`train(self, mode: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786)&quot;">​</a></h4><p>Set the module in training mode.</p><p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e., whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, etc.</p><h4 id="args-58" tabindex="-1">Args <a class="header-anchor" href="#args-58" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>mode</strong> (<code>bool</code>): whether to set training mode (<code>True</code>) or evaluation mode (<code>False</code>). Default: <code>True</code>.</li></ul><h4 id="returns-48" tabindex="-1">Returns <a class="header-anchor" href="#returns-48" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="type-self-dst-type-union-tensorplay-dtype-str-self-source-1" tabindex="-1"><code>type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#type-self-dst-type-union-tensorplay-dtype-str-self-source-1" aria-label="Permalink to &quot;`type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061)&quot;">​</a></h4><p>Casts all parameters and buffers to <code>dst_type</code>.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-59" tabindex="-1">Args <a class="header-anchor" href="#args-59" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>dst_type</strong> (<code>type or string</code>): the desired type</li></ul><h4 id="returns-49" tabindex="-1">Returns <a class="header-anchor" href="#returns-49" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="zero-grad-self-set-to-none-bool-true-none-source-1" tabindex="-1"><code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#zero-grad-self-set-to-none-bool-true-none-source-1" aria-label="Permalink to &quot;`zero_grad(self, set_to_none: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849)&quot;">​</a></h4><p>Reset gradients of all model parameters.</p><p>See similar function under <code>tensorplay.optim.Optimizer</code> for more context.</p><h4 id="args-60" tabindex="-1">Args <a class="header-anchor" href="#args-60" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>set_to_none</strong> (<code>bool</code>): instead of setting to zero, set the grads to None. See <code>tensorplay.optim.Optimizer.zero_grad</code> for details.</li></ul><hr></details><h3 id="class-parameterdict-source" tabindex="-1"><code>class ParameterDict</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L752" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-parameterdict-source" aria-label="Permalink to &quot;`class ParameterDict` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L752)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">ParameterDict(parameters: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Any&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;None&#39;</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>Module</code></p><p>Holds parameters in a dictionary.</p><p>ParameterDict can be indexed like a regular Python dictionary, but Parameters it contains are properly registered, and will be visible by all Module methods. Other objects are treated as would be done by a regular Python dictionary</p><p><code>~tensorplay.nn.ParameterDict</code> is an <strong>ordered</strong> dictionary. <code>~tensorplay.nn.ParameterDict.update</code> with other unordered mapping types (e.g., Python&#39;s plain <code>dict</code>) does not preserve the order of the merged mapping. On the other hand, <code>OrderedDict</code> or another <code>~tensorplay.nn.ParameterDict</code> will preserve their ordering.</p><p>Note that the constructor, assigning an element of the dictionary and the <code>~tensorplay.nn.ParameterDict.update</code> method will convert any <code>~tensorplay.Tensor</code> into <code>~tensorplay.nn.Parameter</code>.</p><h4 id="args-61" tabindex="-1">Args <a class="header-anchor" href="#args-61" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>values</strong> (<code>iterable, optional</code>): a mapping (dictionary) of (string : Any) or an iterable of key-value pairs of type (string, Any)</li></ul><h4 id="example-22" tabindex="-1">Example <a class="header-anchor" href="#example-22" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">class</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> MyModule</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">nn</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">Module</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> __init__</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self) -&gt; </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    super</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">().</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">__init__</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.params </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.ParameterDict(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            &quot;left&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: nn.Parameter(tensorplay.randn(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)),</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            &quot;right&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: nn.Parameter(tensorplay.randn(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)),</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        }</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    )</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> forward</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, x, choice):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.params[choice].mm(x)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><details><summary>Methods</summary><h4 id="init-self-parameters-any-none-none-source" tabindex="-1"><code>__init__(self, parameters: &#39;Any&#39; = None) -&gt; &#39;None&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L791" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-parameters-any-none-none-source" aria-label="Permalink to &quot;`__init__(self, parameters: &#39;Any&#39; = None) -&gt; &#39;None&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L791)&quot;">​</a></h4><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p><hr><h4 id="add-module-self-name-str-module-optional-forwardref-module-none-source-2" tabindex="-1"><code>add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#add-module-self-name-str-module-optional-forwardref-module-none-source-2" aria-label="Permalink to &quot;`add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667)&quot;">​</a></h4><p>Add a child module to the current module.</p><p>The module can be accessed as an attribute using the given name.</p><h4 id="args-62" tabindex="-1">Args <a class="header-anchor" href="#args-62" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the child module. The child module can be accessed from this module using the given name</li><li><strong>module</strong> (<code>Module</code>): child module to be added to the module.</li></ul><hr><h4 id="apply-self-fn-callable-forwardref-module-nonetype-self-source-2" tabindex="-1"><code>apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#apply-self-fn-callable-forwardref-module-nonetype-self-source-2" aria-label="Permalink to &quot;`apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990)&quot;">​</a></h4><p>Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self.</p><p>Typical use includes initializing the parameters of a model (see also <code>nn-init-doc</code>).</p><h4 id="args-63" tabindex="-1">Args <a class="header-anchor" href="#args-63" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>fn</strong> (``Module<code> -&gt; None</code>): function to be applied to each submodule</li></ul><h4 id="returns-50" tabindex="-1">Returns <a class="header-anchor" href="#returns-50" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="example-23" tabindex="-1">Example <a class="header-anchor" href="#example-23" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@tensorplay.no_grad</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> init_weights</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        m.weight.fill_(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m.weight)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net.apply(init_weights)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><hr><h4 id="bfloat16-self-self-source-2" tabindex="-1"><code>bfloat16(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#bfloat16-self-self-source-2" aria-label="Permalink to &quot;`bfloat16(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-51" tabindex="-1">Returns <a class="header-anchor" href="#returns-51" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source-2" tabindex="-1"><code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source-2" aria-label="Permalink to &quot;`buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627)&quot;">​</a></h4><p>Return an iterator over module buffers.</p><h4 id="args-64" tabindex="-1">Args <a class="header-anchor" href="#args-64" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.</li></ul><h4 id="yields-16" tabindex="-1">Yields <a class="header-anchor" href="#yields-16" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>tensorplay.Tensor: module buffer
</code></pre><h4 id="example-24" tabindex="-1">Example <a class="header-anchor" href="#example-24" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.buffers():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf), buf.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="children-self-collections-abc-iterator-module-source-2" tabindex="-1"><code>children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#children-self-collections-abc-iterator-module-source-2" aria-label="Permalink to &quot;`children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681)&quot;">​</a></h4><p>Return an iterator over immediate children modules.</p><h4 id="yields-17" tabindex="-1">Yields <a class="header-anchor" href="#yields-17" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a child module</li></ul><hr><h4 id="clear-self-none-source-1" tabindex="-1"><code>clear(self) -&gt; &#39;None&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L861" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#clear-self-none-source-1" aria-label="Permalink to &quot;`clear(self) -&gt; &#39;None&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L861)&quot;">​</a></h4><p>Remove all items from the ParameterDict.</p><hr><h4 id="compile-self-args-kwargs-source-2" tabindex="-1"><code>compile(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#compile-self-args-kwargs-source-2" aria-label="Permalink to &quot;`compile(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944)&quot;">​</a></h4><p>Compile this Module&#39;s forward using <code>tensorplay.compile</code>.</p><p>This Module&#39;s <code>__call__</code> method is compiled and all arguments are passed as-is to <code>tensorplay.compile</code>.</p><p>See <code>tensorplay.compile</code> for details on the arguments for this function.</p><hr><h4 id="copy-self-parameterdict-source" tabindex="-1"><code>copy(self) -&gt; &#39;ParameterDict&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L837" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#copy-self-parameterdict-source" aria-label="Permalink to &quot;`copy(self) -&gt; &#39;ParameterDict&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L837)&quot;">​</a></h4><p>Return a copy of this <code>~tensorplay.nn.ParameterDict</code> instance.</p><hr><h4 id="cpu-self-self-source-2" tabindex="-1"><code>cpu(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cpu-self-self-source-2" aria-label="Permalink to &quot;`cpu(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050)&quot;">​</a></h4><p>Move all model parameters and buffers to the CPU.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-52" tabindex="-1">Returns <a class="header-anchor" href="#returns-52" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source-2" tabindex="-1"><code>cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source-2" aria-label="Permalink to &quot;`cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031)&quot;">​</a></h4><p>Move all model parameters and buffers to the GPU.</p><p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on GPU while being optimized.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-65" tabindex="-1">Args <a class="header-anchor" href="#args-65" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>int, optional</code>): if specified, all parameters will be copied to that device</li></ul><h4 id="returns-53" tabindex="-1">Returns <a class="header-anchor" href="#returns-53" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="double-self-self-source-2" tabindex="-1"><code>double(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#double-self-self-source-2" aria-label="Permalink to &quot;`double(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-54" tabindex="-1">Returns <a class="header-anchor" href="#returns-54" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="eval-self-self-source-2" tabindex="-1"><code>eval(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#eval-self-self-source-2" aria-label="Permalink to &quot;`eval(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808)&quot;">​</a></h4><p>Set the module in evaluation mode.</p><p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e. whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, etc.</p><p>This is equivalent with <code>self.train(False) &lt;tensorplay.nn.Module.train&gt;</code>.</p><p>See <code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.</p><h4 id="returns-55" tabindex="-1">Returns <a class="header-anchor" href="#returns-55" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="extra-repr-self-str-source-2" tabindex="-1"><code>extra_repr(self) -&gt; &#39;str&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L955" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#extra-repr-self-str-source-2" aria-label="Permalink to &quot;`extra_repr(self) -&gt; &#39;str&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L955)&quot;">​</a></h4><p>Return the extra representation of the module.</p><p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p><hr><h4 id="float-self-self-source-2" tabindex="-1"><code>float(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#float-self-self-source-2" aria-label="Permalink to &quot;`float(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-56" tabindex="-1">Returns <a class="header-anchor" href="#returns-56" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="forward-self-input-any-none-source-2" tabindex="-1"><code>forward(self, *input: Any) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L388" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#forward-self-input-any-none-source-2" aria-label="Permalink to &quot;`forward(self, *input: Any) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L388)&quot;">​</a></h4><p>Define the computation performed at every call.</p><p>Should be overridden by all subclasses.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code> instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.</p></div><hr><h4 id="fromkeys-keys-iterable-str-default-optional-any-none-parameterdict-source" tabindex="-1"><code>fromkeys(keys: &#39;Iterable[str]&#39;, default: &#39;Optional[Any]&#39; = None) -&gt; &#39;ParameterDict&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L894" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#fromkeys-keys-iterable-str-default-optional-any-none-parameterdict-source" aria-label="Permalink to &quot;`fromkeys(keys: &#39;Iterable[str]&#39;, default: &#39;Optional[Any]&#39; = None) -&gt; &#39;ParameterDict&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L894)&quot;">​</a></h4><p>Return a new ParameterDict with the keys provided.</p><h4 id="args-66" tabindex="-1">Args <a class="header-anchor" href="#args-66" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>keys</strong> (<code>iterable, string</code>): keys to make the new ParameterDict from</li><li><strong>default</strong> (<code>Parameter, optional</code>): value to set for all keys</li></ul><hr><h4 id="get-self-key-str-default-optional-any-none-any-source" tabindex="-1"><code>get(self, key: &#39;str&#39;, default: &#39;Optional[Any]&#39; = None) -&gt; &#39;Any&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L885" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-self-key-str-default-optional-any-none-any-source" aria-label="Permalink to &quot;`get(self, key: &#39;str&#39;, default: &#39;Optional[Any]&#39; = None) -&gt; &#39;Any&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L885)&quot;">​</a></h4><p>Return the parameter associated with key if present. Otherwise return default if provided, None if not.</p><h4 id="args-67" tabindex="-1">Args <a class="header-anchor" href="#args-67" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>key</strong> (<code>str</code>): key to get from the ParameterDict</li><li><strong>default</strong> (<code>Parameter, optional</code>): value to return if key not present</li></ul><hr><h4 id="get-buffer-self-target-str-tensor-source-2" tabindex="-1"><code>get_buffer(self, target: str) -&gt; &#39;Tensor&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-buffer-self-target-str-tensor-source-2" aria-label="Permalink to &quot;`get_buffer(self, target: str) -&gt; &#39;Tensor&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881)&quot;">​</a></h4><p>Return the buffer given by <code>target</code> if it exists, otherwise throw an error.</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-68" tabindex="-1">Args <a class="header-anchor" href="#args-68" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the buffer to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-57" tabindex="-1">Returns <a class="header-anchor" href="#returns-57" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.Tensor: The buffer referenced by ``target``
</code></pre><h4 id="raises-8" tabindex="-1">Raises <a class="header-anchor" href="#raises-8" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not a buffer</li></ul><hr><h4 id="get-extra-state-self-any-source-2" tabindex="-1"><code>get_extra_state(self) -&gt; Any</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-extra-state-self-any-source-2" aria-label="Permalink to &quot;`get_extra_state(self) -&gt; Any` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917)&quot;">​</a></h4><p>Return any extra state to include in the module&#39;s state_dict.</p><p>Implement this and a corresponding <code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module&#39;s <code>state_dict()</code>.</p><p>Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.</p><h4 id="returns-58" tabindex="-1">Returns <a class="header-anchor" href="#returns-58" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>object</strong>: Any extra state to store in the module&#39;s state_dict</li></ul><hr><h4 id="get-parameter-self-target-str-parameter-source-2" tabindex="-1"><code>get_parameter(self, target: str) -&gt; &#39;Parameter&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-parameter-self-target-str-parameter-source-2" aria-label="Permalink to &quot;`get_parameter(self, target: str) -&gt; &#39;Parameter&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845)&quot;">​</a></h4><p>Return the parameter given by <code>target</code> if it exists, otherwise throw an error.</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-69" tabindex="-1">Args <a class="header-anchor" href="#args-69" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the Parameter to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-59" tabindex="-1">Returns <a class="header-anchor" href="#returns-59" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Parameter: The Parameter referenced by ``target``
</code></pre><h4 id="raises-9" tabindex="-1">Raises <a class="header-anchor" href="#raises-9" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not an <code>nn.Parameter</code></li></ul><hr><h4 id="get-submodule-self-target-str-module-source-2" tabindex="-1"><code>get_submodule(self, target: str) -&gt; &#39;Module&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-submodule-self-target-str-module-source-2" aria-label="Permalink to &quot;`get_submodule(self, target: str) -&gt; &#39;Module&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699)&quot;">​</a></h4><p>Return the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> which has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(&quot;net_b.linear&quot;)</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(&quot;net_b.net_c.conv&quot;)</code>.</p><p>The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.</p><h4 id="args-70" tabindex="-1">Args <a class="header-anchor" href="#args-70" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li></ul><h4 id="returns-60" tabindex="-1">Returns <a class="header-anchor" href="#returns-60" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Module: The submodule referenced by ``target``
</code></pre><h4 id="raises-10" tabindex="-1">Raises <a class="header-anchor" href="#raises-10" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If at any point along the path resulting from the target string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="half-self-self-source-2" tabindex="-1"><code>half(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#half-self-self-source-2" aria-label="Permalink to &quot;`half(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-61" tabindex="-1">Returns <a class="header-anchor" href="#returns-61" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="items-self-iterable-tuple-str-any-source" tabindex="-1"><code>items(self) -&gt; &#39;Iterable[tuple[str, Any]]&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L908" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#items-self-iterable-tuple-str-any-source" aria-label="Permalink to &quot;`items(self) -&gt; &#39;Iterable[tuple[str, Any]]&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L908)&quot;">​</a></h4><p>Return an iterable of the ParameterDict key/value pairs.</p><hr><h4 id="keys-self-container-abcs-keysview-str-source-1" tabindex="-1"><code>keys(self) -&gt; &#39;container_abcs.KeysView[str]&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L904" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#keys-self-container-abcs-keysview-str-source-1" aria-label="Permalink to &quot;`keys(self) -&gt; &#39;container_abcs.KeysView[str]&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L904)&quot;">​</a></h4><p>Return an iterable of the ParameterDict keys.</p><hr><h4 id="load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source-2" tabindex="-1"><code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source-2" aria-label="Permalink to &quot;`load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436)&quot;">​</a></h4><p>Copy parameters and buffers from <code>state_dict</code> into this module and its descendants.</p><p>If <code>strict</code> is <code>True</code>, then the keys of <code>state_dict</code> must exactly match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function.</p><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>If <code>assign</code> is <code>True</code> the optimizer must be created after the call to <code>load_state_dict</code> unless <code>~tensorplay.__future__.get_swap_module_params_on_conversion</code> is <code>True</code>.</p></div><h4 id="args-71" tabindex="-1">Args <a class="header-anchor" href="#args-71" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state_dict</strong> (<code>dict</code>): a dict containing parameters and persistent buffers.</li><li><strong>strict</strong> (<code>bool, optional</code>): whether to strictly enforce that the keys in <code>state_dict</code> match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function. Default: <code>True</code></li><li><strong>assign</strong> (<code>bool, optional</code>): When set to <code>False</code>, the properties of the tensors in the current module are preserved whereas setting it to <code>True</code> preserves properties of the Tensors in the state dict. The only exception is the <code>requires_grad</code> field of <code>~tensorplay.nn.Parameter</code> for which the value from the module is preserved. Default: <code>False</code></li></ul><h4 id="returns-62" tabindex="-1">Returns <a class="header-anchor" href="#returns-62" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:
    * ``missing_keys`` is a list of str containing any keys that are expected
        by this module but missing from the provided ``state_dict``.
    * ``unexpected_keys`` is a list of str containing the keys that are not
        expected by this module but present in the provided ``state_dict``.
</code></pre><h4 id="note-6" tabindex="-1">Note <a class="header-anchor" href="#note-6" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>If a parameter or buffer is registered as ``None`` and its corresponding key
exists in `state_dict`, `load_state_dict` will raise a
``RuntimeError``.
</code></pre><hr><h4 id="modules-self-collections-abc-iterator-module-source-2" tabindex="-1"><code>modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#modules-self-collections-abc-iterator-module-source-2" aria-label="Permalink to &quot;`modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710)&quot;">​</a></h4><p>Return an iterator over all modules in the network.</p><h4 id="yields-18" tabindex="-1">Yields <a class="header-anchor" href="#yields-18" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a module in the network</li></ul><h4 id="note-7" tabindex="-1">Note <a class="header-anchor" href="#note-7" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-25" tabindex="-1">Example <a class="header-anchor" href="#example-25" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source-2" tabindex="-1"><code>named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source-2" aria-label="Permalink to &quot;`named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650)&quot;">​</a></h4><p>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p><h4 id="args-72" tabindex="-1">Args <a class="header-anchor" href="#args-72" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all buffer names.</li><li><strong>recurse</strong> (<code>bool, optional</code>): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Defaults to True.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated buffers in the result. Defaults to True.</li></ul><h4 id="yields-19" tabindex="-1">Yields <a class="header-anchor" href="#yields-19" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, tensorplay.Tensor): Tuple containing the name and buffer
</code></pre><h4 id="example-26" tabindex="-1">Example <a class="header-anchor" href="#example-26" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_buffers():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_var&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-children-self-collections-abc-iterator-tuple-str-module-source-2" tabindex="-1"><code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-children-self-collections-abc-iterator-tuple-str-module-source-2" aria-label="Permalink to &quot;`named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690)&quot;">​</a></h4><p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p><h4 id="yields-20" tabindex="-1">Yields <a class="header-anchor" href="#yields-20" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple containing a name and child module
</code></pre><h4 id="example-27" tabindex="-1">Example <a class="header-anchor" href="#example-27" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, module </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.named_children():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv4&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv5&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(module)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source-2" tabindex="-1"><code>named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source-2" aria-label="Permalink to &quot;`named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737)&quot;">​</a></h4><p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p><h4 id="args-73" tabindex="-1">Args <a class="header-anchor" href="#args-73" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>memo</strong>: a memo to store the set of modules already added to the result</li><li><strong>prefix</strong>: a prefix that will be added to the name of the module</li><li><strong>remove_duplicate</strong>: whether to remove the duplicated module instances in the result or not</li></ul><h4 id="yields-21" tabindex="-1">Yields <a class="header-anchor" href="#yields-21" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple of name and module
</code></pre><h4 id="note-8" tabindex="-1">Note <a class="header-anchor" href="#note-8" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-28" tabindex="-1">Example <a class="header-anchor" href="#example-28" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.named_modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;0&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source-2" tabindex="-1"><code>named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source-2" aria-label="Permalink to &quot;`named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595)&quot;">​</a></h4><p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p><h4 id="args-74" tabindex="-1">Args <a class="header-anchor" href="#args-74" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all parameter names.</li><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated parameters in the result. Defaults to True.</li></ul><h4 id="yields-22" tabindex="-1">Yields <a class="header-anchor" href="#yields-22" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Parameter): Tuple containing the name and parameter
</code></pre><h4 id="example-29" tabindex="-1">Example <a class="header-anchor" href="#example-29" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_parameters():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source-2" tabindex="-1"><code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source-2" aria-label="Permalink to &quot;`parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570)&quot;">​</a></h4><p>Return an iterator over module parameters.</p><p>This is typically passed to an optimizer.</p><h4 id="args-75" tabindex="-1">Args <a class="header-anchor" href="#args-75" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</li></ul><h4 id="yields-23" tabindex="-1">Yields <a class="header-anchor" href="#yields-23" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Parameter</strong>: module parameter</li></ul><h4 id="example-30" tabindex="-1">Example <a class="header-anchor" href="#example-30" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.parameters():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param), param.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="pop-self-key-str-any-source" tabindex="-1"><code>pop(self, key: &#39;str&#39;) -&gt; &#39;Any&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L866" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#pop-self-key-str-any-source" aria-label="Permalink to &quot;`pop(self, key: &#39;str&#39;) -&gt; &#39;Any&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L866)&quot;">​</a></h4><p>Remove key from the ParameterDict and return its parameter.</p><h4 id="args-76" tabindex="-1">Args <a class="header-anchor" href="#args-76" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>key</strong> (<code>str</code>): key to pop from the ParameterDict</li></ul><hr><h4 id="popitem-self-tuple-str-any-source" tabindex="-1"><code>popitem(self) -&gt; &#39;tuple[str, Any]&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L876" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#popitem-self-tuple-str-any-source" aria-label="Permalink to &quot;`popitem(self) -&gt; &#39;tuple[str, Any]&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L876)&quot;">​</a></h4><p>Remove and return the last inserted <code>(key, parameter)</code> pair from the ParameterDict.</p><hr><h4 id="register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source-2" tabindex="-1"><code>register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source-2" aria-label="Permalink to &quot;`register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>This function is deprecated in favor of <code>~tensorplay.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.</p><h4 id="returns-63" tabindex="-1">Returns <a class="header-anchor" href="#returns-63" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source-2" tabindex="-1"><code>register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source-2" aria-label="Permalink to &quot;`register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553)&quot;">​</a></h4><p>Add a buffer to the module.</p><p>This is typically used to register a buffer that should not be considered a model parameter. For example, BatchNorm&#39;s <code>running_mean</code> is not a parameter, but is part of the module&#39;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting <code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&#39;s <code>state_dict</code>.</p><p>Buffers can be accessed as attributes using given names.</p><h4 id="args-77" tabindex="-1">Args <a class="header-anchor" href="#args-77" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the buffer. The buffer can be accessed from this module using the given name</li><li><strong>tensor</strong> (<code>Tensor or None</code>): buffer to be registered. If <code>None</code>, then operations that run on buffers, such as <code>cuda</code>, are ignored. If <code>None</code>, the buffer is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li><li><strong>persistent</strong> (<code>bool</code>): whether the buffer is part of this module&#39;s <code>state_dict</code>.</li></ul><h4 id="example-31" tabindex="-1">Example <a class="header-anchor" href="#example-31" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.register_buffer(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_mean&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, tensorplay.zeros(num_features))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><hr><h4 id="register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source-2" tabindex="-1"><code>register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source-2" aria-label="Permalink to &quot;`register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592)&quot;">​</a></h4><p>Register a forward hook on the module.</p><p>The hook will be called every time after <code>forward</code> has computed an output.</p><p>If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after <code>forward</code> is called. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-78" tabindex="-1">Args <a class="header-anchor" href="#args-78" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If <code>True</code>, the provided <code>hook</code> will be fired before all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward</code> hooks registered with <code>register_module_forward_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If <code>True</code>, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>always_call</strong> (<code>bool</code>): If <code>True</code> the <code>hook</code> will be run regardless of whether an exception is raised while calling the Module.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-64" tabindex="-1">Returns <a class="header-anchor" href="#returns-64" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source-2" tabindex="-1"><code>register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source-2" aria-label="Permalink to &quot;`register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526)&quot;">​</a></h4><p>Register a forward pre-hook on the module.</p><p>The hook will be called every time before <code>forward</code> is invoked.</p><p>If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> of modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> and</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> kwargs</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-79" tabindex="-1">Args <a class="header-anchor" href="#args-79" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward_pre</code> hooks registered with <code>register_module_forward_pre_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If true, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-65" tabindex="-1">Returns <a class="header-anchor" href="#returns-65" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-2" tabindex="-1"><code>register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-2" aria-label="Permalink to &quot;`register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>The hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:</p><pre><code>1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.
2. If none of the module inputs require gradients, the hook will fire when the gradients are computed
   with respect to module outputs.
3. If none of the module outputs require gradients, then the hooks will not fire.
</code></pre><p>The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_input, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Tensor) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_input</code> and <code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of <code>grad_input</code> in subsequent computations. <code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in <code>grad_input</code> and <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> outputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-80" tabindex="-1">Args <a class="header-anchor" href="#args-80" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward</code> hooks registered with <code>register_module_full_backward_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-66" tabindex="-1">Returns <a class="header-anchor" href="#returns-66" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-2" tabindex="-1"><code>register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-2" aria-label="Permalink to &quot;`register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287)&quot;">​</a></h4><p>Register a backward pre-hook on the module.</p><p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tuple[Tensor] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of <code>grad_output</code> in subsequent computations. Entries in <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-81" tabindex="-1">Args <a class="header-anchor" href="#args-81" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward_pre</code> hooks registered with <code>register_module_full_backward_pre_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-67" tabindex="-1">Returns <a class="header-anchor" href="#returns-67" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-post-hook-self-hook-source-2" tabindex="-1"><code>register_load_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-post-hook-self-hook-source-2" aria-label="Permalink to &quot;`register_load_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226)&quot;">​</a></h4><p>Register a post-hook to be run after module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, incompatible_keys) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.</p><p>The given incompatible_keys can be modified inplace if needed.</p><p>Note that the checks performed when calling <code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.</p><h4 id="returns-68" tabindex="-1">Returns <a class="header-anchor" href="#returns-68" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-pre-hook-self-hook-source-2" tabindex="-1"><code>register_load_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-pre-hook-self-hook-source-2" aria-label="Permalink to &quot;`register_load_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214)&quot;">​</a></h4><p>Register a pre-hook to be run before module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # noqa: B950</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="arguments-2" tabindex="-1">Arguments <a class="header-anchor" href="#arguments-2" aria-label="Permalink to &quot;Arguments&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): Callable hook that will be invoked before loading the state dict.</li></ul><hr><h4 id="register-module-self-name-str-module-optional-forwardref-module-none-source-2" tabindex="-1"><code>register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-module-self-name-str-module-optional-forwardref-module-none-source-2" aria-label="Permalink to &quot;`register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695)&quot;">​</a></h4><p>Alias for <code>add_module</code>.</p><hr><h4 id="register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source-2" tabindex="-1"><code>register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source-2" aria-label="Permalink to &quot;`register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617)&quot;">​</a></h4><p>Add a parameter to the module.</p><p>The parameter can be accessed as an attribute using given name.</p><h4 id="args-82" tabindex="-1">Args <a class="header-anchor" href="#args-82" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the parameter. The parameter can be accessed from this module using the given name</li><li><strong>param</strong> (<code>Parameter or None</code>): parameter to be added to the module. If <code>None</code>, then operations that run on parameters, such as <code>cuda</code>, are ignored. If <code>None</code>, the parameter is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li></ul><hr><h4 id="register-state-dict-post-hook-self-hook-source-2" tabindex="-1"><code>register_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-post-hook-self-hook-source-2" aria-label="Permalink to &quot;`register_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017)&quot;">​</a></h4><p>Register a post-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can modify the <code>state_dict</code> inplace.</p><hr><h4 id="register-state-dict-pre-hook-self-hook-source-2" tabindex="-1"><code>register_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-pre-hook-self-hook-source-2" aria-label="Permalink to &quot;`register_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041)&quot;">​</a></h4><p>Register a pre-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, prefix, keep_vars) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p><hr><h4 id="requires-grad-self-requires-grad-bool-true-self-source-2" tabindex="-1"><code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#requires-grad-self-requires-grad-bool-true-self-source-2" aria-label="Permalink to &quot;`requires_grad_(self, requires_grad: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826)&quot;">​</a></h4><p>Change if autograd should record operations on parameters in this module.</p><p>This method sets the parameters&#39; <code>requires_grad</code> attributes in-place.</p><p>This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).</p><p>See <code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.</p><h4 id="args-83" tabindex="-1">Args <a class="header-anchor" href="#args-83" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>requires_grad</strong> (<code>bool</code>): whether autograd should record operations on parameters in this module. Default: <code>True</code>.</li></ul><h4 id="returns-69" tabindex="-1">Returns <a class="header-anchor" href="#returns-69" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="set-extra-state-self-state-any-none-source-2" tabindex="-1"><code>set_extra_state(self, state: Any) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-extra-state-self-state-any-none-source-2" aria-label="Permalink to &quot;`set_extra_state(self, state: Any) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938)&quot;">​</a></h4><p>Set extra state contained in the loaded <code>state_dict</code>.</p><p>This function is called from <code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding <code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.</p><h4 id="args-84" tabindex="-1">Args <a class="header-anchor" href="#args-84" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state</strong> (<code>dict</code>): Extra state from the <code>state_dict</code></li></ul><hr><h4 id="set-submodule-self-target-str-module-module-strict-bool-false-none-source-2" tabindex="-1"><code>set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-submodule-self-target-str-module-module-strict-bool-false-none-source-2" aria-label="Permalink to &quot;`set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764)&quot;">​</a></h4><p>Set the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>If <code>strict</code> is set to <code>False</code> (default), the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>strict</code> is set to <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule does not exist.</p></div><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(3, 3, 3) ) (linear): Linear(3, 3) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To override the <code>Conv2d</code> with a new submodule <code>Linear</code>, you could call <code>set_submodule(&quot;net_b.net_c.conv&quot;, nn.Linear(1, 1))</code> where <code>strict</code> could be <code>True</code> or <code>False</code></p><p>To add a new submodule <code>Conv2d</code> to the existing <code>net_b</code> module, you would call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1))</code>.</p><p>In the above if you set <code>strict=True</code> and call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1), strict=True)</code>, an AttributeError will be raised because <code>net_b</code> does not have a submodule named <code>conv</code>.</p><h4 id="args-85" tabindex="-1">Args <a class="header-anchor" href="#args-85" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li><li><strong>module</strong>: The module to set the submodule to.</li><li><strong>strict</strong>: If <code>False</code>, the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule doesn&#39;t already exist.</li></ul><h4 id="raises-11" tabindex="-1">Raises <a class="header-anchor" href="#raises-11" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>ValueError</strong>: If the <code>target</code> string is empty or if <code>module</code> is not an instance of <code>nn.Module</code>.</li><li><strong>AttributeError</strong>: If at any point along the path resulting from the <code>target</code> string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="setdefault-self-key-str-default-optional-any-none-any-source" tabindex="-1"><code>setdefault(self, key: &#39;str&#39;, default: &#39;Optional[Any]&#39; = None) -&gt; &#39;Any&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L846" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#setdefault-self-key-str-default-optional-any-none-any-source" aria-label="Permalink to &quot;`setdefault(self, key: &#39;str&#39;, default: &#39;Optional[Any]&#39; = None) -&gt; &#39;Any&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L846)&quot;">​</a></h4><p>Set the default for a key in the ParameterDict.</p><p>If key is in the ParameterDict, return its value. If not, insert <code>key</code> with a parameter <code>default</code> and return <code>default</code>. <code>default</code> defaults to <code>None</code>.</p><h4 id="args-86" tabindex="-1">Args <a class="header-anchor" href="#args-86" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>key</strong> (<code>str</code>): key to set default for</li><li><strong>default</strong> (<code>Any</code>): the parameter set to the key</li></ul><hr><h4 id="share-memory-self-self-source-2" tabindex="-1"><code>share_memory(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#share-memory-self-self-source-2" aria-label="Permalink to &quot;`share_memory(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877)&quot;">​</a></h4><p>See <code>tensorplay.Tensor.share_memory_</code>.</p><hr><h4 id="state-dict-self-args-destination-none-prefix-keep-vars-false-source-2" tabindex="-1"><code>state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#state-dict-self-args-destination-none-prefix-keep-vars-false-source-2" aria-label="Permalink to &quot;`state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105)&quot;">​</a></h4><p>Return a dictionary containing references to the whole state of the module.</p><p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>The returned object is a shallow copy. It contains references to the module&#39;s parameters and buffers.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Currently <code>state_dict()</code> also accepts positional arguments for <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However, this is being deprecated and keyword arguments will be enforced in future releases.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Please avoid the use of argument <code>destination</code> as it is not designed for end-users.</p></div><h4 id="args-87" tabindex="-1">Args <a class="header-anchor" href="#args-87" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>destination</strong> (<code>dict, optional</code>): If provided, the state of module will be updated into the dict and the same object is returned. Otherwise, an <code>OrderedDict</code> will be created and returned.</li><li><strong>Default</strong>: <code>None</code>.</li><li><strong>prefix</strong> (<code>str, optional</code>): a prefix added to parameter and buffer names to compose the keys in state_dict. Default: <code>&#39;&#39;</code>.</li><li><strong>keep_vars</strong> (<code>bool, optional</code>): by default the <code>~tensorplay.Tensor</code> s returned in the state dict are detached from autograd. If it&#39;s set to <code>True</code>, detaching will not be performed.</li><li><strong>Default</strong>: <code>False</code>.</li></ul><h4 id="returns-70" tabindex="-1">Returns <a class="header-anchor" href="#returns-70" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>dict</strong>: a dictionary containing a whole state of the module</li></ul><h4 id="example-32" tabindex="-1">Example <a class="header-anchor" href="#example-32" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">module.state_dict().keys()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;weight&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><hr><h4 id="to-self-args-kwargs-source-2" tabindex="-1"><code>to(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-self-args-kwargs-source-2" aria-label="Permalink to &quot;`to(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151)&quot;">​</a></h4><p>Move and/or cast the parameters and buffers.</p><p>This can be called as</p><p>.. function:: to(device=None, dtype=None, non_blocking=False) :noindex:</p><p>.. function:: to(dtype, non_blocking=False) :noindex:</p><p>.. function:: to(tensor, non_blocking=False) :noindex:</p><p>.. function:: to(memory_format=tensorplay.channels_last) :noindex:</p><p>Its signature is similar to <code>tensorplay.Tensor.to</code>, but only accepts floating point or complex <code>dtype</code>\ s. In addition, this method will only cast the floating point or complex parameters and buffers to <code>dtype</code> (if given). The integral parameters and buffers will be moved <code>device</code>, if that is given, but with dtypes unchanged. When <code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.</p><p>See below for examples.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-88" tabindex="-1">Args <a class="header-anchor" href="#args-88" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): the desired device of the parameters and buffers in this module</li><li><strong>dtype</strong> (<code>tensorplay.dtype</code>): the desired floating point or complex dtype of the parameters and buffers in this module</li><li><strong>tensor</strong> (<code>tensorplay.Tensor</code>): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module</li><li><strong>memory_format</strong> (<code>tensorplay.memory_format</code>): the desired memory format for 4D parameters and buffers in this module (keyword only argument)</li></ul><h4 id="returns-71" tabindex="-1">Returns <a class="header-anchor" href="#returns-71" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="examples-2" tabindex="-1">Examples <a class="header-anchor" href="#examples-2" aria-label="Permalink to &quot;Examples&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +IGNORE_WANT(&quot;non-deterministic&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(tensorplay.double)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float64)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +REQUIRES(env:TENSORPLAY_DOCTEST_CUDA1)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">gpu1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cuda:1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(gpu1, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.half, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">non_blocking</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;cuda:1&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cpu </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cpu&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(cpu)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).to(tensorplay.cdouble)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3741</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,  </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2382</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5593</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.4443</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear(tensorplay.ones(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.cdouble))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br></div></div><hr><h4 id="to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source-2" tabindex="-1"><code>to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source-2" aria-label="Permalink to &quot;`to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119)&quot;">​</a></h4><p>Move the parameters and buffers to the specified device without copying storage.</p><h4 id="args-89" tabindex="-1">Args <a class="header-anchor" href="#args-89" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): The desired device of the parameters and buffers in this module.</li><li><strong>recurse</strong> (<code>bool</code>): Whether parameters and buffers of submodules should be recursively moved to the specified device.</li></ul><h4 id="returns-72" tabindex="-1">Returns <a class="header-anchor" href="#returns-72" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="train-self-mode-bool-true-self-source-2" tabindex="-1"><code>train(self, mode: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#train-self-mode-bool-true-self-source-2" aria-label="Permalink to &quot;`train(self, mode: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786)&quot;">​</a></h4><p>Set the module in training mode.</p><p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e., whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, etc.</p><h4 id="args-90" tabindex="-1">Args <a class="header-anchor" href="#args-90" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>mode</strong> (<code>bool</code>): whether to set training mode (<code>True</code>) or evaluation mode (<code>False</code>). Default: <code>True</code>.</li></ul><h4 id="returns-73" tabindex="-1">Returns <a class="header-anchor" href="#returns-73" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="type-self-dst-type-union-tensorplay-dtype-str-self-source-2" tabindex="-1"><code>type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#type-self-dst-type-union-tensorplay-dtype-str-self-source-2" aria-label="Permalink to &quot;`type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061)&quot;">​</a></h4><p>Casts all parameters and buffers to <code>dst_type</code>.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-91" tabindex="-1">Args <a class="header-anchor" href="#args-91" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>dst_type</strong> (<code>type or string</code>): the desired type</li></ul><h4 id="returns-74" tabindex="-1">Returns <a class="header-anchor" href="#returns-74" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="update-self-parameters-union-mapping-str-any-parameterdict-none-source" tabindex="-1"><code>update(self, parameters: &#39;Union[Mapping[str, Any], ParameterDict]&#39;) -&gt; &#39;None&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L916" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#update-self-parameters-union-mapping-str-any-parameterdict-none-source" aria-label="Permalink to &quot;`update(self, parameters: &#39;Union[Mapping[str, Any], ParameterDict]&#39;) -&gt; &#39;None&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L916)&quot;">​</a></h4><p>Update the <code>~tensorplay.nn.ParameterDict</code> with key-value pairs from <code>parameters</code>, overwriting existing keys.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>If <code>parameters</code> is an <code>OrderedDict</code>, a <code>~tensorplay.nn.ParameterDict</code>, or an iterable of key-value pairs, the order of new elements in it is preserved.</p></div><h4 id="args-92" tabindex="-1">Args <a class="header-anchor" href="#args-92" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>parameters</strong> (<code>iterable</code>): a mapping (dictionary) from string to <code>~tensorplay.nn.Parameter</code>, or an iterable of key-value pairs of type (string, <code>~tensorplay.nn.Parameter</code>)</li></ul><hr><h4 id="values-self-iterable-any-source" tabindex="-1"><code>values(self) -&gt; &#39;Iterable[Any]&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L912" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#values-self-iterable-any-source" aria-label="Permalink to &quot;`values(self) -&gt; &#39;Iterable[Any]&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L912)&quot;">​</a></h4><p>Return an iterable of the ParameterDict values.</p><hr><h4 id="zero-grad-self-set-to-none-bool-true-none-source-2" tabindex="-1"><code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#zero-grad-self-set-to-none-bool-true-none-source-2" aria-label="Permalink to &quot;`zero_grad(self, set_to_none: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849)&quot;">​</a></h4><p>Reset gradients of all model parameters.</p><p>See similar function under <code>tensorplay.optim.Optimizer</code> for more context.</p><h4 id="args-93" tabindex="-1">Args <a class="header-anchor" href="#args-93" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>set_to_none</strong> (<code>bool</code>): instead of setting to zero, set the grads to None. See <code>tensorplay.optim.Optimizer.zero_grad</code> for details.</li></ul><hr></details><h3 id="class-parameterlist-source" tabindex="-1"><code>class ParameterList</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L611" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-parameterlist-source" aria-label="Permalink to &quot;`class ParameterList` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L611)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">ParameterList(values: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Optional[Iterable[Any]]&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;None&#39;</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>Module</code></p><p>Holds parameters in a list.</p><p><code>~tensorplay.nn.ParameterList</code> can be used like a regular Python list, but Tensors that are <code>~tensorplay.nn.Parameter</code> are properly registered, and will be visible by all <code>~tensorplay.nn.Module</code> methods.</p><p>Note that the constructor, assigning an element of the list, the <code>~tensorplay.nn.ParameterList.append</code> method and the <code>~tensorplay.nn.ParameterList.extend</code> method will convert any <code>~tensorplay.Tensor</code> into <code>~tensorplay.nn.Parameter</code>.</p><h4 id="args-94" tabindex="-1">Args <a class="header-anchor" href="#args-94" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>values</strong> (<code>iterable, optional</code>): an iterable of elements to add to the list.</li></ul><h4 id="example-33" tabindex="-1">Example <a class="header-anchor" href="#example-33" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">class</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> MyModule</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">nn</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">Module</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> __init__</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self) -&gt; </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    super</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">().</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">__init__</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.params </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.ParameterList(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [nn.Parameter(tensorplay.randn(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> i </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> range</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    )</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> forward</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, x):</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # ParameterList can act as an iterable, or be indexed using ints</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> i, p </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.params):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.params[i </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">//</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">].mm(x) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> p.mm(x)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><details><summary>Methods</summary><h4 id="init-self-values-optional-iterable-any-none-none-source" tabindex="-1"><code>__init__(self, values: &#39;Optional[Iterable[Any]]&#39; = None) -&gt; &#39;None&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L641" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-values-optional-iterable-any-none-none-source" aria-label="Permalink to &quot;`__init__(self, values: &#39;Optional[Iterable[Any]]&#39; = None) -&gt; &#39;None&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L641)&quot;">​</a></h4><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p><hr><h4 id="add-module-self-name-str-module-optional-forwardref-module-none-source-3" tabindex="-1"><code>add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#add-module-self-name-str-module-optional-forwardref-module-none-source-3" aria-label="Permalink to &quot;`add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667)&quot;">​</a></h4><p>Add a child module to the current module.</p><p>The module can be accessed as an attribute using the given name.</p><h4 id="args-95" tabindex="-1">Args <a class="header-anchor" href="#args-95" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the child module. The child module can be accessed from this module using the given name</li><li><strong>module</strong> (<code>Module</code>): child module to be added to the module.</li></ul><hr><h4 id="append-self-value-any-self-source" tabindex="-1"><code>append(self, value: &#39;Any&#39;) -&gt; &#39;Self&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L698" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#append-self-value-any-self-source" aria-label="Permalink to &quot;`append(self, value: &#39;Any&#39;) -&gt; &#39;Self&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L698)&quot;">​</a></h4><p>Append a given value at the end of the list.</p><h4 id="args-96" tabindex="-1">Args <a class="header-anchor" href="#args-96" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>value</strong> (<code>Any</code>): value to append</li></ul><hr><h4 id="apply-self-fn-callable-forwardref-module-nonetype-self-source-3" tabindex="-1"><code>apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#apply-self-fn-callable-forwardref-module-nonetype-self-source-3" aria-label="Permalink to &quot;`apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990)&quot;">​</a></h4><p>Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self.</p><p>Typical use includes initializing the parameters of a model (see also <code>nn-init-doc</code>).</p><h4 id="args-97" tabindex="-1">Args <a class="header-anchor" href="#args-97" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>fn</strong> (``Module<code> -&gt; None</code>): function to be applied to each submodule</li></ul><h4 id="returns-75" tabindex="-1">Returns <a class="header-anchor" href="#returns-75" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="example-34" tabindex="-1">Example <a class="header-anchor" href="#example-34" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@tensorplay.no_grad</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> init_weights</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        m.weight.fill_(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m.weight)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net.apply(init_weights)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><hr><h4 id="bfloat16-self-self-source-3" tabindex="-1"><code>bfloat16(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#bfloat16-self-self-source-3" aria-label="Permalink to &quot;`bfloat16(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-76" tabindex="-1">Returns <a class="header-anchor" href="#returns-76" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source-3" tabindex="-1"><code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source-3" aria-label="Permalink to &quot;`buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627)&quot;">​</a></h4><p>Return an iterator over module buffers.</p><h4 id="args-98" tabindex="-1">Args <a class="header-anchor" href="#args-98" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.</li></ul><h4 id="yields-24" tabindex="-1">Yields <a class="header-anchor" href="#yields-24" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>tensorplay.Tensor: module buffer
</code></pre><h4 id="example-35" tabindex="-1">Example <a class="header-anchor" href="#example-35" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.buffers():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf), buf.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="children-self-collections-abc-iterator-module-source-3" tabindex="-1"><code>children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#children-self-collections-abc-iterator-module-source-3" aria-label="Permalink to &quot;`children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681)&quot;">​</a></h4><p>Return an iterator over immediate children modules.</p><h4 id="yields-25" tabindex="-1">Yields <a class="header-anchor" href="#yields-25" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a child module</li></ul><hr><h4 id="compile-self-args-kwargs-source-3" tabindex="-1"><code>compile(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#compile-self-args-kwargs-source-3" aria-label="Permalink to &quot;`compile(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944)&quot;">​</a></h4><p>Compile this Module&#39;s forward using <code>tensorplay.compile</code>.</p><p>This Module&#39;s <code>__call__</code> method is compiled and all arguments are passed as-is to <code>tensorplay.compile</code>.</p><p>See <code>tensorplay.compile</code> for details on the arguments for this function.</p><hr><h4 id="cpu-self-self-source-3" tabindex="-1"><code>cpu(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cpu-self-self-source-3" aria-label="Permalink to &quot;`cpu(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050)&quot;">​</a></h4><p>Move all model parameters and buffers to the CPU.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-77" tabindex="-1">Returns <a class="header-anchor" href="#returns-77" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source-3" tabindex="-1"><code>cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source-3" aria-label="Permalink to &quot;`cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031)&quot;">​</a></h4><p>Move all model parameters and buffers to the GPU.</p><p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on GPU while being optimized.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-99" tabindex="-1">Args <a class="header-anchor" href="#args-99" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>int, optional</code>): if specified, all parameters will be copied to that device</li></ul><h4 id="returns-78" tabindex="-1">Returns <a class="header-anchor" href="#returns-78" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="double-self-self-source-3" tabindex="-1"><code>double(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#double-self-self-source-3" aria-label="Permalink to &quot;`double(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-79" tabindex="-1">Returns <a class="header-anchor" href="#returns-79" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="eval-self-self-source-3" tabindex="-1"><code>eval(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#eval-self-self-source-3" aria-label="Permalink to &quot;`eval(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808)&quot;">​</a></h4><p>Set the module in evaluation mode.</p><p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e. whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, etc.</p><p>This is equivalent with <code>self.train(False) &lt;tensorplay.nn.Module.train&gt;</code>.</p><p>See <code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.</p><h4 id="returns-80" tabindex="-1">Returns <a class="header-anchor" href="#returns-80" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="extend-self-values-iterable-any-self-source" tabindex="-1"><code>extend(self, values: &#39;Iterable[Any]&#39;) -&gt; &#39;Self&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L709" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#extend-self-values-iterable-any-self-source" aria-label="Permalink to &quot;`extend(self, values: &#39;Iterable[Any]&#39;) -&gt; &#39;Self&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L709)&quot;">​</a></h4><p>Append values from a Python iterable to the end of the list.</p><h4 id="args-100" tabindex="-1">Args <a class="header-anchor" href="#args-100" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>values</strong> (<code>iterable</code>): iterable of values to append</li></ul><hr><h4 id="extra-repr-self-str-source-3" tabindex="-1"><code>extra_repr(self) -&gt; &#39;str&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L725" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#extra-repr-self-str-source-3" aria-label="Permalink to &quot;`extra_repr(self) -&gt; &#39;str&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L725)&quot;">​</a></h4><p>Return the extra representation of the module.</p><hr><h4 id="float-self-self-source-3" tabindex="-1"><code>float(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#float-self-self-source-3" aria-label="Permalink to &quot;`float(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-81" tabindex="-1">Returns <a class="header-anchor" href="#returns-81" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="forward-self-input-any-none-source-3" tabindex="-1"><code>forward(self, *input: Any) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L388" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#forward-self-input-any-none-source-3" aria-label="Permalink to &quot;`forward(self, *input: Any) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L388)&quot;">​</a></h4><p>Define the computation performed at every call.</p><p>Should be overridden by all subclasses.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code> instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.</p></div><hr><h4 id="get-buffer-self-target-str-tensor-source-3" tabindex="-1"><code>get_buffer(self, target: str) -&gt; &#39;Tensor&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-buffer-self-target-str-tensor-source-3" aria-label="Permalink to &quot;`get_buffer(self, target: str) -&gt; &#39;Tensor&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881)&quot;">​</a></h4><p>Return the buffer given by <code>target</code> if it exists, otherwise throw an error.</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-101" tabindex="-1">Args <a class="header-anchor" href="#args-101" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the buffer to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-82" tabindex="-1">Returns <a class="header-anchor" href="#returns-82" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.Tensor: The buffer referenced by ``target``
</code></pre><h4 id="raises-12" tabindex="-1">Raises <a class="header-anchor" href="#raises-12" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not a buffer</li></ul><hr><h4 id="get-extra-state-self-any-source-3" tabindex="-1"><code>get_extra_state(self) -&gt; Any</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-extra-state-self-any-source-3" aria-label="Permalink to &quot;`get_extra_state(self) -&gt; Any` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917)&quot;">​</a></h4><p>Return any extra state to include in the module&#39;s state_dict.</p><p>Implement this and a corresponding <code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module&#39;s <code>state_dict()</code>.</p><p>Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.</p><h4 id="returns-83" tabindex="-1">Returns <a class="header-anchor" href="#returns-83" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>object</strong>: Any extra state to store in the module&#39;s state_dict</li></ul><hr><h4 id="get-parameter-self-target-str-parameter-source-3" tabindex="-1"><code>get_parameter(self, target: str) -&gt; &#39;Parameter&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-parameter-self-target-str-parameter-source-3" aria-label="Permalink to &quot;`get_parameter(self, target: str) -&gt; &#39;Parameter&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845)&quot;">​</a></h4><p>Return the parameter given by <code>target</code> if it exists, otherwise throw an error.</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-102" tabindex="-1">Args <a class="header-anchor" href="#args-102" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the Parameter to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-84" tabindex="-1">Returns <a class="header-anchor" href="#returns-84" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Parameter: The Parameter referenced by ``target``
</code></pre><h4 id="raises-13" tabindex="-1">Raises <a class="header-anchor" href="#raises-13" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not an <code>nn.Parameter</code></li></ul><hr><h4 id="get-submodule-self-target-str-module-source-3" tabindex="-1"><code>get_submodule(self, target: str) -&gt; &#39;Module&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-submodule-self-target-str-module-source-3" aria-label="Permalink to &quot;`get_submodule(self, target: str) -&gt; &#39;Module&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699)&quot;">​</a></h4><p>Return the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> which has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(&quot;net_b.linear&quot;)</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(&quot;net_b.net_c.conv&quot;)</code>.</p><p>The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.</p><h4 id="args-103" tabindex="-1">Args <a class="header-anchor" href="#args-103" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li></ul><h4 id="returns-85" tabindex="-1">Returns <a class="header-anchor" href="#returns-85" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Module: The submodule referenced by ``target``
</code></pre><h4 id="raises-14" tabindex="-1">Raises <a class="header-anchor" href="#raises-14" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If at any point along the path resulting from the target string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="half-self-self-source-3" tabindex="-1"><code>half(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#half-self-self-source-3" aria-label="Permalink to &quot;`half(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-86" tabindex="-1">Returns <a class="header-anchor" href="#returns-86" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source-3" tabindex="-1"><code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source-3" aria-label="Permalink to &quot;`load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436)&quot;">​</a></h4><p>Copy parameters and buffers from <code>state_dict</code> into this module and its descendants.</p><p>If <code>strict</code> is <code>True</code>, then the keys of <code>state_dict</code> must exactly match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function.</p><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>If <code>assign</code> is <code>True</code> the optimizer must be created after the call to <code>load_state_dict</code> unless <code>~tensorplay.__future__.get_swap_module_params_on_conversion</code> is <code>True</code>.</p></div><h4 id="args-104" tabindex="-1">Args <a class="header-anchor" href="#args-104" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state_dict</strong> (<code>dict</code>): a dict containing parameters and persistent buffers.</li><li><strong>strict</strong> (<code>bool, optional</code>): whether to strictly enforce that the keys in <code>state_dict</code> match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function. Default: <code>True</code></li><li><strong>assign</strong> (<code>bool, optional</code>): When set to <code>False</code>, the properties of the tensors in the current module are preserved whereas setting it to <code>True</code> preserves properties of the Tensors in the state dict. The only exception is the <code>requires_grad</code> field of <code>~tensorplay.nn.Parameter</code> for which the value from the module is preserved. Default: <code>False</code></li></ul><h4 id="returns-87" tabindex="-1">Returns <a class="header-anchor" href="#returns-87" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:
    * ``missing_keys`` is a list of str containing any keys that are expected
        by this module but missing from the provided ``state_dict``.
    * ``unexpected_keys`` is a list of str containing the keys that are not
        expected by this module but present in the provided ``state_dict``.
</code></pre><h4 id="note-9" tabindex="-1">Note <a class="header-anchor" href="#note-9" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>If a parameter or buffer is registered as ``None`` and its corresponding key
exists in `state_dict`, `load_state_dict` will raise a
``RuntimeError``.
</code></pre><hr><h4 id="modules-self-collections-abc-iterator-module-source-3" tabindex="-1"><code>modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#modules-self-collections-abc-iterator-module-source-3" aria-label="Permalink to &quot;`modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710)&quot;">​</a></h4><p>Return an iterator over all modules in the network.</p><h4 id="yields-26" tabindex="-1">Yields <a class="header-anchor" href="#yields-26" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a module in the network</li></ul><h4 id="note-10" tabindex="-1">Note <a class="header-anchor" href="#note-10" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-36" tabindex="-1">Example <a class="header-anchor" href="#example-36" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source-3" tabindex="-1"><code>named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source-3" aria-label="Permalink to &quot;`named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650)&quot;">​</a></h4><p>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p><h4 id="args-105" tabindex="-1">Args <a class="header-anchor" href="#args-105" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all buffer names.</li><li><strong>recurse</strong> (<code>bool, optional</code>): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Defaults to True.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated buffers in the result. Defaults to True.</li></ul><h4 id="yields-27" tabindex="-1">Yields <a class="header-anchor" href="#yields-27" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, tensorplay.Tensor): Tuple containing the name and buffer
</code></pre><h4 id="example-37" tabindex="-1">Example <a class="header-anchor" href="#example-37" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_buffers():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_var&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-children-self-collections-abc-iterator-tuple-str-module-source-3" tabindex="-1"><code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-children-self-collections-abc-iterator-tuple-str-module-source-3" aria-label="Permalink to &quot;`named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690)&quot;">​</a></h4><p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p><h4 id="yields-28" tabindex="-1">Yields <a class="header-anchor" href="#yields-28" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple containing a name and child module
</code></pre><h4 id="example-38" tabindex="-1">Example <a class="header-anchor" href="#example-38" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, module </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.named_children():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv4&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv5&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(module)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source-3" tabindex="-1"><code>named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source-3" aria-label="Permalink to &quot;`named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737)&quot;">​</a></h4><p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p><h4 id="args-106" tabindex="-1">Args <a class="header-anchor" href="#args-106" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>memo</strong>: a memo to store the set of modules already added to the result</li><li><strong>prefix</strong>: a prefix that will be added to the name of the module</li><li><strong>remove_duplicate</strong>: whether to remove the duplicated module instances in the result or not</li></ul><h4 id="yields-29" tabindex="-1">Yields <a class="header-anchor" href="#yields-29" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple of name and module
</code></pre><h4 id="note-11" tabindex="-1">Note <a class="header-anchor" href="#note-11" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-39" tabindex="-1">Example <a class="header-anchor" href="#example-39" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.named_modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;0&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source-3" tabindex="-1"><code>named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source-3" aria-label="Permalink to &quot;`named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595)&quot;">​</a></h4><p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p><h4 id="args-107" tabindex="-1">Args <a class="header-anchor" href="#args-107" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all parameter names.</li><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated parameters in the result. Defaults to True.</li></ul><h4 id="yields-30" tabindex="-1">Yields <a class="header-anchor" href="#yields-30" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Parameter): Tuple containing the name and parameter
</code></pre><h4 id="example-40" tabindex="-1">Example <a class="header-anchor" href="#example-40" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_parameters():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source-3" tabindex="-1"><code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source-3" aria-label="Permalink to &quot;`parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570)&quot;">​</a></h4><p>Return an iterator over module parameters.</p><p>This is typically passed to an optimizer.</p><h4 id="args-108" tabindex="-1">Args <a class="header-anchor" href="#args-108" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</li></ul><h4 id="yields-31" tabindex="-1">Yields <a class="header-anchor" href="#yields-31" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Parameter</strong>: module parameter</li></ul><h4 id="example-41" tabindex="-1">Example <a class="header-anchor" href="#example-41" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.parameters():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param), param.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source-3" tabindex="-1"><code>register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source-3" aria-label="Permalink to &quot;`register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>This function is deprecated in favor of <code>~tensorplay.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.</p><h4 id="returns-88" tabindex="-1">Returns <a class="header-anchor" href="#returns-88" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source-3" tabindex="-1"><code>register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source-3" aria-label="Permalink to &quot;`register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553)&quot;">​</a></h4><p>Add a buffer to the module.</p><p>This is typically used to register a buffer that should not be considered a model parameter. For example, BatchNorm&#39;s <code>running_mean</code> is not a parameter, but is part of the module&#39;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting <code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&#39;s <code>state_dict</code>.</p><p>Buffers can be accessed as attributes using given names.</p><h4 id="args-109" tabindex="-1">Args <a class="header-anchor" href="#args-109" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the buffer. The buffer can be accessed from this module using the given name</li><li><strong>tensor</strong> (<code>Tensor or None</code>): buffer to be registered. If <code>None</code>, then operations that run on buffers, such as <code>cuda</code>, are ignored. If <code>None</code>, the buffer is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li><li><strong>persistent</strong> (<code>bool</code>): whether the buffer is part of this module&#39;s <code>state_dict</code>.</li></ul><h4 id="example-42" tabindex="-1">Example <a class="header-anchor" href="#example-42" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.register_buffer(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_mean&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, tensorplay.zeros(num_features))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><hr><h4 id="register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source-3" tabindex="-1"><code>register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source-3" aria-label="Permalink to &quot;`register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592)&quot;">​</a></h4><p>Register a forward hook on the module.</p><p>The hook will be called every time after <code>forward</code> has computed an output.</p><p>If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after <code>forward</code> is called. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-110" tabindex="-1">Args <a class="header-anchor" href="#args-110" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If <code>True</code>, the provided <code>hook</code> will be fired before all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward</code> hooks registered with <code>register_module_forward_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If <code>True</code>, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>always_call</strong> (<code>bool</code>): If <code>True</code> the <code>hook</code> will be run regardless of whether an exception is raised while calling the Module.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-89" tabindex="-1">Returns <a class="header-anchor" href="#returns-89" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source-3" tabindex="-1"><code>register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source-3" aria-label="Permalink to &quot;`register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526)&quot;">​</a></h4><p>Register a forward pre-hook on the module.</p><p>The hook will be called every time before <code>forward</code> is invoked.</p><p>If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> of modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> and</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> kwargs</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-111" tabindex="-1">Args <a class="header-anchor" href="#args-111" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward_pre</code> hooks registered with <code>register_module_forward_pre_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If true, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-90" tabindex="-1">Returns <a class="header-anchor" href="#returns-90" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-3" tabindex="-1"><code>register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-3" aria-label="Permalink to &quot;`register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>The hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:</p><pre><code>1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.
2. If none of the module inputs require gradients, the hook will fire when the gradients are computed
   with respect to module outputs.
3. If none of the module outputs require gradients, then the hooks will not fire.
</code></pre><p>The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_input, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Tensor) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_input</code> and <code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of <code>grad_input</code> in subsequent computations. <code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in <code>grad_input</code> and <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> outputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-112" tabindex="-1">Args <a class="header-anchor" href="#args-112" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward</code> hooks registered with <code>register_module_full_backward_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-91" tabindex="-1">Returns <a class="header-anchor" href="#returns-91" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-3" tabindex="-1"><code>register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-3" aria-label="Permalink to &quot;`register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287)&quot;">​</a></h4><p>Register a backward pre-hook on the module.</p><p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tuple[Tensor] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of <code>grad_output</code> in subsequent computations. Entries in <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-113" tabindex="-1">Args <a class="header-anchor" href="#args-113" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward_pre</code> hooks registered with <code>register_module_full_backward_pre_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-92" tabindex="-1">Returns <a class="header-anchor" href="#returns-92" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-post-hook-self-hook-source-3" tabindex="-1"><code>register_load_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-post-hook-self-hook-source-3" aria-label="Permalink to &quot;`register_load_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226)&quot;">​</a></h4><p>Register a post-hook to be run after module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, incompatible_keys) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.</p><p>The given incompatible_keys can be modified inplace if needed.</p><p>Note that the checks performed when calling <code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.</p><h4 id="returns-93" tabindex="-1">Returns <a class="header-anchor" href="#returns-93" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-pre-hook-self-hook-source-3" tabindex="-1"><code>register_load_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-pre-hook-self-hook-source-3" aria-label="Permalink to &quot;`register_load_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214)&quot;">​</a></h4><p>Register a pre-hook to be run before module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # noqa: B950</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="arguments-3" tabindex="-1">Arguments <a class="header-anchor" href="#arguments-3" aria-label="Permalink to &quot;Arguments&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): Callable hook that will be invoked before loading the state dict.</li></ul><hr><h4 id="register-module-self-name-str-module-optional-forwardref-module-none-source-3" tabindex="-1"><code>register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-module-self-name-str-module-optional-forwardref-module-none-source-3" aria-label="Permalink to &quot;`register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695)&quot;">​</a></h4><p>Alias for <code>add_module</code>.</p><hr><h4 id="register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source-3" tabindex="-1"><code>register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source-3" aria-label="Permalink to &quot;`register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617)&quot;">​</a></h4><p>Add a parameter to the module.</p><p>The parameter can be accessed as an attribute using given name.</p><h4 id="args-114" tabindex="-1">Args <a class="header-anchor" href="#args-114" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the parameter. The parameter can be accessed from this module using the given name</li><li><strong>param</strong> (<code>Parameter or None</code>): parameter to be added to the module. If <code>None</code>, then operations that run on parameters, such as <code>cuda</code>, are ignored. If <code>None</code>, the parameter is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li></ul><hr><h4 id="register-state-dict-post-hook-self-hook-source-3" tabindex="-1"><code>register_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-post-hook-self-hook-source-3" aria-label="Permalink to &quot;`register_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017)&quot;">​</a></h4><p>Register a post-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can modify the <code>state_dict</code> inplace.</p><hr><h4 id="register-state-dict-pre-hook-self-hook-source-3" tabindex="-1"><code>register_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-pre-hook-self-hook-source-3" aria-label="Permalink to &quot;`register_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041)&quot;">​</a></h4><p>Register a pre-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, prefix, keep_vars) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p><hr><h4 id="requires-grad-self-requires-grad-bool-true-self-source-3" tabindex="-1"><code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#requires-grad-self-requires-grad-bool-true-self-source-3" aria-label="Permalink to &quot;`requires_grad_(self, requires_grad: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826)&quot;">​</a></h4><p>Change if autograd should record operations on parameters in this module.</p><p>This method sets the parameters&#39; <code>requires_grad</code> attributes in-place.</p><p>This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).</p><p>See <code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.</p><h4 id="args-115" tabindex="-1">Args <a class="header-anchor" href="#args-115" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>requires_grad</strong> (<code>bool</code>): whether autograd should record operations on parameters in this module. Default: <code>True</code>.</li></ul><h4 id="returns-94" tabindex="-1">Returns <a class="header-anchor" href="#returns-94" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="set-extra-state-self-state-any-none-source-3" tabindex="-1"><code>set_extra_state(self, state: Any) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-extra-state-self-state-any-none-source-3" aria-label="Permalink to &quot;`set_extra_state(self, state: Any) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938)&quot;">​</a></h4><p>Set extra state contained in the loaded <code>state_dict</code>.</p><p>This function is called from <code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding <code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.</p><h4 id="args-116" tabindex="-1">Args <a class="header-anchor" href="#args-116" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state</strong> (<code>dict</code>): Extra state from the <code>state_dict</code></li></ul><hr><h4 id="set-submodule-self-target-str-module-module-strict-bool-false-none-source-3" tabindex="-1"><code>set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-submodule-self-target-str-module-module-strict-bool-false-none-source-3" aria-label="Permalink to &quot;`set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764)&quot;">​</a></h4><p>Set the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>If <code>strict</code> is set to <code>False</code> (default), the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>strict</code> is set to <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule does not exist.</p></div><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(3, 3, 3) ) (linear): Linear(3, 3) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To override the <code>Conv2d</code> with a new submodule <code>Linear</code>, you could call <code>set_submodule(&quot;net_b.net_c.conv&quot;, nn.Linear(1, 1))</code> where <code>strict</code> could be <code>True</code> or <code>False</code></p><p>To add a new submodule <code>Conv2d</code> to the existing <code>net_b</code> module, you would call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1))</code>.</p><p>In the above if you set <code>strict=True</code> and call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1), strict=True)</code>, an AttributeError will be raised because <code>net_b</code> does not have a submodule named <code>conv</code>.</p><h4 id="args-117" tabindex="-1">Args <a class="header-anchor" href="#args-117" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li><li><strong>module</strong>: The module to set the submodule to.</li><li><strong>strict</strong>: If <code>False</code>, the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule doesn&#39;t already exist.</li></ul><h4 id="raises-15" tabindex="-1">Raises <a class="header-anchor" href="#raises-15" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>ValueError</strong>: If the <code>target</code> string is empty or if <code>module</code> is not an instance of <code>nn.Module</code>.</li><li><strong>AttributeError</strong>: If at any point along the path resulting from the <code>target</code> string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="share-memory-self-self-source-3" tabindex="-1"><code>share_memory(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#share-memory-self-self-source-3" aria-label="Permalink to &quot;`share_memory(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877)&quot;">​</a></h4><p>See <code>tensorplay.Tensor.share_memory_</code>.</p><hr><h4 id="state-dict-self-args-destination-none-prefix-keep-vars-false-source-3" tabindex="-1"><code>state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#state-dict-self-args-destination-none-prefix-keep-vars-false-source-3" aria-label="Permalink to &quot;`state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105)&quot;">​</a></h4><p>Return a dictionary containing references to the whole state of the module.</p><p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>The returned object is a shallow copy. It contains references to the module&#39;s parameters and buffers.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Currently <code>state_dict()</code> also accepts positional arguments for <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However, this is being deprecated and keyword arguments will be enforced in future releases.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Please avoid the use of argument <code>destination</code> as it is not designed for end-users.</p></div><h4 id="args-118" tabindex="-1">Args <a class="header-anchor" href="#args-118" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>destination</strong> (<code>dict, optional</code>): If provided, the state of module will be updated into the dict and the same object is returned. Otherwise, an <code>OrderedDict</code> will be created and returned.</li><li><strong>Default</strong>: <code>None</code>.</li><li><strong>prefix</strong> (<code>str, optional</code>): a prefix added to parameter and buffer names to compose the keys in state_dict. Default: <code>&#39;&#39;</code>.</li><li><strong>keep_vars</strong> (<code>bool, optional</code>): by default the <code>~tensorplay.Tensor</code> s returned in the state dict are detached from autograd. If it&#39;s set to <code>True</code>, detaching will not be performed.</li><li><strong>Default</strong>: <code>False</code>.</li></ul><h4 id="returns-95" tabindex="-1">Returns <a class="header-anchor" href="#returns-95" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>dict</strong>: a dictionary containing a whole state of the module</li></ul><h4 id="example-43" tabindex="-1">Example <a class="header-anchor" href="#example-43" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">module.state_dict().keys()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;weight&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><hr><h4 id="to-self-args-kwargs-source-3" tabindex="-1"><code>to(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-self-args-kwargs-source-3" aria-label="Permalink to &quot;`to(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151)&quot;">​</a></h4><p>Move and/or cast the parameters and buffers.</p><p>This can be called as</p><p>.. function:: to(device=None, dtype=None, non_blocking=False) :noindex:</p><p>.. function:: to(dtype, non_blocking=False) :noindex:</p><p>.. function:: to(tensor, non_blocking=False) :noindex:</p><p>.. function:: to(memory_format=tensorplay.channels_last) :noindex:</p><p>Its signature is similar to <code>tensorplay.Tensor.to</code>, but only accepts floating point or complex <code>dtype</code>\ s. In addition, this method will only cast the floating point or complex parameters and buffers to <code>dtype</code> (if given). The integral parameters and buffers will be moved <code>device</code>, if that is given, but with dtypes unchanged. When <code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.</p><p>See below for examples.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-119" tabindex="-1">Args <a class="header-anchor" href="#args-119" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): the desired device of the parameters and buffers in this module</li><li><strong>dtype</strong> (<code>tensorplay.dtype</code>): the desired floating point or complex dtype of the parameters and buffers in this module</li><li><strong>tensor</strong> (<code>tensorplay.Tensor</code>): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module</li><li><strong>memory_format</strong> (<code>tensorplay.memory_format</code>): the desired memory format for 4D parameters and buffers in this module (keyword only argument)</li></ul><h4 id="returns-96" tabindex="-1">Returns <a class="header-anchor" href="#returns-96" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="examples-3" tabindex="-1">Examples <a class="header-anchor" href="#examples-3" aria-label="Permalink to &quot;Examples&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +IGNORE_WANT(&quot;non-deterministic&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(tensorplay.double)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float64)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +REQUIRES(env:TENSORPLAY_DOCTEST_CUDA1)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">gpu1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cuda:1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(gpu1, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.half, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">non_blocking</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;cuda:1&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cpu </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cpu&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(cpu)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).to(tensorplay.cdouble)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3741</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,  </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2382</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5593</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.4443</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear(tensorplay.ones(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.cdouble))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br></div></div><hr><h4 id="to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source-3" tabindex="-1"><code>to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source-3" aria-label="Permalink to &quot;`to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119)&quot;">​</a></h4><p>Move the parameters and buffers to the specified device without copying storage.</p><h4 id="args-120" tabindex="-1">Args <a class="header-anchor" href="#args-120" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): The desired device of the parameters and buffers in this module.</li><li><strong>recurse</strong> (<code>bool</code>): Whether parameters and buffers of submodules should be recursively moved to the specified device.</li></ul><h4 id="returns-97" tabindex="-1">Returns <a class="header-anchor" href="#returns-97" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="train-self-mode-bool-true-self-source-3" tabindex="-1"><code>train(self, mode: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#train-self-mode-bool-true-self-source-3" aria-label="Permalink to &quot;`train(self, mode: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786)&quot;">​</a></h4><p>Set the module in training mode.</p><p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e., whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, etc.</p><h4 id="args-121" tabindex="-1">Args <a class="header-anchor" href="#args-121" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>mode</strong> (<code>bool</code>): whether to set training mode (<code>True</code>) or evaluation mode (<code>False</code>). Default: <code>True</code>.</li></ul><h4 id="returns-98" tabindex="-1">Returns <a class="header-anchor" href="#returns-98" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="type-self-dst-type-union-tensorplay-dtype-str-self-source-3" tabindex="-1"><code>type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#type-self-dst-type-union-tensorplay-dtype-str-self-source-3" aria-label="Permalink to &quot;`type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061)&quot;">​</a></h4><p>Casts all parameters and buffers to <code>dst_type</code>.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-122" tabindex="-1">Args <a class="header-anchor" href="#args-122" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>dst_type</strong> (<code>type or string</code>): the desired type</li></ul><h4 id="returns-99" tabindex="-1">Returns <a class="header-anchor" href="#returns-99" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="zero-grad-self-set-to-none-bool-true-none-source-3" tabindex="-1"><code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#zero-grad-self-set-to-none-bool-true-none-source-3" aria-label="Permalink to &quot;`zero_grad(self, set_to_none: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849)&quot;">​</a></h4><p>Reset gradients of all model parameters.</p><p>See similar function under <code>tensorplay.optim.Optimizer</code> for more context.</p><h4 id="args-123" tabindex="-1">Args <a class="header-anchor" href="#args-123" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>set_to_none</strong> (<code>bool</code>): instead of setting to zero, set the grads to None. See <code>tensorplay.optim.Optimizer.zero_grad</code> for details.</li></ul><hr></details><h3 id="class-sequential-source" tabindex="-1"><code>class Sequential</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L44" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-sequential-source" aria-label="Permalink to &quot;`class Sequential` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L44)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Sequential(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">args)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>Module</code></p><p>A sequential container.</p><p>Modules will be added to it in the order they are passed in the constructor. Alternatively, an <code>OrderedDict</code> of modules can be passed in. The <code>forward()</code> method of <code>Sequential</code> accepts any input and forwards it to the first module it contains. It then &quot;chains&quot; outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.</p><p>The value a <code>Sequential</code> provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the <code>Sequential</code> applies to each of the modules it stores (which are each a registered submodule of the <code>Sequential</code>).</p><p>What&#39;s the difference between a <code>Sequential</code> and a <code>tensorplay.nn.ModuleList</code>? A <code>ModuleList</code> is exactly what it sounds like--a list for storing <code>Module</code> s! On the other hand, the layers in a <code>Sequential</code> are connected in a cascading way.</p><h4 id="example-44" tabindex="-1">Example <a class="header-anchor" href="#example-44" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Using Sequential to create a small model. When `model` is run,</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># input will first be passed to `Conv2d(1,20,5)`. The output of</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># `Conv2d(1,20,5)` will be used as the input to the first</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># `ReLU`; the output of the first `ReLU` will become the input</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># for `Conv2d(20,64,5)`. Finally, the output of</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># `Conv2d(20,64,5)` will be used as input to the second `ReLU`</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    nn.Conv2d(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), nn.ReLU(), nn.Conv2d(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">64</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), nn.ReLU()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Using Sequential with OrderedDict. This is functionally the</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># same as the above code</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    OrderedDict(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;conv1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, nn.Conv2d(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)),</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;relu1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, nn.ReLU()),</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;conv2&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, nn.Conv2d(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">64</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)),</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;relu2&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, nn.ReLU()),</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        ]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    )</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br></div></div><details><summary>Methods</summary><h4 id="init-self-args-source" tabindex="-1"><code>__init__(self, *args)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L101" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-args-source" aria-label="Permalink to &quot;`__init__(self, *args)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L101)&quot;">​</a></h4><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p><hr><h4 id="add-module-self-name-str-module-optional-forwardref-module-none-source-4" tabindex="-1"><code>add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#add-module-self-name-str-module-optional-forwardref-module-none-source-4" aria-label="Permalink to &quot;`add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667)&quot;">​</a></h4><p>Add a child module to the current module.</p><p>The module can be accessed as an attribute using the given name.</p><h4 id="args-124" tabindex="-1">Args <a class="header-anchor" href="#args-124" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the child module. The child module can be accessed from this module using the given name</li><li><strong>module</strong> (<code>Module</code>): child module to be added to the module.</li></ul><hr><h4 id="append-self-module-module-self-source-1" tabindex="-1"><code>append(self, module: &#39;Module&#39;) -&gt; &#39;Self&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L230" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#append-self-module-module-self-source-1" aria-label="Permalink to &quot;`append(self, module: &#39;Module&#39;) -&gt; &#39;Self&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L230)&quot;">​</a></h4><p>Append a given module to the end.</p><h4 id="args-125" tabindex="-1">Args <a class="header-anchor" href="#args-125" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>module</strong> (<code>nn.Module</code>): module to append</li></ul><h4 id="example-45" tabindex="-1">Example <a class="header-anchor" href="#example-45" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.nn </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">n </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">n.append(nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><hr><h4 id="apply-self-fn-callable-forwardref-module-nonetype-self-source-4" tabindex="-1"><code>apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#apply-self-fn-callable-forwardref-module-nonetype-self-source-4" aria-label="Permalink to &quot;`apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990)&quot;">​</a></h4><p>Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self.</p><p>Typical use includes initializing the parameters of a model (see also <code>nn-init-doc</code>).</p><h4 id="args-126" tabindex="-1">Args <a class="header-anchor" href="#args-126" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>fn</strong> (``Module<code> -&gt; None</code>): function to be applied to each submodule</li></ul><h4 id="returns-100" tabindex="-1">Returns <a class="header-anchor" href="#returns-100" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="example-46" tabindex="-1">Example <a class="header-anchor" href="#example-46" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@tensorplay.no_grad</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> init_weights</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        m.weight.fill_(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m.weight)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net.apply(init_weights)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><hr><h4 id="bfloat16-self-self-source-4" tabindex="-1"><code>bfloat16(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#bfloat16-self-self-source-4" aria-label="Permalink to &quot;`bfloat16(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-101" tabindex="-1">Returns <a class="header-anchor" href="#returns-101" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source-4" tabindex="-1"><code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source-4" aria-label="Permalink to &quot;`buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627)&quot;">​</a></h4><p>Return an iterator over module buffers.</p><h4 id="args-127" tabindex="-1">Args <a class="header-anchor" href="#args-127" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.</li></ul><h4 id="yields-32" tabindex="-1">Yields <a class="header-anchor" href="#yields-32" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>tensorplay.Tensor: module buffer
</code></pre><h4 id="example-47" tabindex="-1">Example <a class="header-anchor" href="#example-47" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.buffers():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf), buf.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="children-self-collections-abc-iterator-module-source-4" tabindex="-1"><code>children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#children-self-collections-abc-iterator-module-source-4" aria-label="Permalink to &quot;`children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681)&quot;">​</a></h4><p>Return an iterator over immediate children modules.</p><h4 id="yields-33" tabindex="-1">Yields <a class="header-anchor" href="#yields-33" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a child module</li></ul><hr><h4 id="compile-self-args-kwargs-source-4" tabindex="-1"><code>compile(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#compile-self-args-kwargs-source-4" aria-label="Permalink to &quot;`compile(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944)&quot;">​</a></h4><p>Compile this Module&#39;s forward using <code>tensorplay.compile</code>.</p><p>This Module&#39;s <code>__call__</code> method is compiled and all arguments are passed as-is to <code>tensorplay.compile</code>.</p><p>See <code>tensorplay.compile</code> for details on the arguments for this function.</p><hr><h4 id="cpu-self-self-source-4" tabindex="-1"><code>cpu(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cpu-self-self-source-4" aria-label="Permalink to &quot;`cpu(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050)&quot;">​</a></h4><p>Move all model parameters and buffers to the CPU.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-102" tabindex="-1">Returns <a class="header-anchor" href="#returns-102" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source-4" tabindex="-1"><code>cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source-4" aria-label="Permalink to &quot;`cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031)&quot;">​</a></h4><p>Move all model parameters and buffers to the GPU.</p><p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on GPU while being optimized.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-128" tabindex="-1">Args <a class="header-anchor" href="#args-128" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>int, optional</code>): if specified, all parameters will be copied to that device</li></ul><h4 id="returns-103" tabindex="-1">Returns <a class="header-anchor" href="#returns-103" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="double-self-self-source-4" tabindex="-1"><code>double(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#double-self-self-source-4" aria-label="Permalink to &quot;`double(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-104" tabindex="-1">Returns <a class="header-anchor" href="#returns-104" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="eval-self-self-source-4" tabindex="-1"><code>eval(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#eval-self-self-source-4" aria-label="Permalink to &quot;`eval(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808)&quot;">​</a></h4><p>Set the module in evaluation mode.</p><p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e. whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, etc.</p><p>This is equivalent with <code>self.train(False) &lt;tensorplay.nn.Module.train&gt;</code>.</p><p>See <code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.</p><h4 id="returns-105" tabindex="-1">Returns <a class="header-anchor" href="#returns-105" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="extend-self-sequential-iterable-module-self-source" tabindex="-1"><code>extend(self, sequential: &#39;Iterable[Module]&#39;) -&gt; &#39;Self&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L283" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#extend-self-sequential-iterable-module-self-source" aria-label="Permalink to &quot;`extend(self, sequential: &#39;Iterable[Module]&#39;) -&gt; &#39;Self&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L283)&quot;">​</a></h4><p>Extends the current Sequential container with layers from another Sequential container.</p><h4 id="args-129" tabindex="-1">Args <a class="header-anchor" href="#args-129" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>sequential</strong> (<code>Sequential</code>): A Sequential container whose layers will be added to the current container.</li></ul><h4 id="example-48" tabindex="-1">Example <a class="header-anchor" href="#example-48" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.nn </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">n </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">other </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">n.extend(other) </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># or `n + other`</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="extra-repr-self-str-source-4" tabindex="-1"><code>extra_repr(self) -&gt; str</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2884" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#extra-repr-self-str-source-4" aria-label="Permalink to &quot;`extra_repr(self) -&gt; str` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2884)&quot;">​</a></h4><p>Return the extra representation of the module.</p><p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p><hr><h4 id="float-self-self-source-4" tabindex="-1"><code>float(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#float-self-self-source-4" aria-label="Permalink to &quot;`float(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-106" tabindex="-1">Returns <a class="header-anchor" href="#returns-106" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="forward-self-input-source" tabindex="-1"><code>forward(self, input)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L222" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#forward-self-input-source" aria-label="Permalink to &quot;`forward(self, input)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L222)&quot;">​</a></h4><p>Runs the forward pass.</p><hr><h4 id="get-buffer-self-target-str-tensor-source-4" tabindex="-1"><code>get_buffer(self, target: str) -&gt; &#39;Tensor&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-buffer-self-target-str-tensor-source-4" aria-label="Permalink to &quot;`get_buffer(self, target: str) -&gt; &#39;Tensor&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881)&quot;">​</a></h4><p>Return the buffer given by <code>target</code> if it exists, otherwise throw an error.</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-130" tabindex="-1">Args <a class="header-anchor" href="#args-130" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the buffer to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-107" tabindex="-1">Returns <a class="header-anchor" href="#returns-107" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.Tensor: The buffer referenced by ``target``
</code></pre><h4 id="raises-16" tabindex="-1">Raises <a class="header-anchor" href="#raises-16" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not a buffer</li></ul><hr><h4 id="get-extra-state-self-any-source-4" tabindex="-1"><code>get_extra_state(self) -&gt; Any</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-extra-state-self-any-source-4" aria-label="Permalink to &quot;`get_extra_state(self) -&gt; Any` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917)&quot;">​</a></h4><p>Return any extra state to include in the module&#39;s state_dict.</p><p>Implement this and a corresponding <code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module&#39;s <code>state_dict()</code>.</p><p>Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.</p><h4 id="returns-108" tabindex="-1">Returns <a class="header-anchor" href="#returns-108" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>object</strong>: Any extra state to store in the module&#39;s state_dict</li></ul><hr><h4 id="get-parameter-self-target-str-parameter-source-4" tabindex="-1"><code>get_parameter(self, target: str) -&gt; &#39;Parameter&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-parameter-self-target-str-parameter-source-4" aria-label="Permalink to &quot;`get_parameter(self, target: str) -&gt; &#39;Parameter&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845)&quot;">​</a></h4><p>Return the parameter given by <code>target</code> if it exists, otherwise throw an error.</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-131" tabindex="-1">Args <a class="header-anchor" href="#args-131" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the Parameter to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-109" tabindex="-1">Returns <a class="header-anchor" href="#returns-109" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Parameter: The Parameter referenced by ``target``
</code></pre><h4 id="raises-17" tabindex="-1">Raises <a class="header-anchor" href="#raises-17" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not an <code>nn.Parameter</code></li></ul><hr><h4 id="get-submodule-self-target-str-module-source-4" tabindex="-1"><code>get_submodule(self, target: str) -&gt; &#39;Module&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-submodule-self-target-str-module-source-4" aria-label="Permalink to &quot;`get_submodule(self, target: str) -&gt; &#39;Module&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699)&quot;">​</a></h4><p>Return the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> which has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(&quot;net_b.linear&quot;)</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(&quot;net_b.net_c.conv&quot;)</code>.</p><p>The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.</p><h4 id="args-132" tabindex="-1">Args <a class="header-anchor" href="#args-132" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li></ul><h4 id="returns-110" tabindex="-1">Returns <a class="header-anchor" href="#returns-110" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Module: The submodule referenced by ``target``
</code></pre><h4 id="raises-18" tabindex="-1">Raises <a class="header-anchor" href="#raises-18" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If at any point along the path resulting from the target string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="half-self-self-source-4" tabindex="-1"><code>half(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#half-self-self-source-4" aria-label="Permalink to &quot;`half(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-111" tabindex="-1">Returns <a class="header-anchor" href="#returns-111" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="insert-self-index-int-module-module-self-source" tabindex="-1"><code>insert(self, index: &#39;int&#39;, module: &#39;Module&#39;) -&gt; &#39;Self&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L251" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#insert-self-index-int-module-module-self-source" aria-label="Permalink to &quot;`insert(self, index: &#39;int&#39;, module: &#39;Module&#39;) -&gt; &#39;Self&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L251)&quot;">​</a></h4><p>Inserts a module into the Sequential container at the specified index.</p><h4 id="args-133" tabindex="-1">Args <a class="header-anchor" href="#args-133" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>index</strong> (<code>int</code>): The index to insert the module.</li><li><strong>module</strong> (<code>Module</code>): The module to be inserted.</li></ul><h4 id="example-49" tabindex="-1">Example <a class="header-anchor" href="#example-49" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.nn </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">n </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">n.insert(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><hr><h4 id="load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source-4" tabindex="-1"><code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source-4" aria-label="Permalink to &quot;`load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436)&quot;">​</a></h4><p>Copy parameters and buffers from <code>state_dict</code> into this module and its descendants.</p><p>If <code>strict</code> is <code>True</code>, then the keys of <code>state_dict</code> must exactly match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function.</p><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>If <code>assign</code> is <code>True</code> the optimizer must be created after the call to <code>load_state_dict</code> unless <code>~tensorplay.__future__.get_swap_module_params_on_conversion</code> is <code>True</code>.</p></div><h4 id="args-134" tabindex="-1">Args <a class="header-anchor" href="#args-134" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state_dict</strong> (<code>dict</code>): a dict containing parameters and persistent buffers.</li><li><strong>strict</strong> (<code>bool, optional</code>): whether to strictly enforce that the keys in <code>state_dict</code> match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function. Default: <code>True</code></li><li><strong>assign</strong> (<code>bool, optional</code>): When set to <code>False</code>, the properties of the tensors in the current module are preserved whereas setting it to <code>True</code> preserves properties of the Tensors in the state dict. The only exception is the <code>requires_grad</code> field of <code>~tensorplay.nn.Parameter</code> for which the value from the module is preserved. Default: <code>False</code></li></ul><h4 id="returns-112" tabindex="-1">Returns <a class="header-anchor" href="#returns-112" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:
    * ``missing_keys`` is a list of str containing any keys that are expected
        by this module but missing from the provided ``state_dict``.
    * ``unexpected_keys`` is a list of str containing the keys that are not
        expected by this module but present in the provided ``state_dict``.
</code></pre><h4 id="note-12" tabindex="-1">Note <a class="header-anchor" href="#note-12" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>If a parameter or buffer is registered as ``None`` and its corresponding key
exists in `state_dict`, `load_state_dict` will raise a
``RuntimeError``.
</code></pre><hr><h4 id="modules-self-collections-abc-iterator-module-source-4" tabindex="-1"><code>modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#modules-self-collections-abc-iterator-module-source-4" aria-label="Permalink to &quot;`modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710)&quot;">​</a></h4><p>Return an iterator over all modules in the network.</p><h4 id="yields-34" tabindex="-1">Yields <a class="header-anchor" href="#yields-34" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a module in the network</li></ul><h4 id="note-13" tabindex="-1">Note <a class="header-anchor" href="#note-13" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-50" tabindex="-1">Example <a class="header-anchor" href="#example-50" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source-4" tabindex="-1"><code>named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source-4" aria-label="Permalink to &quot;`named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650)&quot;">​</a></h4><p>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p><h4 id="args-135" tabindex="-1">Args <a class="header-anchor" href="#args-135" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all buffer names.</li><li><strong>recurse</strong> (<code>bool, optional</code>): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Defaults to True.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated buffers in the result. Defaults to True.</li></ul><h4 id="yields-35" tabindex="-1">Yields <a class="header-anchor" href="#yields-35" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, tensorplay.Tensor): Tuple containing the name and buffer
</code></pre><h4 id="example-51" tabindex="-1">Example <a class="header-anchor" href="#example-51" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_buffers():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_var&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-children-self-collections-abc-iterator-tuple-str-module-source-4" tabindex="-1"><code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-children-self-collections-abc-iterator-tuple-str-module-source-4" aria-label="Permalink to &quot;`named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690)&quot;">​</a></h4><p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p><h4 id="yields-36" tabindex="-1">Yields <a class="header-anchor" href="#yields-36" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple containing a name and child module
</code></pre><h4 id="example-52" tabindex="-1">Example <a class="header-anchor" href="#example-52" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, module </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.named_children():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv4&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv5&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(module)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source-4" tabindex="-1"><code>named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source-4" aria-label="Permalink to &quot;`named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737)&quot;">​</a></h4><p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p><h4 id="args-136" tabindex="-1">Args <a class="header-anchor" href="#args-136" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>memo</strong>: a memo to store the set of modules already added to the result</li><li><strong>prefix</strong>: a prefix that will be added to the name of the module</li><li><strong>remove_duplicate</strong>: whether to remove the duplicated module instances in the result or not</li></ul><h4 id="yields-37" tabindex="-1">Yields <a class="header-anchor" href="#yields-37" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple of name and module
</code></pre><h4 id="note-14" tabindex="-1">Note <a class="header-anchor" href="#note-14" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-53" tabindex="-1">Example <a class="header-anchor" href="#example-53" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.named_modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;0&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source-4" tabindex="-1"><code>named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source-4" aria-label="Permalink to &quot;`named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595)&quot;">​</a></h4><p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p><h4 id="args-137" tabindex="-1">Args <a class="header-anchor" href="#args-137" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all parameter names.</li><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated parameters in the result. Defaults to True.</li></ul><h4 id="yields-38" tabindex="-1">Yields <a class="header-anchor" href="#yields-38" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Parameter): Tuple containing the name and parameter
</code></pre><h4 id="example-54" tabindex="-1">Example <a class="header-anchor" href="#example-54" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_parameters():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source-4" tabindex="-1"><code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source-4" aria-label="Permalink to &quot;`parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570)&quot;">​</a></h4><p>Return an iterator over module parameters.</p><p>This is typically passed to an optimizer.</p><h4 id="args-138" tabindex="-1">Args <a class="header-anchor" href="#args-138" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</li></ul><h4 id="yields-39" tabindex="-1">Yields <a class="header-anchor" href="#yields-39" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Parameter</strong>: module parameter</li></ul><h4 id="example-55" tabindex="-1">Example <a class="header-anchor" href="#example-55" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.parameters():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param), param.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="pop-self-key-union-int-slice-module-source-1" tabindex="-1"><code>pop(self, key: &#39;Union[int, slice]&#39;) -&gt; &#39;Module&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L110" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#pop-self-key-union-int-slice-module-source-1" aria-label="Permalink to &quot;`pop(self, key: &#39;Union[int, slice]&#39;) -&gt; &#39;Module&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/container.py#L110)&quot;">​</a></h4><p>Pop <code>key</code> from self.</p><hr><h4 id="register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source-4" tabindex="-1"><code>register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source-4" aria-label="Permalink to &quot;`register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>This function is deprecated in favor of <code>~tensorplay.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.</p><h4 id="returns-113" tabindex="-1">Returns <a class="header-anchor" href="#returns-113" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source-4" tabindex="-1"><code>register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source-4" aria-label="Permalink to &quot;`register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553)&quot;">​</a></h4><p>Add a buffer to the module.</p><p>This is typically used to register a buffer that should not be considered a model parameter. For example, BatchNorm&#39;s <code>running_mean</code> is not a parameter, but is part of the module&#39;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting <code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&#39;s <code>state_dict</code>.</p><p>Buffers can be accessed as attributes using given names.</p><h4 id="args-139" tabindex="-1">Args <a class="header-anchor" href="#args-139" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the buffer. The buffer can be accessed from this module using the given name</li><li><strong>tensor</strong> (<code>Tensor or None</code>): buffer to be registered. If <code>None</code>, then operations that run on buffers, such as <code>cuda</code>, are ignored. If <code>None</code>, the buffer is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li><li><strong>persistent</strong> (<code>bool</code>): whether the buffer is part of this module&#39;s <code>state_dict</code>.</li></ul><h4 id="example-56" tabindex="-1">Example <a class="header-anchor" href="#example-56" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.register_buffer(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_mean&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, tensorplay.zeros(num_features))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><hr><h4 id="register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source-4" tabindex="-1"><code>register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source-4" aria-label="Permalink to &quot;`register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592)&quot;">​</a></h4><p>Register a forward hook on the module.</p><p>The hook will be called every time after <code>forward</code> has computed an output.</p><p>If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after <code>forward</code> is called. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-140" tabindex="-1">Args <a class="header-anchor" href="#args-140" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If <code>True</code>, the provided <code>hook</code> will be fired before all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward</code> hooks registered with <code>register_module_forward_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If <code>True</code>, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>always_call</strong> (<code>bool</code>): If <code>True</code> the <code>hook</code> will be run regardless of whether an exception is raised while calling the Module.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-114" tabindex="-1">Returns <a class="header-anchor" href="#returns-114" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source-4" tabindex="-1"><code>register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source-4" aria-label="Permalink to &quot;`register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526)&quot;">​</a></h4><p>Register a forward pre-hook on the module.</p><p>The hook will be called every time before <code>forward</code> is invoked.</p><p>If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> of modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> and</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> kwargs</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-141" tabindex="-1">Args <a class="header-anchor" href="#args-141" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward_pre</code> hooks registered with <code>register_module_forward_pre_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If true, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-115" tabindex="-1">Returns <a class="header-anchor" href="#returns-115" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-4" tabindex="-1"><code>register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-4" aria-label="Permalink to &quot;`register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>The hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:</p><pre><code>1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.
2. If none of the module inputs require gradients, the hook will fire when the gradients are computed
   with respect to module outputs.
3. If none of the module outputs require gradients, then the hooks will not fire.
</code></pre><p>The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_input, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Tensor) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_input</code> and <code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of <code>grad_input</code> in subsequent computations. <code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in <code>grad_input</code> and <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> outputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-142" tabindex="-1">Args <a class="header-anchor" href="#args-142" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward</code> hooks registered with <code>register_module_full_backward_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-116" tabindex="-1">Returns <a class="header-anchor" href="#returns-116" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-4" tabindex="-1"><code>register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-4" aria-label="Permalink to &quot;`register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287)&quot;">​</a></h4><p>Register a backward pre-hook on the module.</p><p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tuple[Tensor] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of <code>grad_output</code> in subsequent computations. Entries in <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-143" tabindex="-1">Args <a class="header-anchor" href="#args-143" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward_pre</code> hooks registered with <code>register_module_full_backward_pre_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-117" tabindex="-1">Returns <a class="header-anchor" href="#returns-117" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-post-hook-self-hook-source-4" tabindex="-1"><code>register_load_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-post-hook-self-hook-source-4" aria-label="Permalink to &quot;`register_load_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226)&quot;">​</a></h4><p>Register a post-hook to be run after module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, incompatible_keys) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.</p><p>The given incompatible_keys can be modified inplace if needed.</p><p>Note that the checks performed when calling <code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.</p><h4 id="returns-118" tabindex="-1">Returns <a class="header-anchor" href="#returns-118" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-pre-hook-self-hook-source-4" tabindex="-1"><code>register_load_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-pre-hook-self-hook-source-4" aria-label="Permalink to &quot;`register_load_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214)&quot;">​</a></h4><p>Register a pre-hook to be run before module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # noqa: B950</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="arguments-4" tabindex="-1">Arguments <a class="header-anchor" href="#arguments-4" aria-label="Permalink to &quot;Arguments&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): Callable hook that will be invoked before loading the state dict.</li></ul><hr><h4 id="register-module-self-name-str-module-optional-forwardref-module-none-source-4" tabindex="-1"><code>register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-module-self-name-str-module-optional-forwardref-module-none-source-4" aria-label="Permalink to &quot;`register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695)&quot;">​</a></h4><p>Alias for <code>add_module</code>.</p><hr><h4 id="register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source-4" tabindex="-1"><code>register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source-4" aria-label="Permalink to &quot;`register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617)&quot;">​</a></h4><p>Add a parameter to the module.</p><p>The parameter can be accessed as an attribute using given name.</p><h4 id="args-144" tabindex="-1">Args <a class="header-anchor" href="#args-144" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the parameter. The parameter can be accessed from this module using the given name</li><li><strong>param</strong> (<code>Parameter or None</code>): parameter to be added to the module. If <code>None</code>, then operations that run on parameters, such as <code>cuda</code>, are ignored. If <code>None</code>, the parameter is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li></ul><hr><h4 id="register-state-dict-post-hook-self-hook-source-4" tabindex="-1"><code>register_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-post-hook-self-hook-source-4" aria-label="Permalink to &quot;`register_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017)&quot;">​</a></h4><p>Register a post-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can modify the <code>state_dict</code> inplace.</p><hr><h4 id="register-state-dict-pre-hook-self-hook-source-4" tabindex="-1"><code>register_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-pre-hook-self-hook-source-4" aria-label="Permalink to &quot;`register_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041)&quot;">​</a></h4><p>Register a pre-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, prefix, keep_vars) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p><hr><h4 id="requires-grad-self-requires-grad-bool-true-self-source-4" tabindex="-1"><code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#requires-grad-self-requires-grad-bool-true-self-source-4" aria-label="Permalink to &quot;`requires_grad_(self, requires_grad: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826)&quot;">​</a></h4><p>Change if autograd should record operations on parameters in this module.</p><p>This method sets the parameters&#39; <code>requires_grad</code> attributes in-place.</p><p>This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).</p><p>See <code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.</p><h4 id="args-145" tabindex="-1">Args <a class="header-anchor" href="#args-145" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>requires_grad</strong> (<code>bool</code>): whether autograd should record operations on parameters in this module. Default: <code>True</code>.</li></ul><h4 id="returns-119" tabindex="-1">Returns <a class="header-anchor" href="#returns-119" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="set-extra-state-self-state-any-none-source-4" tabindex="-1"><code>set_extra_state(self, state: Any) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-extra-state-self-state-any-none-source-4" aria-label="Permalink to &quot;`set_extra_state(self, state: Any) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938)&quot;">​</a></h4><p>Set extra state contained in the loaded <code>state_dict</code>.</p><p>This function is called from <code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding <code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.</p><h4 id="args-146" tabindex="-1">Args <a class="header-anchor" href="#args-146" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state</strong> (<code>dict</code>): Extra state from the <code>state_dict</code></li></ul><hr><h4 id="set-submodule-self-target-str-module-module-strict-bool-false-none-source-4" tabindex="-1"><code>set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-submodule-self-target-str-module-module-strict-bool-false-none-source-4" aria-label="Permalink to &quot;`set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764)&quot;">​</a></h4><p>Set the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>If <code>strict</code> is set to <code>False</code> (default), the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>strict</code> is set to <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule does not exist.</p></div><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(3, 3, 3) ) (linear): Linear(3, 3) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To override the <code>Conv2d</code> with a new submodule <code>Linear</code>, you could call <code>set_submodule(&quot;net_b.net_c.conv&quot;, nn.Linear(1, 1))</code> where <code>strict</code> could be <code>True</code> or <code>False</code></p><p>To add a new submodule <code>Conv2d</code> to the existing <code>net_b</code> module, you would call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1))</code>.</p><p>In the above if you set <code>strict=True</code> and call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1), strict=True)</code>, an AttributeError will be raised because <code>net_b</code> does not have a submodule named <code>conv</code>.</p><h4 id="args-147" tabindex="-1">Args <a class="header-anchor" href="#args-147" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li><li><strong>module</strong>: The module to set the submodule to.</li><li><strong>strict</strong>: If <code>False</code>, the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule doesn&#39;t already exist.</li></ul><h4 id="raises-19" tabindex="-1">Raises <a class="header-anchor" href="#raises-19" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>ValueError</strong>: If the <code>target</code> string is empty or if <code>module</code> is not an instance of <code>nn.Module</code>.</li><li><strong>AttributeError</strong>: If at any point along the path resulting from the <code>target</code> string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="share-memory-self-self-source-4" tabindex="-1"><code>share_memory(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#share-memory-self-self-source-4" aria-label="Permalink to &quot;`share_memory(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877)&quot;">​</a></h4><p>See <code>tensorplay.Tensor.share_memory_</code>.</p><hr><h4 id="state-dict-self-args-destination-none-prefix-keep-vars-false-source-4" tabindex="-1"><code>state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#state-dict-self-args-destination-none-prefix-keep-vars-false-source-4" aria-label="Permalink to &quot;`state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105)&quot;">​</a></h4><p>Return a dictionary containing references to the whole state of the module.</p><p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>The returned object is a shallow copy. It contains references to the module&#39;s parameters and buffers.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Currently <code>state_dict()</code> also accepts positional arguments for <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However, this is being deprecated and keyword arguments will be enforced in future releases.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Please avoid the use of argument <code>destination</code> as it is not designed for end-users.</p></div><h4 id="args-148" tabindex="-1">Args <a class="header-anchor" href="#args-148" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>destination</strong> (<code>dict, optional</code>): If provided, the state of module will be updated into the dict and the same object is returned. Otherwise, an <code>OrderedDict</code> will be created and returned.</li><li><strong>Default</strong>: <code>None</code>.</li><li><strong>prefix</strong> (<code>str, optional</code>): a prefix added to parameter and buffer names to compose the keys in state_dict. Default: <code>&#39;&#39;</code>.</li><li><strong>keep_vars</strong> (<code>bool, optional</code>): by default the <code>~tensorplay.Tensor</code> s returned in the state dict are detached from autograd. If it&#39;s set to <code>True</code>, detaching will not be performed.</li><li><strong>Default</strong>: <code>False</code>.</li></ul><h4 id="returns-120" tabindex="-1">Returns <a class="header-anchor" href="#returns-120" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>dict</strong>: a dictionary containing a whole state of the module</li></ul><h4 id="example-57" tabindex="-1">Example <a class="header-anchor" href="#example-57" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">module.state_dict().keys()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;weight&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><hr><h4 id="to-self-args-kwargs-source-4" tabindex="-1"><code>to(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-self-args-kwargs-source-4" aria-label="Permalink to &quot;`to(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151)&quot;">​</a></h4><p>Move and/or cast the parameters and buffers.</p><p>This can be called as</p><p>.. function:: to(device=None, dtype=None, non_blocking=False) :noindex:</p><p>.. function:: to(dtype, non_blocking=False) :noindex:</p><p>.. function:: to(tensor, non_blocking=False) :noindex:</p><p>.. function:: to(memory_format=tensorplay.channels_last) :noindex:</p><p>Its signature is similar to <code>tensorplay.Tensor.to</code>, but only accepts floating point or complex <code>dtype</code>\ s. In addition, this method will only cast the floating point or complex parameters and buffers to <code>dtype</code> (if given). The integral parameters and buffers will be moved <code>device</code>, if that is given, but with dtypes unchanged. When <code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.</p><p>See below for examples.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-149" tabindex="-1">Args <a class="header-anchor" href="#args-149" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): the desired device of the parameters and buffers in this module</li><li><strong>dtype</strong> (<code>tensorplay.dtype</code>): the desired floating point or complex dtype of the parameters and buffers in this module</li><li><strong>tensor</strong> (<code>tensorplay.Tensor</code>): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module</li><li><strong>memory_format</strong> (<code>tensorplay.memory_format</code>): the desired memory format for 4D parameters and buffers in this module (keyword only argument)</li></ul><h4 id="returns-121" tabindex="-1">Returns <a class="header-anchor" href="#returns-121" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="examples-4" tabindex="-1">Examples <a class="header-anchor" href="#examples-4" aria-label="Permalink to &quot;Examples&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +IGNORE_WANT(&quot;non-deterministic&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(tensorplay.double)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float64)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +REQUIRES(env:TENSORPLAY_DOCTEST_CUDA1)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">gpu1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cuda:1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(gpu1, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.half, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">non_blocking</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;cuda:1&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cpu </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cpu&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(cpu)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).to(tensorplay.cdouble)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3741</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,  </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2382</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5593</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.4443</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear(tensorplay.ones(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.cdouble))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br></div></div><hr><h4 id="to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source-4" tabindex="-1"><code>to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source-4" aria-label="Permalink to &quot;`to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119)&quot;">​</a></h4><p>Move the parameters and buffers to the specified device without copying storage.</p><h4 id="args-150" tabindex="-1">Args <a class="header-anchor" href="#args-150" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): The desired device of the parameters and buffers in this module.</li><li><strong>recurse</strong> (<code>bool</code>): Whether parameters and buffers of submodules should be recursively moved to the specified device.</li></ul><h4 id="returns-122" tabindex="-1">Returns <a class="header-anchor" href="#returns-122" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="train-self-mode-bool-true-self-source-4" tabindex="-1"><code>train(self, mode: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#train-self-mode-bool-true-self-source-4" aria-label="Permalink to &quot;`train(self, mode: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786)&quot;">​</a></h4><p>Set the module in training mode.</p><p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e., whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, etc.</p><h4 id="args-151" tabindex="-1">Args <a class="header-anchor" href="#args-151" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>mode</strong> (<code>bool</code>): whether to set training mode (<code>True</code>) or evaluation mode (<code>False</code>). Default: <code>True</code>.</li></ul><h4 id="returns-123" tabindex="-1">Returns <a class="header-anchor" href="#returns-123" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="type-self-dst-type-union-tensorplay-dtype-str-self-source-4" tabindex="-1"><code>type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#type-self-dst-type-union-tensorplay-dtype-str-self-source-4" aria-label="Permalink to &quot;`type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061)&quot;">​</a></h4><p>Casts all parameters and buffers to <code>dst_type</code>.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-152" tabindex="-1">Args <a class="header-anchor" href="#args-152" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>dst_type</strong> (<code>type or string</code>): the desired type</li></ul><h4 id="returns-124" tabindex="-1">Returns <a class="header-anchor" href="#returns-124" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="zero-grad-self-set-to-none-bool-true-none-source-4" tabindex="-1"><code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#zero-grad-self-set-to-none-bool-true-none-source-4" aria-label="Permalink to &quot;`zero_grad(self, set_to_none: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849)&quot;">​</a></h4><p>Reset gradients of all model parameters.</p><p>See similar function under <code>tensorplay.optim.Optimizer</code> for more context.</p><h4 id="args-153" tabindex="-1">Args <a class="header-anchor" href="#args-153" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>set_to_none</strong> (<code>bool</code>): instead of setting to zero, set the grads to None. See <code>tensorplay.optim.Optimizer.zero_grad</code> for details.</li></ul><hr></details></div></div></main><footer class="VPDocFooter" data-v-39a288b8 data-v-e257564d><!--[--><!--]--><!----><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-e257564d><span class="visually-hidden" id="doc-footer-aria-label" data-v-e257564d>Pager</span><div class="pager" data-v-e257564d><a class="VPLink link pager-link prev" href="/zh/api/dropout.html" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>Previous page</span><span class="title" data-v-e257564d>Dropout Layers</span><!--]--></a></div><div class="pager" data-v-e257564d><!----></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-5d98c3a5 data-v-e315a0ad><div class="container" data-v-e315a0ad><p class="message" data-v-e315a0ad>基于 Apache 2.0 许可发布。</p><p class="copyright" data-v-e315a0ad>版权所有 © 2025 zlx。保留所有权利。</p></div></footer><!--[--><a href="https://deepwiki.com/bluemoon-o2/TensorPlay" target="_blank" rel="noopener noreferrer" class="deepwiki-floating-badge" title="View on DeepWiki" data-v-5bf2a798><div class="badge-content" data-v-5bf2a798><span class="badge-icon" data-v-5bf2a798>📚</span><span class="badge-text" data-v-5bf2a798>DeepWiki</span></div></a><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"about_index.md\":\"CivpnJLS\",\"api__c.md\":\"C_J0EWHA\",\"api__reduction.md\":\"CcQTGa52\",\"api_activation.md\":\"BGBcfbA_\",\"api_adagrad.md\":\"Duucap4L\",\"api_adam.md\":\"B6F6HfPc\",\"api_adamw.md\":\"BRkR1iuY\",\"api_alexnet.md\":\"DY6a19PR\",\"api_amp.md\":\"C-bU3qHi\",\"api_audio.md\":\"COxDUPkl\",\"api_autocast_mode.md\":\"0iN_nBmE\",\"api_autograd.md\":\"9kpZfXtl\",\"api_backend.md\":\"CXUhZEyb\",\"api_backends.md\":\"B_bG74Gi\",\"api_batchnorm.md\":\"XLqeoBPh\",\"api_collate.md\":\"D4ONEdYq\",\"api_common_types.md\":\"Dh75-e2Y\",\"api_comparison.md\":\"COlCxpxH\",\"api_container.md\":\"DMH3KEiI\",\"api_conv.md\":\"DF0GFaql\",\"api_cpp.md\":\"B9E_vT7t\",\"api_cpu.md\":\"BOjn8BaM\",\"api_cuda.md\":\"v6ikcKwC\",\"api_data.md\":\"B7m_AC2B\",\"api_dataloader.md\":\"C057K852\",\"api_dataset.md\":\"B-DwApTM\",\"api_datasets.md\":\"i2152NFq\",\"api_dropout.md\":\"DzMMlVs2\",\"api_flatten.md\":\"D4zA_qjP\",\"api_folder.md\":\"DaSoqjRC\",\"api_function.md\":\"nCZMqEoD\",\"api_functional.md\":\"DRB1e583\",\"api_grad_mode.md\":\"CwdRNt1O\",\"api_grad_scaler.md\":\"Cx6Iula5\",\"api_hooks.md\":\"BIKb4iGy\",\"api_hub.md\":\"BlxbfAE8\",\"api_index.md\":\"CU1NhOLw\",\"api_init.md\":\"yxiO6Ie6\",\"api_instancenorm.md\":\"CyhqGI7E\",\"api_io.md\":\"D5L0xo2J\",\"api_lazy.md\":\"C1xShkoe\",\"api_linear.md\":\"D0-JMnEM\",\"api_loss.md\":\"BJQVFhNb\",\"api_lr_scheduler.md\":\"qKIcmL6f\",\"api_mkl.md\":\"CGkKnAIU\",\"api_mkldnn.md\":\"DLcmdCYf\",\"api_mnist.md\":\"DvSH5G-2\",\"api_models.md\":\"PxIjWRF1\",\"api_module.md\":\"CnrRone4\",\"api_modules.md\":\"yK73B9y6\",\"api_multiprocessing.md\":\"DrisgWAY\",\"api_nn.md\":\"4XkbCLIj\",\"api_normalization.md\":\"D0JAaa1R\",\"api_onnx.md\":\"maU2_GEX\",\"api_openmp.md\":\"BCYWCTil\",\"api_ops.md\":\"BMv0aklF\",\"api_optim.md\":\"CaxPVV4U\",\"api_optimizer.md\":\"TLuDU7KR\",\"api_parameter.md\":\"BQlUd1Rx\",\"api_pooling.md\":\"Csm8n1fJ\",\"api_primitives.md\":\"h9kmOGWa\",\"api_rmsprop.md\":\"K8IflYqt\",\"api_sampler.md\":\"Docoa_mD\",\"api_serialization.md\":\"Yz3Vj80d\",\"api_sgd.md\":\"CvX_KTaM\",\"api_sparse.md\":\"BREaz85i\",\"api_stax.md\":\"XChG2EZZ\",\"api_tensorplay.md\":\"DmrkfdPn\",\"api_transforms.md\":\"Be1af3uD\",\"api_triton.md\":\"JKz49eZ7\",\"api_types.md\":\"943V7hWH\",\"api_utils.md\":\"DVKjBrp2\",\"api_vision.md\":\"zbCijo_o\",\"api_viz.md\":\"CiZx1vUZ\",\"api_worker.md\":\"Bz9t8sLd\",\"blog_index.md\":\"B2h2By8z\",\"blog_posts_deep-dive-p10-dispatcher.md\":\"DFKzJbLT\",\"blog_posts_tensorplay-architecture-design.md\":\"jfjrNyAD\",\"blog_posts_tpx-autograd-decoupling.md\":\"BvCyyeZP\",\"changelog.md\":\"BUIkw7Qb\",\"community_index.md\":\"ClpoEdVU\",\"contributing.md\":\"D8ZfsEom\",\"ecosystem_index.md\":\"gtvnBFE0\",\"guide_architecture.md\":\"6oaLgXdc\",\"guide_getting-started.md\":\"Buqy5xfl\",\"guide_install.md\":\"CMrQwZ6d\",\"guide_quickstart.md\":\"B6_UpAzb\",\"guide_resources.md\":\"BV5U6Os2\",\"guide_tutorials.md\":\"BZfLWtiC\",\"guide_tutorials_cnn-classification.md\":\"gUJGnFt_\",\"guide_tutorials_custom-autograd.md\":\"sQU_lsig\",\"guide_tutorials_linear-regression.md\":\"DgenOgcp\",\"guide_what-is-tensorplay.md\":\"Cr9LJ0YI\",\"index.md\":\"nsvbk11h\",\"join_index.md\":\"DEJ6DZZ0\",\"privacy.md\":\"DEhyX8TP\",\"zh_about_index.md\":\"C0qMuj3_\",\"zh_api__c.md\":\"CSuKrWkC\",\"zh_api__reduction.md\":\"B3daZSCA\",\"zh_api_activation.md\":\"CZMJ9jbW\",\"zh_api_adagrad.md\":\"DHu6ziwz\",\"zh_api_adam.md\":\"B0FY4EOy\",\"zh_api_adamw.md\":\"CWrIOxtJ\",\"zh_api_alexnet.md\":\"DFqPETqU\",\"zh_api_amp.md\":\"Cp6DUBtV\",\"zh_api_audio.md\":\"DzU_oAU1\",\"zh_api_autocast_mode.md\":\"C5QatGaz\",\"zh_api_autograd.md\":\"C3rUrDnv\",\"zh_api_backend.md\":\"BvUHlOkJ\",\"zh_api_backends.md\":\"D5m5NMmh\",\"zh_api_batchnorm.md\":\"Bo4Pl8ij\",\"zh_api_collate.md\":\"C9yeRKMz\",\"zh_api_common_types.md\":\"51LRPJtH\",\"zh_api_comparison.md\":\"CmoYjd2x\",\"zh_api_container.md\":\"ecpuGL9c\",\"zh_api_conv.md\":\"CCAFGFF2\",\"zh_api_cpp.md\":\"C_Qv-gh-\",\"zh_api_cuda.md\":\"Ch-0DTcR\",\"zh_api_data.md\":\"D8Ewk7N7\",\"zh_api_dataloader.md\":\"CcmiB811\",\"zh_api_dataset.md\":\"D-cegnxU\",\"zh_api_datasets.md\":\"DYHjDpEe\",\"zh_api_dropout.md\":\"B4KevHXm\",\"zh_api_flatten.md\":\"DEAvbacp\",\"zh_api_folder.md\":\"Cgxw6iWX\",\"zh_api_function.md\":\"BJWjNfM0\",\"zh_api_functional.md\":\"DQbKL0xC\",\"zh_api_grad_mode.md\":\"BFeDCfd7\",\"zh_api_grad_scaler.md\":\"DCTY_-th\",\"zh_api_hooks.md\":\"3iIg6lb4\",\"zh_api_hub.md\":\"CEDAtQ6E\",\"zh_api_index.md\":\"CbRR1k25\",\"zh_api_init.md\":\"BqQy98Zn\",\"zh_api_instancenorm.md\":\"LzLgKUnO\",\"zh_api_io.md\":\"D2S8h9kl\",\"zh_api_lazy.md\":\"aFzKlDTe\",\"zh_api_linear.md\":\"CNWJcDWd\",\"zh_api_loss.md\":\"CJx9AuDU\",\"zh_api_lr_scheduler.md\":\"CEmO09Z4\",\"zh_api_mkl.md\":\"CJb3speK\",\"zh_api_mkldnn.md\":\"IkiVLyDS\",\"zh_api_mnist.md\":\"B9ADoHZ5\",\"zh_api_models.md\":\"g3-O97YN\",\"zh_api_module.md\":\"BPDJ-1Kk\",\"zh_api_modules.md\":\"Cgu9RPlU\",\"zh_api_multiprocessing.md\":\"G2hJzSLM\",\"zh_api_nn.md\":\"DT072hnu\",\"zh_api_normalization.md\":\"CMzsTRlT\",\"zh_api_onnx.md\":\"CjhD6BJR\",\"zh_api_openmp.md\":\"D9v_3MHS\",\"zh_api_ops.md\":\"BkisPtY5\",\"zh_api_optim.md\":\"CP5UB3gJ\",\"zh_api_optimizer.md\":\"CPOByaaw\",\"zh_api_parameter.md\":\"XiiS9CGF\",\"zh_api_pooling.md\":\"D950IsK0\",\"zh_api_primitives.md\":\"Dd3Dk8aE\",\"zh_api_rmsprop.md\":\"CPe7ryaO\",\"zh_api_sampler.md\":\"Bfhil5Ns\",\"zh_api_serialization.md\":\"Cmp7efEt\",\"zh_api_sgd.md\":\"C5LAFD4S\",\"zh_api_sparse.md\":\"CARPRVJP\",\"zh_api_stax.md\":\"B38hG-b1\",\"zh_api_tensorplay.md\":\"CPudKJkw\",\"zh_api_transforms.md\":\"CqQu24bY\",\"zh_api_types.md\":\"CWZNrGJb\",\"zh_api_utils.md\":\"B2MXTLh6\",\"zh_api_vision.md\":\"nVKKiAhu\",\"zh_api_viz.md\":\"CgkacQOO\",\"zh_api_worker.md\":\"Ca4fmC1_\",\"zh_blog_index.md\":\"K40rMw4R\",\"zh_blog_posts_deep-dive-p10-dispatcher.md\":\"DOKalT1C\",\"zh_blog_posts_tensorplay-architecture-design.md\":\"DSwajTlo\",\"zh_blog_posts_tpx-autograd-decoupling.md\":\"D-SG-adq\",\"zh_changelog.md\":\"DWHyZqpt\",\"zh_community_index.md\":\"eV1HDn5F\",\"zh_contributing.md\":\"Ba70JOpw\",\"zh_ecosystem_index.md\":\"CtS2mywe\",\"zh_guide_architecture.md\":\"C-yDD8tS\",\"zh_guide_getting-started.md\":\"yqgE04Td\",\"zh_guide_install.md\":\"DuL2QCJ_\",\"zh_guide_quickstart.md\":\"CPJMLO2F\",\"zh_guide_resources.md\":\"6P7d8N8C\",\"zh_guide_tutorials.md\":\"B1F8PlnM\",\"zh_guide_tutorials_cnn-classification.md\":\"6bqTLd2c\",\"zh_guide_tutorials_custom-autograd.md\":\"1odlIk1U\",\"zh_guide_tutorials_linear-regression.md\":\"CRwjWFUg\",\"zh_guide_what-is-tensorplay.md\":\"BjbN8xd_\",\"zh_index.md\":\"D-nbMLAi\",\"zh_join_index.md\":\"Conbfq5r\",\"zh_privacy.md\":\"CaMhTqnu\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"TensorPlay\",\"description\":\"A transparent, educational, and PyTorch-compatible deep learning framework.\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"logo\":\"/images/logo-0.png\",\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/bluemoon-o2/tensorplay\"}],\"search\":{\"provider\":\"local\",\"options\":{\"locales\":{\"zh\":{\"translations\":{\"button\":{\"buttonText\":\"搜索文档\",\"buttonAriaLabel\":\"搜索文档\"},\"modal\":{\"noResultsText\":\"无法找到相关结果\",\"resetButtonTitle\":\"清除查询条件\",\"footer\":{\"selectText\":\"选择\",\"navigateText\":\"切换\",\"closeText\":\"关闭\"}}}}}}}},\"locales\":{\"root\":{\"label\":\"English\",\"lang\":\"en\",\"description\":\"A simple deep learning framework designed for educational purposes and small-scale experiments.\",\"themeConfig\":{\"nav\":[{\"text\":\"Learn\",\"items\":[{\"text\":\"Getting Started\",\"link\":\"/guide/getting-started\"},{\"text\":\"Tutorials\",\"link\":\"/guide/tutorials\"}]},{\"text\":\"Community\",\"link\":\"/community/\"},{\"text\":\"Projects\",\"link\":\"/ecosystem/\"},{\"text\":\"Docs\",\"link\":\"/api/\"},{\"text\":\"Blog & News\",\"link\":\"/blog/\"},{\"text\":\"About\",\"link\":\"/about/\"},{\"text\":\"JOIN\",\"link\":\"/join/\"}],\"sidebar\":{\"/guide/\":[{\"text\":\"Introduction\",\"items\":[{\"text\":\"What is TensorPlay?\",\"link\":\"/guide/what-is-tensorplay\"},{\"text\":\"Getting Started\",\"link\":\"/guide/getting-started\"},{\"text\":\"Installation\",\"link\":\"/guide/install\"},{\"text\":\"Quickstart\",\"link\":\"/guide/quickstart\"},{\"text\":\"Architecture\",\"link\":\"/guide/architecture\"}]},{\"text\":\"Guides & Resources\",\"items\":[{\"text\":\"Tutorials\",\"link\":\"/guide/tutorials\",\"items\":[{\"text\":\"Image Classification\",\"link\":\"/guide/tutorials/cnn-classification\"},{\"text\":\"Linear Regression\",\"link\":\"/guide/tutorials/linear-regression\"},{\"text\":\"Custom Autograd\",\"link\":\"/guide/tutorials/custom-autograd\"}]},{\"text\":\"Resources\",\"link\":\"/guide/resources\"},{\"text\":\"API Reference\",\"link\":\"/api/\"}]}],\"/api/\":[{\"text\":\"Core API\",\"items\":[{\"text\":\"Overview\",\"link\":\"/api/\"},{\"text\":\"tensorplay\",\"link\":\"/api/tensorplay\"},{\"text\":\"autograd\",\"link\":\"/api/autograd\"},{\"text\":\"functional\",\"link\":\"/api/functional\"},{\"text\":\"optim\",\"link\":\"/api/optim\"},{\"text\":\"cuda\",\"link\":\"/api/cuda\"}]},{\"text\":\"Neural Networks (nn)\",\"collapsed\":false,\"items\":[{\"text\":\"nn\",\"link\":\"/api/nn\"},{\"text\":\"Modules\",\"link\":\"/api/modules\"},{\"text\":\"Linear Layers\",\"link\":\"/api/linear\"},{\"text\":\"Convolution Layers\",\"link\":\"/api/conv\"},{\"text\":\"Pooling Layers\",\"link\":\"/api/pooling\"},{\"text\":\"Activation Functions\",\"link\":\"/api/activation\"},{\"text\":\"Normalization Layers\",\"link\":\"/api/normalization\"},{\"text\":\"Loss Functions\",\"link\":\"/api/loss\"},{\"text\":\"Dropout Layers\",\"link\":\"/api/dropout\"},{\"text\":\"Container\",\"link\":\"/api/container\"}]}]},\"footer\":{\"message\":\"Released under the Apache 2.0 License.\",\"copyright\":\"Copyright © 2025 zlx. All rights reserved.\"}}},\"zh\":{\"label\":\"简体中文\",\"lang\":\"zh\",\"link\":\"/zh/\",\"description\":\"一个适合学习者、兼容 PyTorch 的深度学习框架。\",\"themeConfig\":{\"nav\":[{\"text\":\"学习\",\"items\":[{\"text\":\"快速开始\",\"link\":\"/zh/guide/getting-started\"},{\"text\":\"教程\",\"link\":\"/zh/guide/tutorials\"}]},{\"text\":\"社区\",\"link\":\"/zh/community/\"},{\"text\":\"项目\",\"link\":\"/zh/ecosystem/\"},{\"text\":\"文档\",\"link\":\"/zh/api/\"},{\"text\":\"博客与新闻\",\"link\":\"/zh/blog/\"},{\"text\":\"关于\",\"link\":\"/zh/about/\"},{\"text\":\"加入我们\",\"link\":\"/zh/join/\"}],\"sidebar\":{\"/zh/guide/\":[{\"text\":\"介绍\",\"items\":[{\"text\":\"什么是 TensorPlay?\",\"link\":\"/zh/guide/what-is-tensorplay\"},{\"text\":\"快速开始\",\"link\":\"/zh/guide/getting-started\"},{\"text\":\"安装\",\"link\":\"/zh/guide/install\"},{\"text\":\"快速入门\",\"link\":\"/zh/guide/quickstart\"},{\"text\":\"架构设计\",\"link\":\"/zh/guide/architecture\"}]},{\"text\":\"指南与资源\",\"items\":[{\"text\":\"教程\",\"link\":\"/zh/guide/tutorials\",\"items\":[{\"text\":\"图像分类\",\"link\":\"/zh/guide/tutorials/cnn-classification\"},{\"text\":\"线性回归\",\"link\":\"/zh/guide/tutorials/linear-regression\"},{\"text\":\"自定义自动微分\",\"link\":\"/zh/guide/tutorials/custom-autograd\"}]},{\"text\":\"资源\",\"link\":\"/zh/guide/resources\"},{\"text\":\"API 参考\",\"link\":\"/zh/api/\"}]}],\"/zh/api/\":[{\"text\":\"核心 API\",\"items\":[{\"text\":\"概览\",\"link\":\"/zh/api/\"},{\"text\":\"tensorplay\",\"link\":\"/zh/api/tensorplay\"},{\"text\":\"autograd\",\"link\":\"/zh/api/autograd\"},{\"text\":\"functional\",\"link\":\"/zh/api/functional\"},{\"text\":\"optim\",\"link\":\"/zh/api/optim\"},{\"text\":\"cuda\",\"link\":\"/zh/api/cuda\"}]},{\"text\":\"神经网络 (nn)\",\"collapsed\":false,\"items\":[{\"text\":\"nn\",\"link\":\"/zh/api/nn\"},{\"text\":\"Modules\",\"link\":\"/zh/api/modules\"},{\"text\":\"Linear Layers\",\"link\":\"/zh/api/linear\"},{\"text\":\"Convolution Layers\",\"link\":\"/zh/api/conv\"},{\"text\":\"Pooling Layers\",\"link\":\"/zh/api/pooling\"},{\"text\":\"Activation Functions\",\"link\":\"/zh/api/activation\"},{\"text\":\"Normalization Layers\",\"link\":\"/zh/api/normalization\"},{\"text\":\"Loss Functions\",\"link\":\"/zh/api/loss\"},{\"text\":\"Dropout Layers\",\"link\":\"/zh/api/dropout\"},{\"text\":\"Container\",\"link\":\"/zh/api/container\"}]}]},\"footer\":{\"message\":\"基于 Apache 2.0 许可发布。\",\"copyright\":\"版权所有 © 2025 zlx。保留所有权利。\"}}}},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>