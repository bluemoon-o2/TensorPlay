<!DOCTYPE html>
<html lang="zh" dir="ltr">
  <head>
    <meta charset="utf-8">
    
    <title>tensorplay.nn.modules.activation | TensorPlay</title>
    <meta name="description" content="一个适合学习者、兼容 PyTorch 的深度学习框架。">
    <meta name="generator" content="VitePress v1.6.4">
    <link rel="preload stylesheet" href="/assets/style.yShgtg9M.css" as="style">
    <link rel="preload stylesheet" href="/vp-icons.css" as="style">
    
    <script type="module" src="/assets/app.SEtEy2QF.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/theme.q-nHENiD.js">
    <link rel="modulepreload" href="/assets/chunks/framework.DJtYh1Hc.js">
    <link rel="modulepreload" href="/assets/zh_api_activation.md.DhmY2P-X.lean.js">
    <link rel="icon" type="image/png" href="/images/logo-0.png">
    <link rel="apple-touch-icon" href="/images/logo-0.png">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <meta name="keywords" content="deep learning, machine learning, ai framework, pytorch compatible, educational, c++, python, tensorplay, 深度学习, 机器学习, 自动微分">
    <meta name="author" content="TensorPlay Team">
    <meta property="og:type" content="website">
    <meta property="og:title" content="TensorPlay | Transparent &amp; Compatible AI Framework">
    <meta property="og:description" content="A transparent, educational, and PyTorch-compatible deep learning framework for the modern AI era.">
    <meta property="og:site_name" content="TensorPlay">
    <meta property="og:url" content="https://www.tensorplay.cn">
    <meta property="og:image" content="https://www.tensorplay.cn/images/logo-1.png">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="TensorPlay | Transparent &amp; Compatible AI Framework">
    <meta name="twitter:description" content="A transparent, educational, and PyTorch-compatible deep learning framework for the modern AI era.">
    <meta name="twitter:image" content="https://www.tensorplay.cn/images/logo-1.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@300;400;500;600;700&amp;display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-5d98c3a5><!--[--><canvas id="cyberpunk-particles" data-v-02d3b8a7></canvas><!----><!--]--><!--[--><span tabindex="-1" data-v-0b0ada53></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-0b0ada53>Skip to content</a><!--]--><!----><header class="VPNav" data-v-5d98c3a5 data-v-ae24b3ad><div class="VPNavBar" data-v-ae24b3ad data-v-6aa21345><div class="wrapper" data-v-6aa21345><div class="container" data-v-6aa21345><div class="title" data-v-6aa21345><div class="VPNavBarTitle has-sidebar" data-v-6aa21345 data-v-1168a8e4><a class="title" href="/zh/" data-v-1168a8e4><!--[--><!--]--><!--[--><img class="VPImage logo" src="/images/logo-0.png" alt data-v-8426fc1a><!--]--><span data-v-1168a8e4>TensorPlay</span><!--[--><!--]--></a></div></div><div class="content" data-v-6aa21345><div class="content-body" data-v-6aa21345><!--[--><!--]--><div class="VPNavBarSearch search" data-v-6aa21345><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="搜索文档"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">搜索文档</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-6aa21345 data-v-dc692963><span id="main-nav-aria-label" class="visually-hidden" data-v-dc692963> Main Navigation </span><!--[--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-dc692963 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-cf11d7a2><span class="text" data-v-cf11d7a2><!----><span data-v-cf11d7a2>学习</span><span class="vpi-chevron-down text-icon" data-v-cf11d7a2></span></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><div class="items" data-v-b98bc113><!--[--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/zh/guide/getting-started" data-v-35975db6><!--[--><span data-v-35975db6>快速开始</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/zh/guide/tutorials" data-v-35975db6><!--[--><span data-v-35975db6>教程</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/community/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>社区</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/ecosystem/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>项目</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/api/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>文档</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/blog/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>博客与新闻</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/about/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>关于</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/join/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>加入我们</span><!--]--></a><!--]--><!--]--></nav><div class="VPFlyout VPNavBarTranslations translations" data-v-6aa21345 data-v-88af2de4 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="Change language" data-v-cf11d7a2><span class="text" data-v-cf11d7a2><span class="vpi-languages option-icon" data-v-cf11d7a2></span><!----><span class="vpi-chevron-down text-icon" data-v-cf11d7a2></span></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><!----><!--[--><!--[--><div class="items" data-v-88af2de4><p class="title" data-v-88af2de4>简体中文</p><!--[--><div class="VPMenuLink" data-v-88af2de4 data-v-35975db6><a class="VPLink link" href="/api/activation" data-v-35975db6><!--[--><span data-v-35975db6>English</span><!--]--></a></div><!--]--></div><!--]--><!--]--></div></div></div><div class="VPNavBarAppearance appearance" data-v-6aa21345 data-v-6c893767><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-6c893767 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-6aa21345 data-v-0394ad82 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/bluemoon-o2/tensorplay" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-6aa21345 data-v-bb2aa2f0 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-cf11d7a2><span class="vpi-more-horizontal icon" data-v-cf11d7a2></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><!----><!--[--><!--[--><div class="group translations" data-v-bb2aa2f0><p class="trans-title" data-v-bb2aa2f0>简体中文</p><!--[--><div class="VPMenuLink" data-v-bb2aa2f0 data-v-35975db6><a class="VPLink link" href="/api/activation" data-v-35975db6><!--[--><span data-v-35975db6>English</span><!--]--></a></div><!--]--></div><div class="group" data-v-bb2aa2f0><div class="item appearance" data-v-bb2aa2f0><p class="label" data-v-bb2aa2f0>Appearance</p><div class="appearance-action" data-v-bb2aa2f0><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-bb2aa2f0 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div></div></div><div class="group" data-v-bb2aa2f0><div class="item social-links" data-v-bb2aa2f0><div class="VPSocialLinks social-links-list" data-v-bb2aa2f0 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/bluemoon-o2/tensorplay" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-6aa21345 data-v-e5dd9c1c><span class="container" data-v-e5dd9c1c><span class="top" data-v-e5dd9c1c></span><span class="middle" data-v-e5dd9c1c></span><span class="bottom" data-v-e5dd9c1c></span></span></button></div></div></div></div><div class="divider" data-v-6aa21345><div class="divider-line" data-v-6aa21345></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-5d98c3a5 data-v-a6f0e41e><div class="container" data-v-a6f0e41e><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-a6f0e41e><span class="vpi-align-left menu-icon" data-v-a6f0e41e></span><span class="menu-text" data-v-a6f0e41e>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-a6f0e41e data-v-8a42e2b4><button data-v-8a42e2b4>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-5d98c3a5 data-v-319d5ca6><div class="curtain" data-v-319d5ca6></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-319d5ca6><span class="visually-hidden" id="sidebar-aria-label" data-v-319d5ca6> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-c40bc020><section class="VPSidebarItem level-0" data-v-c40bc020 data-v-b3fd67f8><div class="item" role="button" tabindex="0" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><h2 class="text" data-v-b3fd67f8>核心 API</h2><!----></div><div class="items" data-v-b3fd67f8><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>概览</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/tensorplay" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>tensorplay</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/autograd" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>autograd</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/functional" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>functional</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/optim" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>optim</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/cuda" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>cuda</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-c40bc020><section class="VPSidebarItem level-0 collapsible has-active" data-v-c40bc020 data-v-b3fd67f8><div class="item" role="button" tabindex="0" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><h2 class="text" data-v-b3fd67f8>神经网络 (nn)</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-b3fd67f8><span class="vpi-chevron-right caret-icon" data-v-b3fd67f8></span></div></div><div class="items" data-v-b3fd67f8><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/nn" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>nn</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/modules" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Modules</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/linear" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Linear Layers</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/conv" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Convolution Layers</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/pooling" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Pooling Layers</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/activation" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Activation Functions</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/normalization" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Normalization Layers</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/loss" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Loss Functions</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/dropout" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Dropout Layers</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/zh/api/container" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Container</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-5d98c3a5 data-v-1428d186><div class="VPDoc has-sidebar has-aside" data-v-1428d186 data-v-39a288b8><!--[--><!--]--><div class="container" data-v-39a288b8><div class="aside" data-v-39a288b8><div class="aside-curtain" data-v-39a288b8></div><div class="aside-container" data-v-39a288b8><div class="aside-content" data-v-39a288b8><div class="VPDocAside" data-v-39a288b8 data-v-3f215769><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-3f215769 data-v-a5bbad30><div class="content" data-v-a5bbad30><div class="outline-marker" data-v-a5bbad30></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-a5bbad30>On this page</div><ul class="VPDocOutlineItem root" data-v-a5bbad30 data-v-b933a997><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-3f215769></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-39a288b8><div class="content-container" data-v-39a288b8><!--[--><!--[--><!--[--><!----><!--]--><!--]--><!--]--><main class="main" data-v-39a288b8><div style="position:relative;" class="vp-doc _zh_api_activation" data-v-39a288b8><div><h1 id="tensorplay-nn-modules-activation" tabindex="-1">tensorplay.nn.modules.activation <a class="header-anchor" href="#tensorplay-nn-modules-activation" aria-label="Permalink to &quot;tensorplay.nn.modules.activation&quot;">​</a></h1><h2 id="classes" tabindex="-1">Classes <a class="header-anchor" href="#classes" aria-label="Permalink to &quot;Classes&quot;">​</a></h2><h3 id="class-gelu-source" tabindex="-1"><code>class GELU</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L292" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-gelu-source" aria-label="Permalink to &quot;`class GELU` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L292)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">GELU(approximate: </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">str</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;none&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>Module</code></p><p>应用高斯误差线性单元 (GELU) 函数。</p><p>.. math:: \text{GELU}(x) = x * \Phi(x)</p><p>其中 <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="4.688ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2072 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="3A6" d="M312 622Q310 623 307 625T303 629T297 631T286 634T270 635T246 636T211 637H184V683H196Q220 680 361 680T526 683H538V637H511Q468 637 447 635T422 631T411 622V533L425 531Q525 519 595 466T665 342Q665 301 642 267T583 209T506 172T425 152L411 150V61Q417 55 421 53T447 48T511 46H538V0H526Q502 3 361 3T196 0H184V46H211Q231 46 245 46T270 47T286 48T297 51T303 54T307 57T312 61V150H310Q309 151 289 153T232 166T160 195Q149 201 136 210T103 238T69 284T56 342Q56 414 128 467T294 530Q309 532 310 533H312V622ZM170 342Q170 207 307 188H312V495H309Q301 495 282 491T231 469T186 423Q170 389 170 342ZM415 188Q487 199 519 236T551 342Q551 384 539 414T507 459T470 481T434 491T415 495H410V188H415Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(722,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1111,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1683,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Φ</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> 是高斯分布的累积分布函数。</p><p>当 approximate 参数为 &#39;tanh&#39; 时，GELU 估计如下：</p><p>.. math:: \text{GELU}(x) = 0.5 * x * (1 + \text{Tanh}(\sqrt{2 / \pi} * (x + 0.044715 * x^3)))</p><h4 id="args" tabindex="-1">Args <a class="header-anchor" href="#args" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>approximate</strong> (<code>str, optional</code>): 使用的 GELU 近似算法： <code>&#39;none&#39;</code> | <code>&#39;tanh&#39;</code>. 默认值： <code>&#39;none&#39;</code></li></ul><h4 id="shape" tabindex="-1">Shape <a class="header-anchor" href="#shape" aria-label="Permalink to &quot;Shape&quot;">​</a></h4><ul><li>输入： <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="2.891ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1278 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(389,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(889,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mo>∗</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>, 其中 <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:0.079ex;" xmlns="http://www.w3.org/2000/svg" width="1.131ex" height="0.973ex" role="img" focusable="false" viewBox="0 -465 500 430" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>∗</mo></math></mjx-assistive-mml></mjx-container> 表示任意维数。</li><li>输出： <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="2.891ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1278 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(389,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(889,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mo>∗</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>, 与输入形状相同。</li></ul><h4 id="examples" tabindex="-1">Examples <a class="header-anchor" href="#examples" aria-label="Permalink to &quot;Examples&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.GELU()</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.randn(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">output </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> m(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><details><summary>Methods</summary><h4 id="init-self-approximate-str-none-none-source" tabindex="-1"><code>__init__(self, approximate: str = &#39;none&#39;) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L321" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-approximate-str-none-none-source" aria-label="Permalink to &quot;`__init__(self, approximate: str = &#39;none&#39;) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L321)&quot;">​</a></h4><p>初始化内部 Module 状态，由 nn.Module 和 ScriptModule 共享。</p><hr><h4 id="add-module-self-name-str-module-optional-forwardref-module-none-source" tabindex="-1"><code>add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#add-module-self-name-str-module-optional-forwardref-module-none-source" aria-label="Permalink to &quot;`add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667)&quot;">​</a></h4><p>将子模块添加到当前模块。</p><p>可以使用给定名称作为属性访问该模块。</p><h4 id="args-1" tabindex="-1">Args <a class="header-anchor" href="#args-1" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): 子模块的名称。可以使用给定名称 从该模块访问子模块</li><li><strong>module</strong> (<code>Module</code>): 要添加到模块的子模块。</li></ul><hr><h4 id="apply-self-fn-callable-forwardref-module-nonetype-self-source" tabindex="-1"><code>apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#apply-self-fn-callable-forwardref-module-nonetype-self-source" aria-label="Permalink to &quot;`apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990)&quot;">​</a></h4><p>递归地将 <code>fn</code> 应用于每个子模块（由 <code>.children()</code> 返回）以及自身。</p><p>典型用途包括初始化模型的参数 (see also <code>nn-init-doc</code>).</p><h4 id="args-2" tabindex="-1">Args <a class="header-anchor" href="#args-2" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>fn</strong> (``Module<code> -&gt; None</code>): 应用于每个子模块的函数</li></ul><h4 id="returns" tabindex="-1">Returns <a class="header-anchor" href="#returns" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="example" tabindex="-1">Example <a class="header-anchor" href="#example" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@tensorplay.no_grad</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> init_weights</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        m.weight.fill_(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m.weight)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net.apply(init_weights)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><hr><h4 id="bfloat16-self-self-source" tabindex="-1"><code>bfloat16(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#bfloat16-self-self-source" aria-label="Permalink to &quot;`bfloat16(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108)&quot;">​</a></h4><p>将所有浮点参数和缓冲区转换为 <code>bfloat16</code> 数据类型。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="returns-1" tabindex="-1">Returns <a class="header-anchor" href="#returns-1" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source" tabindex="-1"><code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source" aria-label="Permalink to &quot;`buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627)&quot;">​</a></h4><p>返回模块缓冲区的迭代器。</p><h4 id="args-3" tabindex="-1">Args <a class="header-anchor" href="#args-3" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): 如果为 True，则生成此模块的缓冲区 以及所有子模块的缓冲区。否则，仅生成 此模块的直接成员缓冲区。</li></ul><h4 id="yields" tabindex="-1">Yields <a class="header-anchor" href="#yields" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>tensorplay.Tensor: 模块缓冲区
</code></pre><h4 id="example-1" tabindex="-1">Example <a class="header-anchor" href="#example-1" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.buffers():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf), buf.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="children-self-collections-abc-iterator-module-source" tabindex="-1"><code>children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#children-self-collections-abc-iterator-module-source" aria-label="Permalink to &quot;`children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681)&quot;">​</a></h4><p>返回直接子模块的迭代器。</p><h4 id="yields-1" tabindex="-1">Yields <a class="header-anchor" href="#yields-1" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: 一个子模块</li></ul><hr><h4 id="compile-self-args-kwargs-source" tabindex="-1"><code>compile(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#compile-self-args-kwargs-source" aria-label="Permalink to &quot;`compile(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944)&quot;">​</a></h4><p>使用 <code>tensorplay.compile</code> 编译此模块的前向传播。</p><p>此模块的 <code>__call__</code> 方法被编译，所有参数按原样传递 给 <code>tensorplay.compile</code>。</p><p>有关此函数的参数详情，请参阅 <code>tensorplay.compile</code>。</p><hr><h4 id="cpu-self-self-source" tabindex="-1"><code>cpu(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cpu-self-self-source" aria-label="Permalink to &quot;`cpu(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050)&quot;">​</a></h4><p>将所有模型参数和缓冲区移动到 CPU。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="returns-2" tabindex="-1">Returns <a class="header-anchor" href="#returns-2" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source" tabindex="-1"><code>cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source" aria-label="Permalink to &quot;`cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031)&quot;">​</a></h4><p>将所有模型参数和缓冲区移动到 GPU。</p><p>这也使得关联的参数和缓冲区成为不同的对象。所以 如果模块将在 GPU 上运行并进行优化， 应在构建优化器之前调用它。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="args-4" tabindex="-1">Args <a class="header-anchor" href="#args-4" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>int, optional</code>): 如果指定，所有参数将被 复制到该设备</li></ul><h4 id="returns-3" tabindex="-1">Returns <a class="header-anchor" href="#returns-3" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="double-self-self-source" tabindex="-1"><code>double(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#double-self-self-source" aria-label="Permalink to &quot;`double(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086)&quot;">​</a></h4><p>将所有浮点参数和缓冲区转换为 <code>double</code> 数据类型。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="returns-4" tabindex="-1">Returns <a class="header-anchor" href="#returns-4" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="eval-self-self-source" tabindex="-1"><code>eval(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#eval-self-self-source" aria-label="Permalink to &quot;`eval(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808)&quot;">​</a></h4><p>将模块设置为评估模式。</p><p>这仅对某些模块有影响。请参阅特定模块的文档 了解它们在训练/评估模式下的行为细节， 即它们是否受影响，例如 <code>Dropout</code>、<code>BatchNorm</code> 等。</p><p>这相当于 <code>self.train(False) &lt;tensorplay.nn.Module.train&gt;</code>。</p><p>有关 <code>.eval()</code> 和其他类似机制的比较， <code>.eval()</code> and several similar mechanisms that may be confused with it.</p><h4 id="returns-5" tabindex="-1">Returns <a class="header-anchor" href="#returns-5" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="extra-repr-self-str-source" tabindex="-1"><code>extra_repr(self) -&gt; str</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L331" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#extra-repr-self-str-source" aria-label="Permalink to &quot;`extra_repr(self) -&gt; str` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L331)&quot;">​</a></h4><p>返回模块的额外表示。</p><hr><h4 id="float-self-self-source" tabindex="-1"><code>float(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#float-self-self-source" aria-label="Permalink to &quot;`float(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075)&quot;">​</a></h4><p>将所有浮点参数和缓冲区转换为 <code>float</code> 数据类型。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="returns-6" tabindex="-1">Returns <a class="header-anchor" href="#returns-6" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="forward-self-input-tensorplay-c-tensorbase-tensorplay-c-tensorbase-source" tabindex="-1"><code>forward(self, input: tensorplay._C.TensorBase) -&gt; tensorplay._C.TensorBase</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L325" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#forward-self-input-tensorplay-c-tensorbase-tensorplay-c-tensorbase-source" aria-label="Permalink to &quot;`forward(self, input: tensorplay._C.TensorBase) -&gt; tensorplay._C.TensorBase` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L325)&quot;">​</a></h4><p>运行前向传播。</p><hr><h4 id="get-buffer-self-target-str-tensor-source" tabindex="-1"><code>get_buffer(self, target: str) -&gt; &#39;Tensor&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-buffer-self-target-str-tensor-source" aria-label="Permalink to &quot;`get_buffer(self, target: str) -&gt; &#39;Tensor&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881)&quot;">​</a></h4><p>如果存在，则返回由 <code>target</code> 指定的缓冲区，否则抛出错误。</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-5" tabindex="-1">Args <a class="header-anchor" href="#args-5" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: 缓冲区的完全限定字符串名称 to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-7" tabindex="-1">Returns <a class="header-anchor" href="#returns-7" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.Tensor: 由 ``target`` 引用的缓冲区
</code></pre><h4 id="raises" tabindex="-1">Raises <a class="header-anchor" href="#raises" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not a buffer</li></ul><hr><h4 id="get-extra-state-self-any-source" tabindex="-1"><code>get_extra_state(self) -&gt; Any</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-extra-state-self-any-source" aria-label="Permalink to &quot;`get_extra_state(self) -&gt; Any` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917)&quot;">​</a></h4><p>返回要包含在模块 state_dict 中的任何额外状态。</p><p>Implement this and a corresponding <code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module&#39;s <code>state_dict()</code>.</p><p>Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.</p><h4 id="returns-8" tabindex="-1">Returns <a class="header-anchor" href="#returns-8" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>object</strong>: Any extra state to store in the module&#39;s state_dict</li></ul><hr><h4 id="get-parameter-self-target-str-parameter-source" tabindex="-1"><code>get_parameter(self, target: str) -&gt; &#39;Parameter&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-parameter-self-target-str-parameter-source" aria-label="Permalink to &quot;`get_parameter(self, target: str) -&gt; &#39;Parameter&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845)&quot;">​</a></h4><p>如果存在，则返回由 <code>target</code> 指定的参数，否则抛出错误。</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-6" tabindex="-1">Args <a class="header-anchor" href="#args-6" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the Parameter to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-9" tabindex="-1">Returns <a class="header-anchor" href="#returns-9" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Parameter: The Parameter referenced by ``target``
</code></pre><h4 id="raises-1" tabindex="-1">Raises <a class="header-anchor" href="#raises-1" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not an <code>nn.Parameter</code></li></ul><hr><h4 id="get-submodule-self-target-str-module-source" tabindex="-1"><code>get_submodule(self, target: str) -&gt; &#39;Module&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-submodule-self-target-str-module-source" aria-label="Permalink to &quot;`get_submodule(self, target: str) -&gt; &#39;Module&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699)&quot;">​</a></h4><p>Return the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> which has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(&quot;net_b.linear&quot;)</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(&quot;net_b.net_c.conv&quot;)</code>.</p><p>The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.</p><h4 id="args-7" tabindex="-1">Args <a class="header-anchor" href="#args-7" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li></ul><h4 id="returns-10" tabindex="-1">Returns <a class="header-anchor" href="#returns-10" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Module: The submodule referenced by ``target``
</code></pre><h4 id="raises-2" tabindex="-1">Raises <a class="header-anchor" href="#raises-2" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If at any point along the path resulting from the target string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="half-self-self-source" tabindex="-1"><code>half(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#half-self-self-source" aria-label="Permalink to &quot;`half(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="returns-11" tabindex="-1">Returns <a class="header-anchor" href="#returns-11" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source" tabindex="-1"><code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source" aria-label="Permalink to &quot;`load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436)&quot;">​</a></h4><p>Copy parameters and buffers from <code>state_dict</code> into this module and its descendants.</p><p>If <code>strict</code> is <code>True</code>, then the keys of <code>state_dict</code> must exactly match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function.</p><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>If <code>assign</code> is <code>True</code> the optimizer must be created after the call to <code>load_state_dict</code> unless <code>~tensorplay.__future__.get_swap_module_params_on_conversion</code> is <code>True</code>.</p></div><h4 id="args-8" tabindex="-1">Args <a class="header-anchor" href="#args-8" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state_dict</strong> (<code>dict</code>): a dict containing parameters and persistent buffers.</li><li><strong>strict</strong> (<code>bool, optional</code>): whether to strictly enforce that the keys in <code>state_dict</code> match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function. 默认值： <code>True</code></li><li><strong>assign</strong> (<code>bool, optional</code>): When set to <code>False</code>, the properties of the tensors in the current module are preserved whereas setting it to <code>True</code> preserves properties of the Tensors in the state dict. The only exception is the <code>requires_grad</code> field of <code>~tensorplay.nn.Parameter</code> for which the value from the module is preserved. 默认值： <code>False</code></li></ul><h4 id="returns-12" tabindex="-1">Returns <a class="header-anchor" href="#returns-12" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:
    * ``missing_keys`` is a list of str containing any keys that are expected
        by this module but missing from the provided ``state_dict``.
    * ``unexpected_keys`` is a list of str containing the keys that are not
        expected by this module but present in the provided ``state_dict``.
</code></pre><h4 id="note" tabindex="-1">Note <a class="header-anchor" href="#note" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>If a parameter or buffer is registered as ``None`` and its corresponding key
exists in `state_dict`, `load_state_dict` will raise a
``RuntimeError``.
</code></pre><hr><h4 id="modules-self-collections-abc-iterator-module-source" tabindex="-1"><code>modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#modules-self-collections-abc-iterator-module-source" aria-label="Permalink to &quot;`modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710)&quot;">​</a></h4><p>Return an iterator over all modules in the network.</p><h4 id="yields-2" tabindex="-1">Yields <a class="header-anchor" href="#yields-2" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a module in the network</li></ul><h4 id="note-1" tabindex="-1">Note <a class="header-anchor" href="#note-1" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-2" tabindex="-1">Example <a class="header-anchor" href="#example-2" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source" tabindex="-1"><code>named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source" aria-label="Permalink to &quot;`named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650)&quot;">​</a></h4><p>Return an iterator over 模块缓冲区s, yielding both the name of the buffer as well as the buffer itself.</p><h4 id="args-9" tabindex="-1">Args <a class="header-anchor" href="#args-9" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all buffer names.</li><li><strong>recurse</strong> (<code>bool, optional</code>): 如果为 True，则生成此模块的缓冲区 以及所有子模块的缓冲区。否则，仅生成 此模块的直接成员缓冲区。 Defaults to True.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated buffers in the result. Defaults to True.</li></ul><h4 id="yields-3" tabindex="-1">Yields <a class="header-anchor" href="#yields-3" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, tensorplay.Tensor): Tuple containing the name and buffer
</code></pre><h4 id="example-3" tabindex="-1">Example <a class="header-anchor" href="#example-3" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_buffers():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_var&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-children-self-collections-abc-iterator-tuple-str-module-source" tabindex="-1"><code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-children-self-collections-abc-iterator-tuple-str-module-source" aria-label="Permalink to &quot;`named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690)&quot;">​</a></h4><p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p><h4 id="yields-4" tabindex="-1">Yields <a class="header-anchor" href="#yields-4" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple containing a name and child module
</code></pre><h4 id="example-4" tabindex="-1">Example <a class="header-anchor" href="#example-4" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, module </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.named_children():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv4&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv5&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(module)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source" tabindex="-1"><code>named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source" aria-label="Permalink to &quot;`named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737)&quot;">​</a></h4><p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p><h4 id="args-10" tabindex="-1">Args <a class="header-anchor" href="#args-10" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>memo</strong>: a memo to store the set of modules already added to the result</li><li><strong>prefix</strong>: a prefix that will be added to the name of the module</li><li><strong>remove_duplicate</strong>: whether to remove the duplicated module instances in the result or not</li></ul><h4 id="yields-5" tabindex="-1">Yields <a class="header-anchor" href="#yields-5" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple of name and module
</code></pre><h4 id="note-2" tabindex="-1">Note <a class="header-anchor" href="#note-2" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-5" tabindex="-1">Example <a class="header-anchor" href="#example-5" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.named_modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;0&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source" tabindex="-1"><code>named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source" aria-label="Permalink to &quot;`named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595)&quot;">​</a></h4><p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p><h4 id="args-11" tabindex="-1">Args <a class="header-anchor" href="#args-11" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all parameter names.</li><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that 此模块的直接成员缓冲区。</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated parameters in the result. Defaults to True.</li></ul><h4 id="yields-6" tabindex="-1">Yields <a class="header-anchor" href="#yields-6" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Parameter): Tuple containing the name and parameter
</code></pre><h4 id="example-6" tabindex="-1">Example <a class="header-anchor" href="#example-6" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_parameters():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source" tabindex="-1"><code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source" aria-label="Permalink to &quot;`parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570)&quot;">​</a></h4><p>Return an iterator over module parameters.</p><p>This is typically passed to an optimizer.</p><h4 id="args-12" tabindex="-1">Args <a class="header-anchor" href="#args-12" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that 此模块的直接成员缓冲区。</li></ul><h4 id="yields-7" tabindex="-1">Yields <a class="header-anchor" href="#yields-7" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Parameter</strong>: module parameter</li></ul><h4 id="example-7" tabindex="-1">Example <a class="header-anchor" href="#example-7" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.parameters():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param), param.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source" tabindex="-1"><code>register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source" aria-label="Permalink to &quot;`register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>This function is deprecated in favor of <code>~tensorplay.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.</p><h4 id="returns-13" tabindex="-1">Returns <a class="header-anchor" href="#returns-13" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source" tabindex="-1"><code>register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source" aria-label="Permalink to &quot;`register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553)&quot;">​</a></h4><p>Add a buffer to the module.</p><p>This is typically used to register a buffer that should not be considered a model parameter. For example, BatchNorm&#39;s <code>running_mean</code> is not a parameter, but is part of the module&#39;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting <code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&#39;s <code>state_dict</code>.</p><p>Buffers can be accessed as attributes using given names.</p><h4 id="args-13" tabindex="-1">Args <a class="header-anchor" href="#args-13" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the buffer. The buffer can be accessed from this module using the given name</li><li><strong>tensor</strong> (<code>Tensor or None</code>): buffer to be registered. If <code>None</code>, then operations that run on buffers, such as <code>cuda</code>, are ignored. If <code>None</code>, the buffer is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li><li><strong>persistent</strong> (<code>bool</code>): whether the buffer is part of this module&#39;s <code>state_dict</code>.</li></ul><h4 id="example-8" tabindex="-1">Example <a class="header-anchor" href="#example-8" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.register_buffer(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_mean&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, tensorplay.zeros(num_features))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><hr><h4 id="register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source" tabindex="-1"><code>register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source" aria-label="Permalink to &quot;`register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592)&quot;">​</a></h4><p>Register a forward hook on the module.</p><p>The hook will be called every time after <code>forward</code> has computed an output.</p><p>If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after <code>forward</code> is called. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-14" tabindex="-1">Args <a class="header-anchor" href="#args-14" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If <code>True</code>, the provided <code>hook</code> will be fired before all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward</code> hooks registered with <code>register_module_forward_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If <code>True</code>, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>always_call</strong> (<code>bool</code>): If <code>True</code> the <code>hook</code> will be run regardless of whether an exception is raised while calling the Module.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-14" tabindex="-1">Returns <a class="header-anchor" href="#returns-14" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source" tabindex="-1"><code>register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source" aria-label="Permalink to &quot;`register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526)&quot;">​</a></h4><p>Register a forward pre-hook on the module.</p><p>The hook will be called every time before <code>forward</code> is invoked.</p><p>If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> of modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> and</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> kwargs</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-15" tabindex="-1">Args <a class="header-anchor" href="#args-15" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward_pre</code> hooks registered with <code>register_module_forward_pre_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If true, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-15" tabindex="-1">Returns <a class="header-anchor" href="#returns-15" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source" tabindex="-1"><code>register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source" aria-label="Permalink to &quot;`register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>The hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:</p><pre><code>1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.
2. If none of the module inputs require gradients, the hook will fire when the gradients are computed
   with respect to module outputs.
3. If none of the module outputs require gradients, then the hooks will not fire.
</code></pre><p>The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_input, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Tensor) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_input</code> and <code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of <code>grad_input</code> in subsequent computations. <code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in <code>grad_input</code> and <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> outputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-16" tabindex="-1">Args <a class="header-anchor" href="#args-16" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward</code> hooks registered with <code>register_module_full_backward_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-16" tabindex="-1">Returns <a class="header-anchor" href="#returns-16" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source" tabindex="-1"><code>register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source" aria-label="Permalink to &quot;`register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287)&quot;">​</a></h4><p>Register a backward pre-hook on the module.</p><p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tuple[Tensor] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of <code>grad_output</code> in subsequent computations. Entries in <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-17" tabindex="-1">Args <a class="header-anchor" href="#args-17" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward_pre</code> hooks registered with <code>register_module_full_backward_pre_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-17" tabindex="-1">Returns <a class="header-anchor" href="#returns-17" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-post-hook-self-hook-source" tabindex="-1"><code>register_load_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-post-hook-self-hook-source" aria-label="Permalink to &quot;`register_load_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226)&quot;">​</a></h4><p>Register a post-hook to be run after module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, incompatible_keys) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.</p><p>The given incompatible_keys can be modified inplace if needed.</p><p>Note that the checks performed when calling <code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.</p><h4 id="returns-18" tabindex="-1">Returns <a class="header-anchor" href="#returns-18" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-pre-hook-self-hook-source" tabindex="-1"><code>register_load_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-pre-hook-self-hook-source" aria-label="Permalink to &quot;`register_load_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214)&quot;">​</a></h4><p>Register a pre-hook to be run before module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # noqa: B950</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="arguments" tabindex="-1">Arguments <a class="header-anchor" href="#arguments" aria-label="Permalink to &quot;Arguments&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): Callable hook that will be invoked before loading the state dict.</li></ul><hr><h4 id="register-module-self-name-str-module-optional-forwardref-module-none-source" tabindex="-1"><code>register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-module-self-name-str-module-optional-forwardref-module-none-source" aria-label="Permalink to &quot;`register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695)&quot;">​</a></h4><p>Alias for <code>add_module</code>.</p><hr><h4 id="register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source" tabindex="-1"><code>register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source" aria-label="Permalink to &quot;`register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617)&quot;">​</a></h4><p>Add a parameter to the module.</p><p>The parameter can be accessed as an attribute using given name.</p><h4 id="args-18" tabindex="-1">Args <a class="header-anchor" href="#args-18" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the parameter. The parameter can be accessed from this module using the given name</li><li><strong>param</strong> (<code>Parameter or None</code>): parameter to be added to the module. If <code>None</code>, then operations that run on parameters, such as <code>cuda</code>, are ignored. If <code>None</code>, the parameter is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li></ul><hr><h4 id="register-state-dict-post-hook-self-hook-source" tabindex="-1"><code>register_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-post-hook-self-hook-source" aria-label="Permalink to &quot;`register_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017)&quot;">​</a></h4><p>Register a post-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can modify the <code>state_dict</code> inplace.</p><hr><h4 id="register-state-dict-pre-hook-self-hook-source" tabindex="-1"><code>register_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-pre-hook-self-hook-source" aria-label="Permalink to &quot;`register_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041)&quot;">​</a></h4><p>Register a pre-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, prefix, keep_vars) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p><hr><h4 id="requires-grad-self-requires-grad-bool-true-self-source" tabindex="-1"><code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#requires-grad-self-requires-grad-bool-true-self-source" aria-label="Permalink to &quot;`requires_grad_(self, requires_grad: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826)&quot;">​</a></h4><p>Change if autograd should record operations on parameters in this module.</p><p>This method sets the parameters&#39; <code>requires_grad</code> attributes in-place.</p><p>This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).</p><p>有关 <code>.eval()</code> 和其他类似机制的比较， <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.</p><h4 id="args-19" tabindex="-1">Args <a class="header-anchor" href="#args-19" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>requires_grad</strong> (<code>bool</code>): whether autograd should record operations on parameters in this module. 默认值： <code>True</code>.</li></ul><h4 id="returns-19" tabindex="-1">Returns <a class="header-anchor" href="#returns-19" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="set-extra-state-self-state-any-none-source" tabindex="-1"><code>set_extra_state(self, state: Any) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-extra-state-self-state-any-none-source" aria-label="Permalink to &quot;`set_extra_state(self, state: Any) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938)&quot;">​</a></h4><p>Set extra state contained in the loaded <code>state_dict</code>.</p><p>This function is called from <code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding <code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.</p><h4 id="args-20" tabindex="-1">Args <a class="header-anchor" href="#args-20" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state</strong> (<code>dict</code>): Extra state from the <code>state_dict</code></li></ul><hr><h4 id="set-submodule-self-target-str-module-module-strict-bool-false-none-source" tabindex="-1"><code>set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-submodule-self-target-str-module-module-strict-bool-false-none-source" aria-label="Permalink to &quot;`set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764)&quot;">​</a></h4><p>Set the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>If <code>strict</code> is set to <code>False</code> (default), the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>strict</code> is set to <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule does not exist.</p></div><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(3, 3, 3) ) (linear): Linear(3, 3) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To override the <code>Conv2d</code> with a new submodule <code>Linear</code>, you could call <code>set_submodule(&quot;net_b.net_c.conv&quot;, nn.Linear(1, 1))</code> where <code>strict</code> could be <code>True</code> or <code>False</code></p><p>To add a new submodule <code>Conv2d</code> to the existing <code>net_b</code> module, you would call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1))</code>.</p><p>In the above if you set <code>strict=True</code> and call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1), strict=True)</code>, an AttributeError will be raised because <code>net_b</code> does not have a submodule named <code>conv</code>.</p><h4 id="args-21" tabindex="-1">Args <a class="header-anchor" href="#args-21" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li><li><strong>module</strong>: The module to set the submodule to.</li><li><strong>strict</strong>: If <code>False</code>, the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule doesn&#39;t already exist.</li></ul><h4 id="raises-3" tabindex="-1">Raises <a class="header-anchor" href="#raises-3" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>ValueError</strong>: If the <code>target</code> string is empty or if <code>module</code> is not an instance of <code>nn.Module</code>.</li><li><strong>AttributeError</strong>: If at any point along the path resulting from the <code>target</code> string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="share-memory-self-self-source" tabindex="-1"><code>share_memory(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#share-memory-self-self-source" aria-label="Permalink to &quot;`share_memory(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877)&quot;">​</a></h4><p>See <code>tensorplay.Tensor.share_memory_</code>.</p><hr><h4 id="state-dict-self-args-destination-none-prefix-keep-vars-false-source" tabindex="-1"><code>state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#state-dict-self-args-destination-none-prefix-keep-vars-false-source" aria-label="Permalink to &quot;`state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105)&quot;">​</a></h4><p>Return a dictionary containing references to the whole state of the module.</p><p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>The returned object is a shallow copy. It contains references to the module&#39;s parameters and buffers.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Currently <code>state_dict()</code> also accepts positional arguments for <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However, this is being deprecated and keyword arguments will be enforced in future releases.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Please avoid the use of argument <code>destination</code> as it is not designed for end-users.</p></div><h4 id="args-22" tabindex="-1">Args <a class="header-anchor" href="#args-22" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>destination</strong> (<code>dict, optional</code>): If provided, the state of module will be updated into the dict and the same object is returned. Otherwise, an <code>OrderedDict</code> will be created and returned.</li><li><strong>Default</strong>: <code>None</code>.</li><li><strong>prefix</strong> (<code>str, optional</code>): a prefix added to parameter and buffer names to compose the keys in state_dict. 默认值： <code>&#39;&#39;</code>.</li><li><strong>keep_vars</strong> (<code>bool, optional</code>): by default the <code>~tensorplay.Tensor</code> s returned in the state dict are detached from autograd. If it&#39;s set to <code>True</code>, detaching will not be performed.</li><li><strong>Default</strong>: <code>False</code>.</li></ul><h4 id="returns-20" tabindex="-1">Returns <a class="header-anchor" href="#returns-20" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>dict</strong>: a dictionary containing a whole state of the module</li></ul><h4 id="example-9" tabindex="-1">Example <a class="header-anchor" href="#example-9" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">module.state_dict().keys()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;weight&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><hr><h4 id="to-self-args-kwargs-source" tabindex="-1"><code>to(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-self-args-kwargs-source" aria-label="Permalink to &quot;`to(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151)&quot;">​</a></h4><p>Move and/or cast the parameters and buffers.</p><p>This can be called as</p><p>.. function:: to(device=None, dtype=None, non_blocking=False) :noindex:</p><p>.. function:: to(dtype, non_blocking=False) :noindex:</p><p>.. function:: to(tensor, non_blocking=False) :noindex:</p><p>.. function:: to(memory_format=tensorplay.channels_last) :noindex:</p><p>Its signature is similar to <code>tensorplay.Tensor.to</code>, but only accepts floating point or complex <code>dtype</code>\ s. In addition, this method will only cast the floating point or complex parameters and buffers to <code>dtype</code> (if given). The integral parameters and buffers will be moved <code>device</code>, if that is given, but with dtypes unchanged. When <code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.</p><p>See below for examples.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="args-23" tabindex="-1">Args <a class="header-anchor" href="#args-23" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): the desired device of the parameters and buffers in this module</li><li><strong>dtype</strong> (<code>tensorplay.dtype</code>): the desired floating point or complex dtype of the parameters and buffers in this module</li><li><strong>tensor</strong> (<code>tensorplay.Tensor</code>): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module</li><li><strong>memory_format</strong> (<code>tensorplay.memory_format</code>): the desired memory format for 4D parameters and buffers in this module (keyword only argument)</li></ul><h4 id="returns-21" tabindex="-1">Returns <a class="header-anchor" href="#returns-21" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="examples-1" tabindex="-1">Examples <a class="header-anchor" href="#examples-1" aria-label="Permalink to &quot;Examples&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +IGNORE_WANT(&quot;non-deterministic&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(tensorplay.double)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float64)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +REQUIRES(env:TENSORPLAY_DOCTEST_CUDA1)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">gpu1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cuda:1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(gpu1, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.half, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">non_blocking</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;cuda:1&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cpu </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cpu&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(cpu)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).to(tensorplay.cdouble)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3741</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,  </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2382</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5593</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.4443</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear(tensorplay.ones(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.cdouble))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br></div></div><hr><h4 id="to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source" tabindex="-1"><code>to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source" aria-label="Permalink to &quot;`to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119)&quot;">​</a></h4><p>Move the parameters and buffers to the specified device without copying storage.</p><h4 id="args-24" tabindex="-1">Args <a class="header-anchor" href="#args-24" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): The desired device of the parameters and buffers in this module.</li><li><strong>recurse</strong> (<code>bool</code>): Whether parameters and buffers of submodules should be recursively moved to the specified device.</li></ul><h4 id="returns-22" tabindex="-1">Returns <a class="header-anchor" href="#returns-22" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="train-self-mode-bool-true-self-source" tabindex="-1"><code>train(self, mode: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#train-self-mode-bool-true-self-source" aria-label="Permalink to &quot;`train(self, mode: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786)&quot;">​</a></h4><p>Set the module in training mode.</p><p>这仅对某些模块有影响。请参阅特定模块的文档 了解它们在训练/评估模式下的行为细节， mode, i.e., whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, 等。</p><h4 id="args-25" tabindex="-1">Args <a class="header-anchor" href="#args-25" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>mode</strong> (<code>bool</code>): whether to set training mode (<code>True</code>) or evaluation mode (<code>False</code>). 默认值： <code>True</code>.</li></ul><h4 id="returns-23" tabindex="-1">Returns <a class="header-anchor" href="#returns-23" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="type-self-dst-type-union-tensorplay-dtype-str-self-source" tabindex="-1"><code>type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#type-self-dst-type-union-tensorplay-dtype-str-self-source" aria-label="Permalink to &quot;`type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061)&quot;">​</a></h4><p>Casts all parameters and buffers to <code>dst_type</code>.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="args-26" tabindex="-1">Args <a class="header-anchor" href="#args-26" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>dst_type</strong> (<code>type or string</code>): the desired type</li></ul><h4 id="returns-24" tabindex="-1">Returns <a class="header-anchor" href="#returns-24" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="zero-grad-self-set-to-none-bool-true-none-source" tabindex="-1"><code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#zero-grad-self-set-to-none-bool-true-none-source" aria-label="Permalink to &quot;`zero_grad(self, set_to_none: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849)&quot;">​</a></h4><p>Reset gradients of all model parameters.</p><p>See similar function under <code>tensorplay.optim.Optimizer</code> for more context.</p><h4 id="args-27" tabindex="-1">Args <a class="header-anchor" href="#args-27" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>set_to_none</strong> (<code>bool</code>): instead of setting to zero, set the grads to None. See <code>tensorplay.optim.Optimizer.zero_grad</code> for details.</li></ul><hr></details><h3 id="class-prelu-source" tabindex="-1"><code>class PReLU</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L113" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-prelu-source" aria-label="Permalink to &quot;`class PReLU` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L113)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">PReLU(num_parameters: </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">int</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, init: </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">float</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.25</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>Module</code></p><p>应用逐元素 PReLU 函数。</p><p>.. math</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">\</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">text{PReLU}(x) = \max(0,x) + a * \min(0,x)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>or</p><p>.. math</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">\</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">text{PReLU}(x) =</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">\</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">begin{cases}</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">x, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&amp;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> \</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">text{ if } x \ge 0 \\</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">ax, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&amp;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> \</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">text{ otherwise }</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">\</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">end{cases}</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>Here <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.197ex" height="1.02ex" role="img" focusable="false" viewBox="0 -441 529 451" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>a</mi></math></mjx-assistive-mml></mjx-container> is a learnable parameter. When called without arguments, <code>nn.PReLU()</code> uses a single parameter <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.197ex" height="1.02ex" role="img" focusable="false" viewBox="0 -441 529 451" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>a</mi></math></mjx-assistive-mml></mjx-container> across all input channels. If called with <code>nn.PReLU(nChannels)</code>, a separate <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.197ex" height="1.02ex" role="img" focusable="false" viewBox="0 -441 529 451" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>a</mi></math></mjx-assistive-mml></mjx-container> is used for each input channel.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>weight decay should not be used when learning <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.197ex" height="1.02ex" role="img" focusable="false" viewBox="0 -441 529 451" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>a</mi></math></mjx-assistive-mml></mjx-container> for good performance.</p></div><div class="info custom-block"><p class="custom-block-title">INFO</p><p>Channel dim is the 2nd dim of input. When input has dims &lt; 2, then there is no channel dim and the number of channels = 1.</p></div><h4 id="args-28" tabindex="-1">Args <a class="header-anchor" href="#args-28" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>num_parameters</strong> (<code>int</code>): number of <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.197ex" height="1.02ex" role="img" focusable="false" viewBox="0 -441 529 451" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>a</mi></math></mjx-assistive-mml></mjx-container> to learn. Although it takes an int as input, there is only two values are legitimate: 1, or the number of channels at input. 默认值： 1</li><li><strong>init</strong> (<code>float</code>): the initial value of <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.197ex" height="1.02ex" role="img" focusable="false" viewBox="0 -441 529 451" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>a</mi></math></mjx-assistive-mml></mjx-container>. 默认值： 0.25</li></ul><h4 id="shape-1" tabindex="-1">Shape <a class="header-anchor" href="#shape-1" aria-label="Permalink to &quot;Shape&quot;">​</a></h4><ul><li>输入： <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="2.891ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1278 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(389,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(889,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mo>∗</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> where <code>*</code> means, any number of additional dimensions.</li><li>输出： <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="2.891ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1278 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(389,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(889,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mo>∗</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>, 与输入形状相同。</li></ul><h4 id="attributes" tabindex="-1">Attributes <a class="header-anchor" href="#attributes" aria-label="Permalink to &quot;Attributes&quot;">​</a></h4><ul><li><strong>weight</strong> (<code>Tensor</code>): the learnable weights of shape (<code>num_parameters</code>).</li></ul><h4 id="examples-2" tabindex="-1">Examples <a class="header-anchor" href="#examples-2" aria-label="Permalink to &quot;Examples&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.PReLU()</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.randn(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">output </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> m(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><details><summary>Methods</summary><h4 id="init-self-num-parameters-int-1-init-float-0-25-device-none-dtype-none-none-source" tabindex="-1"><code>__init__(self, num_parameters: int = 1, init: float = 0.25, device=None, dtype=None) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L164" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-num-parameters-int-1-init-float-0-25-device-none-dtype-none-none-source" aria-label="Permalink to &quot;`__init__(self, num_parameters: int = 1, init: float = 0.25, device=None, dtype=None) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L164)&quot;">​</a></h4><p>初始化内部 Module 状态，由 nn.Module 和 ScriptModule 共享。</p><hr><h4 id="add-module-self-name-str-module-optional-forwardref-module-none-source-1" tabindex="-1"><code>add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#add-module-self-name-str-module-optional-forwardref-module-none-source-1" aria-label="Permalink to &quot;`add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667)&quot;">​</a></h4><p>将子模块添加到当前模块。</p><p>可以使用给定名称作为属性访问该模块。</p><h4 id="args-29" tabindex="-1">Args <a class="header-anchor" href="#args-29" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): 子模块的名称。可以使用给定名称 从该模块访问子模块</li><li><strong>module</strong> (<code>Module</code>): 要添加到模块的子模块。</li></ul><hr><h4 id="apply-self-fn-callable-forwardref-module-nonetype-self-source-1" tabindex="-1"><code>apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#apply-self-fn-callable-forwardref-module-nonetype-self-source-1" aria-label="Permalink to &quot;`apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990)&quot;">​</a></h4><p>递归地将 <code>fn</code> 应用于每个子模块（由 <code>.children()</code> 返回）以及自身。</p><p>典型用途包括初始化模型的参数 (see also <code>nn-init-doc</code>).</p><h4 id="args-30" tabindex="-1">Args <a class="header-anchor" href="#args-30" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>fn</strong> (``Module<code> -&gt; None</code>): 应用于每个子模块的函数</li></ul><h4 id="returns-25" tabindex="-1">Returns <a class="header-anchor" href="#returns-25" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="example-10" tabindex="-1">Example <a class="header-anchor" href="#example-10" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@tensorplay.no_grad</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> init_weights</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        m.weight.fill_(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m.weight)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net.apply(init_weights)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><hr><h4 id="bfloat16-self-self-source-1" tabindex="-1"><code>bfloat16(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#bfloat16-self-self-source-1" aria-label="Permalink to &quot;`bfloat16(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108)&quot;">​</a></h4><p>将所有浮点参数和缓冲区转换为 <code>bfloat16</code> 数据类型。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="returns-26" tabindex="-1">Returns <a class="header-anchor" href="#returns-26" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source-1" tabindex="-1"><code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source-1" aria-label="Permalink to &quot;`buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627)&quot;">​</a></h4><p>返回模块缓冲区的迭代器。</p><h4 id="args-31" tabindex="-1">Args <a class="header-anchor" href="#args-31" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): 如果为 True，则生成此模块的缓冲区 以及所有子模块的缓冲区。否则，仅生成 此模块的直接成员缓冲区。</li></ul><h4 id="yields-8" tabindex="-1">Yields <a class="header-anchor" href="#yields-8" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>tensorplay.Tensor: 模块缓冲区
</code></pre><h4 id="example-11" tabindex="-1">Example <a class="header-anchor" href="#example-11" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.buffers():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf), buf.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="children-self-collections-abc-iterator-module-source-1" tabindex="-1"><code>children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#children-self-collections-abc-iterator-module-source-1" aria-label="Permalink to &quot;`children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681)&quot;">​</a></h4><p>返回直接子模块的迭代器。</p><h4 id="yields-9" tabindex="-1">Yields <a class="header-anchor" href="#yields-9" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: 一个子模块</li></ul><hr><h4 id="compile-self-args-kwargs-source-1" tabindex="-1"><code>compile(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#compile-self-args-kwargs-source-1" aria-label="Permalink to &quot;`compile(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944)&quot;">​</a></h4><p>使用 <code>tensorplay.compile</code> 编译此模块的前向传播。</p><p>此模块的 <code>__call__</code> 方法被编译，所有参数按原样传递 给 <code>tensorplay.compile</code>。</p><p>有关此函数的参数详情，请参阅 <code>tensorplay.compile</code>。</p><hr><h4 id="cpu-self-self-source-1" tabindex="-1"><code>cpu(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cpu-self-self-source-1" aria-label="Permalink to &quot;`cpu(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050)&quot;">​</a></h4><p>将所有模型参数和缓冲区移动到 CPU。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="returns-27" tabindex="-1">Returns <a class="header-anchor" href="#returns-27" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source-1" tabindex="-1"><code>cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source-1" aria-label="Permalink to &quot;`cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031)&quot;">​</a></h4><p>将所有模型参数和缓冲区移动到 GPU。</p><p>这也使得关联的参数和缓冲区成为不同的对象。所以 如果模块将在 GPU 上运行并进行优化， 应在构建优化器之前调用它。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="args-32" tabindex="-1">Args <a class="header-anchor" href="#args-32" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>int, optional</code>): 如果指定，所有参数将被 复制到该设备</li></ul><h4 id="returns-28" tabindex="-1">Returns <a class="header-anchor" href="#returns-28" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="double-self-self-source-1" tabindex="-1"><code>double(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#double-self-self-source-1" aria-label="Permalink to &quot;`double(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086)&quot;">​</a></h4><p>将所有浮点参数和缓冲区转换为 <code>double</code> 数据类型。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="returns-29" tabindex="-1">Returns <a class="header-anchor" href="#returns-29" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="eval-self-self-source-1" tabindex="-1"><code>eval(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#eval-self-self-source-1" aria-label="Permalink to &quot;`eval(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808)&quot;">​</a></h4><p>将模块设置为评估模式。</p><p>这仅对某些模块有影响。请参阅特定模块的文档 了解它们在训练/评估模式下的行为细节， 即它们是否受影响，例如 <code>Dropout</code>、<code>BatchNorm</code> 等。</p><p>这相当于 <code>self.train(False) &lt;tensorplay.nn.Module.train&gt;</code>。</p><p>有关 <code>.eval()</code> 和其他类似机制的比较， <code>.eval()</code> and several similar mechanisms that may be confused with it.</p><h4 id="returns-30" tabindex="-1">Returns <a class="header-anchor" href="#returns-30" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="extra-repr-self-str-source-1" tabindex="-1"><code>extra_repr(self) -&gt; str</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L186" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#extra-repr-self-str-source-1" aria-label="Permalink to &quot;`extra_repr(self) -&gt; str` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L186)&quot;">​</a></h4><p>返回模块的额外表示。</p><hr><h4 id="float-self-self-source-1" tabindex="-1"><code>float(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#float-self-self-source-1" aria-label="Permalink to &quot;`float(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075)&quot;">​</a></h4><p>将所有浮点参数和缓冲区转换为 <code>float</code> 数据类型。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="returns-31" tabindex="-1">Returns <a class="header-anchor" href="#returns-31" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="forward-self-input-tensorplay-c-tensorbase-tensorplay-c-tensorbase-source-1" tabindex="-1"><code>forward(self, input: tensorplay._C.TensorBase) -&gt; tensorplay._C.TensorBase</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L180" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#forward-self-input-tensorplay-c-tensorbase-tensorplay-c-tensorbase-source-1" aria-label="Permalink to &quot;`forward(self, input: tensorplay._C.TensorBase) -&gt; tensorplay._C.TensorBase` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L180)&quot;">​</a></h4><p>运行前向传播。</p><hr><h4 id="get-buffer-self-target-str-tensor-source-1" tabindex="-1"><code>get_buffer(self, target: str) -&gt; &#39;Tensor&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-buffer-self-target-str-tensor-source-1" aria-label="Permalink to &quot;`get_buffer(self, target: str) -&gt; &#39;Tensor&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881)&quot;">​</a></h4><p>如果存在，则返回由 <code>target</code> 指定的缓冲区，否则抛出错误。</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-33" tabindex="-1">Args <a class="header-anchor" href="#args-33" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: 缓冲区的完全限定字符串名称 to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-32" tabindex="-1">Returns <a class="header-anchor" href="#returns-32" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.Tensor: 由 ``target`` 引用的缓冲区
</code></pre><h4 id="raises-4" tabindex="-1">Raises <a class="header-anchor" href="#raises-4" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not a buffer</li></ul><hr><h4 id="get-extra-state-self-any-source-1" tabindex="-1"><code>get_extra_state(self) -&gt; Any</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-extra-state-self-any-source-1" aria-label="Permalink to &quot;`get_extra_state(self) -&gt; Any` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917)&quot;">​</a></h4><p>返回要包含在模块 state_dict 中的任何额外状态。</p><p>Implement this and a corresponding <code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module&#39;s <code>state_dict()</code>.</p><p>Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.</p><h4 id="returns-33" tabindex="-1">Returns <a class="header-anchor" href="#returns-33" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>object</strong>: Any extra state to store in the module&#39;s state_dict</li></ul><hr><h4 id="get-parameter-self-target-str-parameter-source-1" tabindex="-1"><code>get_parameter(self, target: str) -&gt; &#39;Parameter&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-parameter-self-target-str-parameter-source-1" aria-label="Permalink to &quot;`get_parameter(self, target: str) -&gt; &#39;Parameter&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845)&quot;">​</a></h4><p>如果存在，则返回由 <code>target</code> 指定的参数，否则抛出错误。</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-34" tabindex="-1">Args <a class="header-anchor" href="#args-34" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the Parameter to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-34" tabindex="-1">Returns <a class="header-anchor" href="#returns-34" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Parameter: The Parameter referenced by ``target``
</code></pre><h4 id="raises-5" tabindex="-1">Raises <a class="header-anchor" href="#raises-5" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not an <code>nn.Parameter</code></li></ul><hr><h4 id="get-submodule-self-target-str-module-source-1" tabindex="-1"><code>get_submodule(self, target: str) -&gt; &#39;Module&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-submodule-self-target-str-module-source-1" aria-label="Permalink to &quot;`get_submodule(self, target: str) -&gt; &#39;Module&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699)&quot;">​</a></h4><p>Return the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> which has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(&quot;net_b.linear&quot;)</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(&quot;net_b.net_c.conv&quot;)</code>.</p><p>The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.</p><h4 id="args-35" tabindex="-1">Args <a class="header-anchor" href="#args-35" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li></ul><h4 id="returns-35" tabindex="-1">Returns <a class="header-anchor" href="#returns-35" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Module: The submodule referenced by ``target``
</code></pre><h4 id="raises-6" tabindex="-1">Raises <a class="header-anchor" href="#raises-6" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If at any point along the path resulting from the target string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="half-self-self-source-1" tabindex="-1"><code>half(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#half-self-self-source-1" aria-label="Permalink to &quot;`half(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="returns-36" tabindex="-1">Returns <a class="header-anchor" href="#returns-36" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source-1" tabindex="-1"><code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source-1" aria-label="Permalink to &quot;`load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436)&quot;">​</a></h4><p>Copy parameters and buffers from <code>state_dict</code> into this module and its descendants.</p><p>If <code>strict</code> is <code>True</code>, then the keys of <code>state_dict</code> must exactly match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function.</p><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>If <code>assign</code> is <code>True</code> the optimizer must be created after the call to <code>load_state_dict</code> unless <code>~tensorplay.__future__.get_swap_module_params_on_conversion</code> is <code>True</code>.</p></div><h4 id="args-36" tabindex="-1">Args <a class="header-anchor" href="#args-36" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state_dict</strong> (<code>dict</code>): a dict containing parameters and persistent buffers.</li><li><strong>strict</strong> (<code>bool, optional</code>): whether to strictly enforce that the keys in <code>state_dict</code> match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function. 默认值： <code>True</code></li><li><strong>assign</strong> (<code>bool, optional</code>): When set to <code>False</code>, the properties of the tensors in the current module are preserved whereas setting it to <code>True</code> preserves properties of the Tensors in the state dict. The only exception is the <code>requires_grad</code> field of <code>~tensorplay.nn.Parameter</code> for which the value from the module is preserved. 默认值： <code>False</code></li></ul><h4 id="returns-37" tabindex="-1">Returns <a class="header-anchor" href="#returns-37" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:
    * ``missing_keys`` is a list of str containing any keys that are expected
        by this module but missing from the provided ``state_dict``.
    * ``unexpected_keys`` is a list of str containing the keys that are not
        expected by this module but present in the provided ``state_dict``.
</code></pre><h4 id="note-3" tabindex="-1">Note <a class="header-anchor" href="#note-3" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>If a parameter or buffer is registered as ``None`` and its corresponding key
exists in `state_dict`, `load_state_dict` will raise a
``RuntimeError``.
</code></pre><hr><h4 id="modules-self-collections-abc-iterator-module-source-1" tabindex="-1"><code>modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#modules-self-collections-abc-iterator-module-source-1" aria-label="Permalink to &quot;`modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710)&quot;">​</a></h4><p>Return an iterator over all modules in the network.</p><h4 id="yields-10" tabindex="-1">Yields <a class="header-anchor" href="#yields-10" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a module in the network</li></ul><h4 id="note-4" tabindex="-1">Note <a class="header-anchor" href="#note-4" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-12" tabindex="-1">Example <a class="header-anchor" href="#example-12" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source-1" tabindex="-1"><code>named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source-1" aria-label="Permalink to &quot;`named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650)&quot;">​</a></h4><p>Return an iterator over 模块缓冲区s, yielding both the name of the buffer as well as the buffer itself.</p><h4 id="args-37" tabindex="-1">Args <a class="header-anchor" href="#args-37" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all buffer names.</li><li><strong>recurse</strong> (<code>bool, optional</code>): 如果为 True，则生成此模块的缓冲区 以及所有子模块的缓冲区。否则，仅生成 此模块的直接成员缓冲区。 Defaults to True.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated buffers in the result. Defaults to True.</li></ul><h4 id="yields-11" tabindex="-1">Yields <a class="header-anchor" href="#yields-11" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, tensorplay.Tensor): Tuple containing the name and buffer
</code></pre><h4 id="example-13" tabindex="-1">Example <a class="header-anchor" href="#example-13" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_buffers():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_var&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-children-self-collections-abc-iterator-tuple-str-module-source-1" tabindex="-1"><code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-children-self-collections-abc-iterator-tuple-str-module-source-1" aria-label="Permalink to &quot;`named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690)&quot;">​</a></h4><p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p><h4 id="yields-12" tabindex="-1">Yields <a class="header-anchor" href="#yields-12" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple containing a name and child module
</code></pre><h4 id="example-14" tabindex="-1">Example <a class="header-anchor" href="#example-14" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, module </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.named_children():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv4&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv5&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(module)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source-1" tabindex="-1"><code>named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source-1" aria-label="Permalink to &quot;`named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737)&quot;">​</a></h4><p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p><h4 id="args-38" tabindex="-1">Args <a class="header-anchor" href="#args-38" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>memo</strong>: a memo to store the set of modules already added to the result</li><li><strong>prefix</strong>: a prefix that will be added to the name of the module</li><li><strong>remove_duplicate</strong>: whether to remove the duplicated module instances in the result or not</li></ul><h4 id="yields-13" tabindex="-1">Yields <a class="header-anchor" href="#yields-13" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple of name and module
</code></pre><h4 id="note-5" tabindex="-1">Note <a class="header-anchor" href="#note-5" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-15" tabindex="-1">Example <a class="header-anchor" href="#example-15" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.named_modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;0&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source-1" tabindex="-1"><code>named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source-1" aria-label="Permalink to &quot;`named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595)&quot;">​</a></h4><p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p><h4 id="args-39" tabindex="-1">Args <a class="header-anchor" href="#args-39" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all parameter names.</li><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that 此模块的直接成员缓冲区。</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated parameters in the result. Defaults to True.</li></ul><h4 id="yields-14" tabindex="-1">Yields <a class="header-anchor" href="#yields-14" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Parameter): Tuple containing the name and parameter
</code></pre><h4 id="example-16" tabindex="-1">Example <a class="header-anchor" href="#example-16" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_parameters():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source-1" tabindex="-1"><code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source-1" aria-label="Permalink to &quot;`parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570)&quot;">​</a></h4><p>Return an iterator over module parameters.</p><p>This is typically passed to an optimizer.</p><h4 id="args-40" tabindex="-1">Args <a class="header-anchor" href="#args-40" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that 此模块的直接成员缓冲区。</li></ul><h4 id="yields-15" tabindex="-1">Yields <a class="header-anchor" href="#yields-15" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Parameter</strong>: module parameter</li></ul><h4 id="example-17" tabindex="-1">Example <a class="header-anchor" href="#example-17" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.parameters():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param), param.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source-1" tabindex="-1"><code>register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source-1" aria-label="Permalink to &quot;`register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>This function is deprecated in favor of <code>~tensorplay.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.</p><h4 id="returns-38" tabindex="-1">Returns <a class="header-anchor" href="#returns-38" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source-1" tabindex="-1"><code>register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source-1" aria-label="Permalink to &quot;`register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553)&quot;">​</a></h4><p>Add a buffer to the module.</p><p>This is typically used to register a buffer that should not be considered a model parameter. For example, BatchNorm&#39;s <code>running_mean</code> is not a parameter, but is part of the module&#39;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting <code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&#39;s <code>state_dict</code>.</p><p>Buffers can be accessed as attributes using given names.</p><h4 id="args-41" tabindex="-1">Args <a class="header-anchor" href="#args-41" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the buffer. The buffer can be accessed from this module using the given name</li><li><strong>tensor</strong> (<code>Tensor or None</code>): buffer to be registered. If <code>None</code>, then operations that run on buffers, such as <code>cuda</code>, are ignored. If <code>None</code>, the buffer is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li><li><strong>persistent</strong> (<code>bool</code>): whether the buffer is part of this module&#39;s <code>state_dict</code>.</li></ul><h4 id="example-18" tabindex="-1">Example <a class="header-anchor" href="#example-18" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.register_buffer(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_mean&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, tensorplay.zeros(num_features))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><hr><h4 id="register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source-1" tabindex="-1"><code>register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source-1" aria-label="Permalink to &quot;`register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592)&quot;">​</a></h4><p>Register a forward hook on the module.</p><p>The hook will be called every time after <code>forward</code> has computed an output.</p><p>If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after <code>forward</code> is called. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-42" tabindex="-1">Args <a class="header-anchor" href="#args-42" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If <code>True</code>, the provided <code>hook</code> will be fired before all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward</code> hooks registered with <code>register_module_forward_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If <code>True</code>, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>always_call</strong> (<code>bool</code>): If <code>True</code> the <code>hook</code> will be run regardless of whether an exception is raised while calling the Module.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-39" tabindex="-1">Returns <a class="header-anchor" href="#returns-39" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source-1" tabindex="-1"><code>register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source-1" aria-label="Permalink to &quot;`register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526)&quot;">​</a></h4><p>Register a forward pre-hook on the module.</p><p>The hook will be called every time before <code>forward</code> is invoked.</p><p>If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> of modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> and</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> kwargs</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-43" tabindex="-1">Args <a class="header-anchor" href="#args-43" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward_pre</code> hooks registered with <code>register_module_forward_pre_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If true, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-40" tabindex="-1">Returns <a class="header-anchor" href="#returns-40" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-1" tabindex="-1"><code>register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-1" aria-label="Permalink to &quot;`register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>The hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:</p><pre><code>1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.
2. If none of the module inputs require gradients, the hook will fire when the gradients are computed
   with respect to module outputs.
3. If none of the module outputs require gradients, then the hooks will not fire.
</code></pre><p>The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_input, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Tensor) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_input</code> and <code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of <code>grad_input</code> in subsequent computations. <code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in <code>grad_input</code> and <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> outputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-44" tabindex="-1">Args <a class="header-anchor" href="#args-44" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward</code> hooks registered with <code>register_module_full_backward_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-41" tabindex="-1">Returns <a class="header-anchor" href="#returns-41" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-1" tabindex="-1"><code>register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-1" aria-label="Permalink to &quot;`register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287)&quot;">​</a></h4><p>Register a backward pre-hook on the module.</p><p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tuple[Tensor] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of <code>grad_output</code> in subsequent computations. Entries in <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-45" tabindex="-1">Args <a class="header-anchor" href="#args-45" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward_pre</code> hooks registered with <code>register_module_full_backward_pre_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-42" tabindex="-1">Returns <a class="header-anchor" href="#returns-42" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-post-hook-self-hook-source-1" tabindex="-1"><code>register_load_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-post-hook-self-hook-source-1" aria-label="Permalink to &quot;`register_load_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226)&quot;">​</a></h4><p>Register a post-hook to be run after module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, incompatible_keys) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.</p><p>The given incompatible_keys can be modified inplace if needed.</p><p>Note that the checks performed when calling <code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.</p><h4 id="returns-43" tabindex="-1">Returns <a class="header-anchor" href="#returns-43" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-pre-hook-self-hook-source-1" tabindex="-1"><code>register_load_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-pre-hook-self-hook-source-1" aria-label="Permalink to &quot;`register_load_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214)&quot;">​</a></h4><p>Register a pre-hook to be run before module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # noqa: B950</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="arguments-1" tabindex="-1">Arguments <a class="header-anchor" href="#arguments-1" aria-label="Permalink to &quot;Arguments&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): Callable hook that will be invoked before loading the state dict.</li></ul><hr><h4 id="register-module-self-name-str-module-optional-forwardref-module-none-source-1" tabindex="-1"><code>register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-module-self-name-str-module-optional-forwardref-module-none-source-1" aria-label="Permalink to &quot;`register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695)&quot;">​</a></h4><p>Alias for <code>add_module</code>.</p><hr><h4 id="register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source-1" tabindex="-1"><code>register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source-1" aria-label="Permalink to &quot;`register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617)&quot;">​</a></h4><p>Add a parameter to the module.</p><p>The parameter can be accessed as an attribute using given name.</p><h4 id="args-46" tabindex="-1">Args <a class="header-anchor" href="#args-46" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the parameter. The parameter can be accessed from this module using the given name</li><li><strong>param</strong> (<code>Parameter or None</code>): parameter to be added to the module. If <code>None</code>, then operations that run on parameters, such as <code>cuda</code>, are ignored. If <code>None</code>, the parameter is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li></ul><hr><h4 id="register-state-dict-post-hook-self-hook-source-1" tabindex="-1"><code>register_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-post-hook-self-hook-source-1" aria-label="Permalink to &quot;`register_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017)&quot;">​</a></h4><p>Register a post-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can modify the <code>state_dict</code> inplace.</p><hr><h4 id="register-state-dict-pre-hook-self-hook-source-1" tabindex="-1"><code>register_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-pre-hook-self-hook-source-1" aria-label="Permalink to &quot;`register_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041)&quot;">​</a></h4><p>Register a pre-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, prefix, keep_vars) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p><hr><h4 id="requires-grad-self-requires-grad-bool-true-self-source-1" tabindex="-1"><code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#requires-grad-self-requires-grad-bool-true-self-source-1" aria-label="Permalink to &quot;`requires_grad_(self, requires_grad: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826)&quot;">​</a></h4><p>Change if autograd should record operations on parameters in this module.</p><p>This method sets the parameters&#39; <code>requires_grad</code> attributes in-place.</p><p>This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).</p><p>有关 <code>.eval()</code> 和其他类似机制的比较， <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.</p><h4 id="args-47" tabindex="-1">Args <a class="header-anchor" href="#args-47" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>requires_grad</strong> (<code>bool</code>): whether autograd should record operations on parameters in this module. 默认值： <code>True</code>.</li></ul><h4 id="returns-44" tabindex="-1">Returns <a class="header-anchor" href="#returns-44" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="reset-parameters-self-none-source" tabindex="-1"><code>reset_parameters(self) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L174" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#reset-parameters-self-none-source" aria-label="Permalink to &quot;`reset_parameters(self) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L174)&quot;">​</a></h4><p>Resets parameters based on their initialization used in <code>__init__</code>.</p><hr><h4 id="set-extra-state-self-state-any-none-source-1" tabindex="-1"><code>set_extra_state(self, state: Any) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-extra-state-self-state-any-none-source-1" aria-label="Permalink to &quot;`set_extra_state(self, state: Any) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938)&quot;">​</a></h4><p>Set extra state contained in the loaded <code>state_dict</code>.</p><p>This function is called from <code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding <code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.</p><h4 id="args-48" tabindex="-1">Args <a class="header-anchor" href="#args-48" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state</strong> (<code>dict</code>): Extra state from the <code>state_dict</code></li></ul><hr><h4 id="set-submodule-self-target-str-module-module-strict-bool-false-none-source-1" tabindex="-1"><code>set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-submodule-self-target-str-module-module-strict-bool-false-none-source-1" aria-label="Permalink to &quot;`set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764)&quot;">​</a></h4><p>Set the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>If <code>strict</code> is set to <code>False</code> (default), the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>strict</code> is set to <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule does not exist.</p></div><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(3, 3, 3) ) (linear): Linear(3, 3) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To override the <code>Conv2d</code> with a new submodule <code>Linear</code>, you could call <code>set_submodule(&quot;net_b.net_c.conv&quot;, nn.Linear(1, 1))</code> where <code>strict</code> could be <code>True</code> or <code>False</code></p><p>To add a new submodule <code>Conv2d</code> to the existing <code>net_b</code> module, you would call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1))</code>.</p><p>In the above if you set <code>strict=True</code> and call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1), strict=True)</code>, an AttributeError will be raised because <code>net_b</code> does not have a submodule named <code>conv</code>.</p><h4 id="args-49" tabindex="-1">Args <a class="header-anchor" href="#args-49" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li><li><strong>module</strong>: The module to set the submodule to.</li><li><strong>strict</strong>: If <code>False</code>, the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule doesn&#39;t already exist.</li></ul><h4 id="raises-7" tabindex="-1">Raises <a class="header-anchor" href="#raises-7" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>ValueError</strong>: If the <code>target</code> string is empty or if <code>module</code> is not an instance of <code>nn.Module</code>.</li><li><strong>AttributeError</strong>: If at any point along the path resulting from the <code>target</code> string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="share-memory-self-self-source-1" tabindex="-1"><code>share_memory(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#share-memory-self-self-source-1" aria-label="Permalink to &quot;`share_memory(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877)&quot;">​</a></h4><p>See <code>tensorplay.Tensor.share_memory_</code>.</p><hr><h4 id="state-dict-self-args-destination-none-prefix-keep-vars-false-source-1" tabindex="-1"><code>state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#state-dict-self-args-destination-none-prefix-keep-vars-false-source-1" aria-label="Permalink to &quot;`state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105)&quot;">​</a></h4><p>Return a dictionary containing references to the whole state of the module.</p><p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>The returned object is a shallow copy. It contains references to the module&#39;s parameters and buffers.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Currently <code>state_dict()</code> also accepts positional arguments for <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However, this is being deprecated and keyword arguments will be enforced in future releases.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Please avoid the use of argument <code>destination</code> as it is not designed for end-users.</p></div><h4 id="args-50" tabindex="-1">Args <a class="header-anchor" href="#args-50" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>destination</strong> (<code>dict, optional</code>): If provided, the state of module will be updated into the dict and the same object is returned. Otherwise, an <code>OrderedDict</code> will be created and returned.</li><li><strong>Default</strong>: <code>None</code>.</li><li><strong>prefix</strong> (<code>str, optional</code>): a prefix added to parameter and buffer names to compose the keys in state_dict. 默认值： <code>&#39;&#39;</code>.</li><li><strong>keep_vars</strong> (<code>bool, optional</code>): by default the <code>~tensorplay.Tensor</code> s returned in the state dict are detached from autograd. If it&#39;s set to <code>True</code>, detaching will not be performed.</li><li><strong>Default</strong>: <code>False</code>.</li></ul><h4 id="returns-45" tabindex="-1">Returns <a class="header-anchor" href="#returns-45" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>dict</strong>: a dictionary containing a whole state of the module</li></ul><h4 id="example-19" tabindex="-1">Example <a class="header-anchor" href="#example-19" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">module.state_dict().keys()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;weight&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><hr><h4 id="to-self-args-kwargs-source-1" tabindex="-1"><code>to(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-self-args-kwargs-source-1" aria-label="Permalink to &quot;`to(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151)&quot;">​</a></h4><p>Move and/or cast the parameters and buffers.</p><p>This can be called as</p><p>.. function:: to(device=None, dtype=None, non_blocking=False) :noindex:</p><p>.. function:: to(dtype, non_blocking=False) :noindex:</p><p>.. function:: to(tensor, non_blocking=False) :noindex:</p><p>.. function:: to(memory_format=tensorplay.channels_last) :noindex:</p><p>Its signature is similar to <code>tensorplay.Tensor.to</code>, but only accepts floating point or complex <code>dtype</code>\ s. In addition, this method will only cast the floating point or complex parameters and buffers to <code>dtype</code> (if given). The integral parameters and buffers will be moved <code>device</code>, if that is given, but with dtypes unchanged. When <code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.</p><p>See below for examples.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="args-51" tabindex="-1">Args <a class="header-anchor" href="#args-51" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): the desired device of the parameters and buffers in this module</li><li><strong>dtype</strong> (<code>tensorplay.dtype</code>): the desired floating point or complex dtype of the parameters and buffers in this module</li><li><strong>tensor</strong> (<code>tensorplay.Tensor</code>): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module</li><li><strong>memory_format</strong> (<code>tensorplay.memory_format</code>): the desired memory format for 4D parameters and buffers in this module (keyword only argument)</li></ul><h4 id="returns-46" tabindex="-1">Returns <a class="header-anchor" href="#returns-46" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="examples-3" tabindex="-1">Examples <a class="header-anchor" href="#examples-3" aria-label="Permalink to &quot;Examples&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +IGNORE_WANT(&quot;non-deterministic&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(tensorplay.double)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float64)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +REQUIRES(env:TENSORPLAY_DOCTEST_CUDA1)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">gpu1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cuda:1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(gpu1, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.half, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">non_blocking</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;cuda:1&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cpu </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cpu&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(cpu)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).to(tensorplay.cdouble)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3741</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,  </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2382</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5593</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.4443</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear(tensorplay.ones(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.cdouble))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br></div></div><hr><h4 id="to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source-1" tabindex="-1"><code>to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source-1" aria-label="Permalink to &quot;`to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119)&quot;">​</a></h4><p>Move the parameters and buffers to the specified device without copying storage.</p><h4 id="args-52" tabindex="-1">Args <a class="header-anchor" href="#args-52" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): The desired device of the parameters and buffers in this module.</li><li><strong>recurse</strong> (<code>bool</code>): Whether parameters and buffers of submodules should be recursively moved to the specified device.</li></ul><h4 id="returns-47" tabindex="-1">Returns <a class="header-anchor" href="#returns-47" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="train-self-mode-bool-true-self-source-1" tabindex="-1"><code>train(self, mode: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#train-self-mode-bool-true-self-source-1" aria-label="Permalink to &quot;`train(self, mode: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786)&quot;">​</a></h4><p>Set the module in training mode.</p><p>这仅对某些模块有影响。请参阅特定模块的文档 了解它们在训练/评估模式下的行为细节， mode, i.e., whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, 等。</p><h4 id="args-53" tabindex="-1">Args <a class="header-anchor" href="#args-53" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>mode</strong> (<code>bool</code>): whether to set training mode (<code>True</code>) or evaluation mode (<code>False</code>). 默认值： <code>True</code>.</li></ul><h4 id="returns-48" tabindex="-1">Returns <a class="header-anchor" href="#returns-48" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="type-self-dst-type-union-tensorplay-dtype-str-self-source-1" tabindex="-1"><code>type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#type-self-dst-type-union-tensorplay-dtype-str-self-source-1" aria-label="Permalink to &quot;`type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061)&quot;">​</a></h4><p>Casts all parameters and buffers to <code>dst_type</code>.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="args-54" tabindex="-1">Args <a class="header-anchor" href="#args-54" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>dst_type</strong> (<code>type or string</code>): the desired type</li></ul><h4 id="returns-49" tabindex="-1">Returns <a class="header-anchor" href="#returns-49" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="zero-grad-self-set-to-none-bool-true-none-source-1" tabindex="-1"><code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#zero-grad-self-set-to-none-bool-true-none-source-1" aria-label="Permalink to &quot;`zero_grad(self, set_to_none: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849)&quot;">​</a></h4><p>Reset gradients of all model parameters.</p><p>See similar function under <code>tensorplay.optim.Optimizer</code> for more context.</p><h4 id="args-55" tabindex="-1">Args <a class="header-anchor" href="#args-55" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>set_to_none</strong> (<code>bool</code>): instead of setting to zero, set the grads to None. See <code>tensorplay.optim.Optimizer.zero_grad</code> for details.</li></ul><hr></details><h3 id="class-relu-source" tabindex="-1"><code>class ReLU</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L66" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-relu-source" aria-label="Permalink to &quot;`class ReLU` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L66)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">ReLU(inplace: </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bool</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>Module</code></p><p>逐元素应用整流线性单元 (ReLU) 函数。</p><p><mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="28.758ex" height="2.32ex" role="img" focusable="false" viewBox="0 -775.2 12710.9 1025.2" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><path data-c="52" d="M130 622Q123 629 119 631T103 634T60 637H27V683H202H236H300Q376 683 417 677T500 648Q595 600 609 517Q610 512 610 501Q610 468 594 439T556 392T511 361T472 343L456 338Q459 335 467 332Q497 316 516 298T545 254T559 211T568 155T578 94Q588 46 602 31T640 16H645Q660 16 674 32T692 87Q692 98 696 101T712 105T728 103T732 90Q732 59 716 27T672 -16Q656 -22 630 -22Q481 -16 458 90Q456 101 456 163T449 246Q430 304 373 320L363 322L297 323H231V192L232 61Q238 51 249 49T301 46H334V0H323Q302 3 181 3Q59 3 38 0H27V46H60Q102 47 111 49T130 61V622ZM491 499V509Q491 527 490 539T481 570T462 601T424 623T362 636Q360 636 340 636T304 637H283Q238 637 234 628Q231 624 231 492V360H289Q390 360 434 378T489 456Q491 467 491 499Z" style="stroke-width:3;"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(736,0)" style="stroke-width:3;"></path><path data-c="4C" d="M128 622Q121 629 117 631T101 634T58 637H25V683H36Q48 680 182 680Q324 680 348 683H360V637H333Q273 637 258 635T233 622L232 342V129Q232 57 237 52Q243 47 313 47Q384 47 410 53Q470 70 498 110T536 221Q536 226 537 238T540 261T542 272T562 273H582V268Q580 265 568 137T554 5V0H25V46H58Q100 47 109 49T128 61V622Z" transform="translate(1180,0)" style="stroke-width:3;"></path><path data-c="55" d="M128 622Q121 629 117 631T101 634T58 637H25V683H36Q57 680 180 680Q315 680 324 683H335V637H302Q262 636 251 634T233 622L232 418V291Q232 189 240 145T280 67Q325 24 389 24Q454 24 506 64T571 183Q575 206 575 410V598Q569 608 565 613T541 627T489 637H472V683H481Q496 680 598 680T715 683H724V637H707Q634 633 622 598L621 399Q620 194 617 180Q617 179 615 171Q595 83 531 31T389 -22Q304 -22 226 33T130 192Q129 201 128 412V622Z" transform="translate(1805,0)" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2555,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2944,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3516,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(4182.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5238.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(5627.6,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="msup" transform="translate(6199.6,0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(422,363) scale(0.707)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(7499.5,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(8555.2,0)"><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" style="stroke-width:3;"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(833,0)" style="stroke-width:3;"></path><path data-c="78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z" transform="translate(1333,0)" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(10416.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(10805.2,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(11305.2,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(11749.9,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(12321.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>ReLU</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mi>x</mi><msup><mo stretchy="false">)</mo><mo>+</mo></msup><mo>=</mo><mo data-mjx-texclass="OP" movablelimits="true">max</mo><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container></p><h4 id="args-56" tabindex="-1">Args <a class="header-anchor" href="#args-56" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>inplace</strong>: can optionally do the operation in-place. 默认值： <code>False</code></li></ul><h4 id="shape-2" tabindex="-1">Shape <a class="header-anchor" href="#shape-2" aria-label="Permalink to &quot;Shape&quot;">​</a></h4><ul><li>输入： <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="2.891ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1278 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(389,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(889,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mo>∗</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>, 其中 <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:0.079ex;" xmlns="http://www.w3.org/2000/svg" width="1.131ex" height="0.973ex" role="img" focusable="false" viewBox="0 -465 500 430" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>∗</mo></math></mjx-assistive-mml></mjx-container> 表示任意维数。</li><li>输出： <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="2.891ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1278 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(389,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(889,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mo>∗</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>, 与输入形状相同。</li></ul><h4 id="examples-4" tabindex="-1">Examples <a class="header-anchor" href="#examples-4" aria-label="Permalink to &quot;Examples&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.ReLU()</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.randn(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">output </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> m(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>An implementation of CReLU - <a href="https://arxiv.org/abs/1603.05201" target="_blank" rel="noreferrer">https://arxiv.org/abs/1603.05201</a></p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.ReLU()</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.randn(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).unsqueeze(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">output </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.cat((m(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), m(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><details><summary>Methods</summary><h4 id="init-self-inplace-bool-false-none-source" tabindex="-1"><code>__init__(self, inplace: bool = False) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L95" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-inplace-bool-false-none-source" aria-label="Permalink to &quot;`__init__(self, inplace: bool = False) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L95)&quot;">​</a></h4><p>初始化内部 Module 状态，由 nn.Module 和 ScriptModule 共享。</p><hr><h4 id="add-module-self-name-str-module-optional-forwardref-module-none-source-2" tabindex="-1"><code>add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#add-module-self-name-str-module-optional-forwardref-module-none-source-2" aria-label="Permalink to &quot;`add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667)&quot;">​</a></h4><p>将子模块添加到当前模块。</p><p>可以使用给定名称作为属性访问该模块。</p><h4 id="args-57" tabindex="-1">Args <a class="header-anchor" href="#args-57" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): 子模块的名称。可以使用给定名称 从该模块访问子模块</li><li><strong>module</strong> (<code>Module</code>): 要添加到模块的子模块。</li></ul><hr><h4 id="apply-self-fn-callable-forwardref-module-nonetype-self-source-2" tabindex="-1"><code>apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#apply-self-fn-callable-forwardref-module-nonetype-self-source-2" aria-label="Permalink to &quot;`apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990)&quot;">​</a></h4><p>递归地将 <code>fn</code> 应用于每个子模块（由 <code>.children()</code> 返回）以及自身。</p><p>典型用途包括初始化模型的参数 (see also <code>nn-init-doc</code>).</p><h4 id="args-58" tabindex="-1">Args <a class="header-anchor" href="#args-58" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>fn</strong> (``Module<code> -&gt; None</code>): 应用于每个子模块的函数</li></ul><h4 id="returns-50" tabindex="-1">Returns <a class="header-anchor" href="#returns-50" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="example-20" tabindex="-1">Example <a class="header-anchor" href="#example-20" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@tensorplay.no_grad</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> init_weights</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        m.weight.fill_(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m.weight)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net.apply(init_weights)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><hr><h4 id="bfloat16-self-self-source-2" tabindex="-1"><code>bfloat16(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#bfloat16-self-self-source-2" aria-label="Permalink to &quot;`bfloat16(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108)&quot;">​</a></h4><p>将所有浮点参数和缓冲区转换为 <code>bfloat16</code> 数据类型。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="returns-51" tabindex="-1">Returns <a class="header-anchor" href="#returns-51" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source-2" tabindex="-1"><code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source-2" aria-label="Permalink to &quot;`buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627)&quot;">​</a></h4><p>返回模块缓冲区的迭代器。</p><h4 id="args-59" tabindex="-1">Args <a class="header-anchor" href="#args-59" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): 如果为 True，则生成此模块的缓冲区 以及所有子模块的缓冲区。否则，仅生成 此模块的直接成员缓冲区。</li></ul><h4 id="yields-16" tabindex="-1">Yields <a class="header-anchor" href="#yields-16" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>tensorplay.Tensor: 模块缓冲区
</code></pre><h4 id="example-21" tabindex="-1">Example <a class="header-anchor" href="#example-21" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.buffers():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf), buf.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="children-self-collections-abc-iterator-module-source-2" tabindex="-1"><code>children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#children-self-collections-abc-iterator-module-source-2" aria-label="Permalink to &quot;`children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681)&quot;">​</a></h4><p>返回直接子模块的迭代器。</p><h4 id="yields-17" tabindex="-1">Yields <a class="header-anchor" href="#yields-17" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: 一个子模块</li></ul><hr><h4 id="compile-self-args-kwargs-source-2" tabindex="-1"><code>compile(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#compile-self-args-kwargs-source-2" aria-label="Permalink to &quot;`compile(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944)&quot;">​</a></h4><p>使用 <code>tensorplay.compile</code> 编译此模块的前向传播。</p><p>此模块的 <code>__call__</code> 方法被编译，所有参数按原样传递 给 <code>tensorplay.compile</code>。</p><p>有关此函数的参数详情，请参阅 <code>tensorplay.compile</code>。</p><hr><h4 id="cpu-self-self-source-2" tabindex="-1"><code>cpu(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cpu-self-self-source-2" aria-label="Permalink to &quot;`cpu(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050)&quot;">​</a></h4><p>将所有模型参数和缓冲区移动到 CPU。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="returns-52" tabindex="-1">Returns <a class="header-anchor" href="#returns-52" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source-2" tabindex="-1"><code>cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source-2" aria-label="Permalink to &quot;`cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031)&quot;">​</a></h4><p>将所有模型参数和缓冲区移动到 GPU。</p><p>这也使得关联的参数和缓冲区成为不同的对象。所以 如果模块将在 GPU 上运行并进行优化， 应在构建优化器之前调用它。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="args-60" tabindex="-1">Args <a class="header-anchor" href="#args-60" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>int, optional</code>): 如果指定，所有参数将被 复制到该设备</li></ul><h4 id="returns-53" tabindex="-1">Returns <a class="header-anchor" href="#returns-53" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="double-self-self-source-2" tabindex="-1"><code>double(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#double-self-self-source-2" aria-label="Permalink to &quot;`double(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086)&quot;">​</a></h4><p>将所有浮点参数和缓冲区转换为 <code>double</code> 数据类型。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="returns-54" tabindex="-1">Returns <a class="header-anchor" href="#returns-54" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="eval-self-self-source-2" tabindex="-1"><code>eval(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#eval-self-self-source-2" aria-label="Permalink to &quot;`eval(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808)&quot;">​</a></h4><p>将模块设置为评估模式。</p><p>这仅对某些模块有影响。请参阅特定模块的文档 了解它们在训练/评估模式下的行为细节， 即它们是否受影响，例如 <code>Dropout</code>、<code>BatchNorm</code> 等。</p><p>这相当于 <code>self.train(False) &lt;tensorplay.nn.Module.train&gt;</code>。</p><p>有关 <code>.eval()</code> 和其他类似机制的比较， <code>.eval()</code> and several similar mechanisms that may be confused with it.</p><h4 id="returns-55" tabindex="-1">Returns <a class="header-anchor" href="#returns-55" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="extra-repr-self-str-source-2" tabindex="-1"><code>extra_repr(self) -&gt; str</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L105" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#extra-repr-self-str-source-2" aria-label="Permalink to &quot;`extra_repr(self) -&gt; str` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L105)&quot;">​</a></h4><p>返回模块的额外表示。</p><hr><h4 id="float-self-self-source-2" tabindex="-1"><code>float(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#float-self-self-source-2" aria-label="Permalink to &quot;`float(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075)&quot;">​</a></h4><p>将所有浮点参数和缓冲区转换为 <code>float</code> 数据类型。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="returns-56" tabindex="-1">Returns <a class="header-anchor" href="#returns-56" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="forward-self-input-tensorplay-c-tensorbase-tensorplay-c-tensorbase-source-2" tabindex="-1"><code>forward(self, input: tensorplay._C.TensorBase) -&gt; tensorplay._C.TensorBase</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L99" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#forward-self-input-tensorplay-c-tensorbase-tensorplay-c-tensorbase-source-2" aria-label="Permalink to &quot;`forward(self, input: tensorplay._C.TensorBase) -&gt; tensorplay._C.TensorBase` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L99)&quot;">​</a></h4><p>运行前向传播。</p><hr><h4 id="get-buffer-self-target-str-tensor-source-2" tabindex="-1"><code>get_buffer(self, target: str) -&gt; &#39;Tensor&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-buffer-self-target-str-tensor-source-2" aria-label="Permalink to &quot;`get_buffer(self, target: str) -&gt; &#39;Tensor&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881)&quot;">​</a></h4><p>如果存在，则返回由 <code>target</code> 指定的缓冲区，否则抛出错误。</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-61" tabindex="-1">Args <a class="header-anchor" href="#args-61" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: 缓冲区的完全限定字符串名称 to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-57" tabindex="-1">Returns <a class="header-anchor" href="#returns-57" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.Tensor: 由 ``target`` 引用的缓冲区
</code></pre><h4 id="raises-8" tabindex="-1">Raises <a class="header-anchor" href="#raises-8" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not a buffer</li></ul><hr><h4 id="get-extra-state-self-any-source-2" tabindex="-1"><code>get_extra_state(self) -&gt; Any</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-extra-state-self-any-source-2" aria-label="Permalink to &quot;`get_extra_state(self) -&gt; Any` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917)&quot;">​</a></h4><p>返回要包含在模块 state_dict 中的任何额外状态。</p><p>Implement this and a corresponding <code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module&#39;s <code>state_dict()</code>.</p><p>Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.</p><h4 id="returns-58" tabindex="-1">Returns <a class="header-anchor" href="#returns-58" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>object</strong>: Any extra state to store in the module&#39;s state_dict</li></ul><hr><h4 id="get-parameter-self-target-str-parameter-source-2" tabindex="-1"><code>get_parameter(self, target: str) -&gt; &#39;Parameter&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-parameter-self-target-str-parameter-source-2" aria-label="Permalink to &quot;`get_parameter(self, target: str) -&gt; &#39;Parameter&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845)&quot;">​</a></h4><p>如果存在，则返回由 <code>target</code> 指定的参数，否则抛出错误。</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-62" tabindex="-1">Args <a class="header-anchor" href="#args-62" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the Parameter to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-59" tabindex="-1">Returns <a class="header-anchor" href="#returns-59" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Parameter: The Parameter referenced by ``target``
</code></pre><h4 id="raises-9" tabindex="-1">Raises <a class="header-anchor" href="#raises-9" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not an <code>nn.Parameter</code></li></ul><hr><h4 id="get-submodule-self-target-str-module-source-2" tabindex="-1"><code>get_submodule(self, target: str) -&gt; &#39;Module&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-submodule-self-target-str-module-source-2" aria-label="Permalink to &quot;`get_submodule(self, target: str) -&gt; &#39;Module&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699)&quot;">​</a></h4><p>Return the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> which has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(&quot;net_b.linear&quot;)</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(&quot;net_b.net_c.conv&quot;)</code>.</p><p>The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.</p><h4 id="args-63" tabindex="-1">Args <a class="header-anchor" href="#args-63" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li></ul><h4 id="returns-60" tabindex="-1">Returns <a class="header-anchor" href="#returns-60" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Module: The submodule referenced by ``target``
</code></pre><h4 id="raises-10" tabindex="-1">Raises <a class="header-anchor" href="#raises-10" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If at any point along the path resulting from the target string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="half-self-self-source-2" tabindex="-1"><code>half(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#half-self-self-source-2" aria-label="Permalink to &quot;`half(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="returns-61" tabindex="-1">Returns <a class="header-anchor" href="#returns-61" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source-2" tabindex="-1"><code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source-2" aria-label="Permalink to &quot;`load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436)&quot;">​</a></h4><p>Copy parameters and buffers from <code>state_dict</code> into this module and its descendants.</p><p>If <code>strict</code> is <code>True</code>, then the keys of <code>state_dict</code> must exactly match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function.</p><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>If <code>assign</code> is <code>True</code> the optimizer must be created after the call to <code>load_state_dict</code> unless <code>~tensorplay.__future__.get_swap_module_params_on_conversion</code> is <code>True</code>.</p></div><h4 id="args-64" tabindex="-1">Args <a class="header-anchor" href="#args-64" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state_dict</strong> (<code>dict</code>): a dict containing parameters and persistent buffers.</li><li><strong>strict</strong> (<code>bool, optional</code>): whether to strictly enforce that the keys in <code>state_dict</code> match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function. 默认值： <code>True</code></li><li><strong>assign</strong> (<code>bool, optional</code>): When set to <code>False</code>, the properties of the tensors in the current module are preserved whereas setting it to <code>True</code> preserves properties of the Tensors in the state dict. The only exception is the <code>requires_grad</code> field of <code>~tensorplay.nn.Parameter</code> for which the value from the module is preserved. 默认值： <code>False</code></li></ul><h4 id="returns-62" tabindex="-1">Returns <a class="header-anchor" href="#returns-62" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:
    * ``missing_keys`` is a list of str containing any keys that are expected
        by this module but missing from the provided ``state_dict``.
    * ``unexpected_keys`` is a list of str containing the keys that are not
        expected by this module but present in the provided ``state_dict``.
</code></pre><h4 id="note-6" tabindex="-1">Note <a class="header-anchor" href="#note-6" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>If a parameter or buffer is registered as ``None`` and its corresponding key
exists in `state_dict`, `load_state_dict` will raise a
``RuntimeError``.
</code></pre><hr><h4 id="modules-self-collections-abc-iterator-module-source-2" tabindex="-1"><code>modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#modules-self-collections-abc-iterator-module-source-2" aria-label="Permalink to &quot;`modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710)&quot;">​</a></h4><p>Return an iterator over all modules in the network.</p><h4 id="yields-18" tabindex="-1">Yields <a class="header-anchor" href="#yields-18" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a module in the network</li></ul><h4 id="note-7" tabindex="-1">Note <a class="header-anchor" href="#note-7" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-22" tabindex="-1">Example <a class="header-anchor" href="#example-22" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source-2" tabindex="-1"><code>named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source-2" aria-label="Permalink to &quot;`named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650)&quot;">​</a></h4><p>Return an iterator over 模块缓冲区s, yielding both the name of the buffer as well as the buffer itself.</p><h4 id="args-65" tabindex="-1">Args <a class="header-anchor" href="#args-65" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all buffer names.</li><li><strong>recurse</strong> (<code>bool, optional</code>): 如果为 True，则生成此模块的缓冲区 以及所有子模块的缓冲区。否则，仅生成 此模块的直接成员缓冲区。 Defaults to True.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated buffers in the result. Defaults to True.</li></ul><h4 id="yields-19" tabindex="-1">Yields <a class="header-anchor" href="#yields-19" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, tensorplay.Tensor): Tuple containing the name and buffer
</code></pre><h4 id="example-23" tabindex="-1">Example <a class="header-anchor" href="#example-23" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_buffers():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_var&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-children-self-collections-abc-iterator-tuple-str-module-source-2" tabindex="-1"><code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-children-self-collections-abc-iterator-tuple-str-module-source-2" aria-label="Permalink to &quot;`named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690)&quot;">​</a></h4><p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p><h4 id="yields-20" tabindex="-1">Yields <a class="header-anchor" href="#yields-20" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple containing a name and child module
</code></pre><h4 id="example-24" tabindex="-1">Example <a class="header-anchor" href="#example-24" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, module </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.named_children():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv4&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv5&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(module)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source-2" tabindex="-1"><code>named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source-2" aria-label="Permalink to &quot;`named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737)&quot;">​</a></h4><p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p><h4 id="args-66" tabindex="-1">Args <a class="header-anchor" href="#args-66" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>memo</strong>: a memo to store the set of modules already added to the result</li><li><strong>prefix</strong>: a prefix that will be added to the name of the module</li><li><strong>remove_duplicate</strong>: whether to remove the duplicated module instances in the result or not</li></ul><h4 id="yields-21" tabindex="-1">Yields <a class="header-anchor" href="#yields-21" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple of name and module
</code></pre><h4 id="note-8" tabindex="-1">Note <a class="header-anchor" href="#note-8" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-25" tabindex="-1">Example <a class="header-anchor" href="#example-25" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.named_modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;0&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source-2" tabindex="-1"><code>named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source-2" aria-label="Permalink to &quot;`named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595)&quot;">​</a></h4><p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p><h4 id="args-67" tabindex="-1">Args <a class="header-anchor" href="#args-67" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all parameter names.</li><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that 此模块的直接成员缓冲区。</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated parameters in the result. Defaults to True.</li></ul><h4 id="yields-22" tabindex="-1">Yields <a class="header-anchor" href="#yields-22" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Parameter): Tuple containing the name and parameter
</code></pre><h4 id="example-26" tabindex="-1">Example <a class="header-anchor" href="#example-26" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_parameters():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source-2" tabindex="-1"><code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source-2" aria-label="Permalink to &quot;`parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570)&quot;">​</a></h4><p>Return an iterator over module parameters.</p><p>This is typically passed to an optimizer.</p><h4 id="args-68" tabindex="-1">Args <a class="header-anchor" href="#args-68" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that 此模块的直接成员缓冲区。</li></ul><h4 id="yields-23" tabindex="-1">Yields <a class="header-anchor" href="#yields-23" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Parameter</strong>: module parameter</li></ul><h4 id="example-27" tabindex="-1">Example <a class="header-anchor" href="#example-27" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.parameters():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param), param.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source-2" tabindex="-1"><code>register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source-2" aria-label="Permalink to &quot;`register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>This function is deprecated in favor of <code>~tensorplay.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.</p><h4 id="returns-63" tabindex="-1">Returns <a class="header-anchor" href="#returns-63" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source-2" tabindex="-1"><code>register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source-2" aria-label="Permalink to &quot;`register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553)&quot;">​</a></h4><p>Add a buffer to the module.</p><p>This is typically used to register a buffer that should not be considered a model parameter. For example, BatchNorm&#39;s <code>running_mean</code> is not a parameter, but is part of the module&#39;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting <code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&#39;s <code>state_dict</code>.</p><p>Buffers can be accessed as attributes using given names.</p><h4 id="args-69" tabindex="-1">Args <a class="header-anchor" href="#args-69" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the buffer. The buffer can be accessed from this module using the given name</li><li><strong>tensor</strong> (<code>Tensor or None</code>): buffer to be registered. If <code>None</code>, then operations that run on buffers, such as <code>cuda</code>, are ignored. If <code>None</code>, the buffer is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li><li><strong>persistent</strong> (<code>bool</code>): whether the buffer is part of this module&#39;s <code>state_dict</code>.</li></ul><h4 id="example-28" tabindex="-1">Example <a class="header-anchor" href="#example-28" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.register_buffer(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_mean&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, tensorplay.zeros(num_features))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><hr><h4 id="register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source-2" tabindex="-1"><code>register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source-2" aria-label="Permalink to &quot;`register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592)&quot;">​</a></h4><p>Register a forward hook on the module.</p><p>The hook will be called every time after <code>forward</code> has computed an output.</p><p>If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after <code>forward</code> is called. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-70" tabindex="-1">Args <a class="header-anchor" href="#args-70" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If <code>True</code>, the provided <code>hook</code> will be fired before all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward</code> hooks registered with <code>register_module_forward_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If <code>True</code>, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>always_call</strong> (<code>bool</code>): If <code>True</code> the <code>hook</code> will be run regardless of whether an exception is raised while calling the Module.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-64" tabindex="-1">Returns <a class="header-anchor" href="#returns-64" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source-2" tabindex="-1"><code>register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source-2" aria-label="Permalink to &quot;`register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526)&quot;">​</a></h4><p>Register a forward pre-hook on the module.</p><p>The hook will be called every time before <code>forward</code> is invoked.</p><p>If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> of modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> and</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> kwargs</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-71" tabindex="-1">Args <a class="header-anchor" href="#args-71" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward_pre</code> hooks registered with <code>register_module_forward_pre_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If true, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-65" tabindex="-1">Returns <a class="header-anchor" href="#returns-65" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-2" tabindex="-1"><code>register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-2" aria-label="Permalink to &quot;`register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>The hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:</p><pre><code>1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.
2. If none of the module inputs require gradients, the hook will fire when the gradients are computed
   with respect to module outputs.
3. If none of the module outputs require gradients, then the hooks will not fire.
</code></pre><p>The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_input, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Tensor) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_input</code> and <code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of <code>grad_input</code> in subsequent computations. <code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in <code>grad_input</code> and <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> outputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-72" tabindex="-1">Args <a class="header-anchor" href="#args-72" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward</code> hooks registered with <code>register_module_full_backward_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-66" tabindex="-1">Returns <a class="header-anchor" href="#returns-66" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-2" tabindex="-1"><code>register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-2" aria-label="Permalink to &quot;`register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287)&quot;">​</a></h4><p>Register a backward pre-hook on the module.</p><p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tuple[Tensor] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of <code>grad_output</code> in subsequent computations. Entries in <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-73" tabindex="-1">Args <a class="header-anchor" href="#args-73" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward_pre</code> hooks registered with <code>register_module_full_backward_pre_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-67" tabindex="-1">Returns <a class="header-anchor" href="#returns-67" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-post-hook-self-hook-source-2" tabindex="-1"><code>register_load_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-post-hook-self-hook-source-2" aria-label="Permalink to &quot;`register_load_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226)&quot;">​</a></h4><p>Register a post-hook to be run after module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, incompatible_keys) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.</p><p>The given incompatible_keys can be modified inplace if needed.</p><p>Note that the checks performed when calling <code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.</p><h4 id="returns-68" tabindex="-1">Returns <a class="header-anchor" href="#returns-68" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-pre-hook-self-hook-source-2" tabindex="-1"><code>register_load_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-pre-hook-self-hook-source-2" aria-label="Permalink to &quot;`register_load_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214)&quot;">​</a></h4><p>Register a pre-hook to be run before module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # noqa: B950</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="arguments-2" tabindex="-1">Arguments <a class="header-anchor" href="#arguments-2" aria-label="Permalink to &quot;Arguments&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): Callable hook that will be invoked before loading the state dict.</li></ul><hr><h4 id="register-module-self-name-str-module-optional-forwardref-module-none-source-2" tabindex="-1"><code>register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-module-self-name-str-module-optional-forwardref-module-none-source-2" aria-label="Permalink to &quot;`register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695)&quot;">​</a></h4><p>Alias for <code>add_module</code>.</p><hr><h4 id="register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source-2" tabindex="-1"><code>register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source-2" aria-label="Permalink to &quot;`register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617)&quot;">​</a></h4><p>Add a parameter to the module.</p><p>The parameter can be accessed as an attribute using given name.</p><h4 id="args-74" tabindex="-1">Args <a class="header-anchor" href="#args-74" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the parameter. The parameter can be accessed from this module using the given name</li><li><strong>param</strong> (<code>Parameter or None</code>): parameter to be added to the module. If <code>None</code>, then operations that run on parameters, such as <code>cuda</code>, are ignored. If <code>None</code>, the parameter is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li></ul><hr><h4 id="register-state-dict-post-hook-self-hook-source-2" tabindex="-1"><code>register_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-post-hook-self-hook-source-2" aria-label="Permalink to &quot;`register_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017)&quot;">​</a></h4><p>Register a post-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can modify the <code>state_dict</code> inplace.</p><hr><h4 id="register-state-dict-pre-hook-self-hook-source-2" tabindex="-1"><code>register_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-pre-hook-self-hook-source-2" aria-label="Permalink to &quot;`register_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041)&quot;">​</a></h4><p>Register a pre-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, prefix, keep_vars) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p><hr><h4 id="requires-grad-self-requires-grad-bool-true-self-source-2" tabindex="-1"><code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#requires-grad-self-requires-grad-bool-true-self-source-2" aria-label="Permalink to &quot;`requires_grad_(self, requires_grad: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826)&quot;">​</a></h4><p>Change if autograd should record operations on parameters in this module.</p><p>This method sets the parameters&#39; <code>requires_grad</code> attributes in-place.</p><p>This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).</p><p>有关 <code>.eval()</code> 和其他类似机制的比较， <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.</p><h4 id="args-75" tabindex="-1">Args <a class="header-anchor" href="#args-75" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>requires_grad</strong> (<code>bool</code>): whether autograd should record operations on parameters in this module. 默认值： <code>True</code>.</li></ul><h4 id="returns-69" tabindex="-1">Returns <a class="header-anchor" href="#returns-69" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="set-extra-state-self-state-any-none-source-2" tabindex="-1"><code>set_extra_state(self, state: Any) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-extra-state-self-state-any-none-source-2" aria-label="Permalink to &quot;`set_extra_state(self, state: Any) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938)&quot;">​</a></h4><p>Set extra state contained in the loaded <code>state_dict</code>.</p><p>This function is called from <code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding <code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.</p><h4 id="args-76" tabindex="-1">Args <a class="header-anchor" href="#args-76" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state</strong> (<code>dict</code>): Extra state from the <code>state_dict</code></li></ul><hr><h4 id="set-submodule-self-target-str-module-module-strict-bool-false-none-source-2" tabindex="-1"><code>set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-submodule-self-target-str-module-module-strict-bool-false-none-source-2" aria-label="Permalink to &quot;`set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764)&quot;">​</a></h4><p>Set the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>If <code>strict</code> is set to <code>False</code> (default), the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>strict</code> is set to <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule does not exist.</p></div><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(3, 3, 3) ) (linear): Linear(3, 3) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To override the <code>Conv2d</code> with a new submodule <code>Linear</code>, you could call <code>set_submodule(&quot;net_b.net_c.conv&quot;, nn.Linear(1, 1))</code> where <code>strict</code> could be <code>True</code> or <code>False</code></p><p>To add a new submodule <code>Conv2d</code> to the existing <code>net_b</code> module, you would call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1))</code>.</p><p>In the above if you set <code>strict=True</code> and call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1), strict=True)</code>, an AttributeError will be raised because <code>net_b</code> does not have a submodule named <code>conv</code>.</p><h4 id="args-77" tabindex="-1">Args <a class="header-anchor" href="#args-77" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li><li><strong>module</strong>: The module to set the submodule to.</li><li><strong>strict</strong>: If <code>False</code>, the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule doesn&#39;t already exist.</li></ul><h4 id="raises-11" tabindex="-1">Raises <a class="header-anchor" href="#raises-11" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>ValueError</strong>: If the <code>target</code> string is empty or if <code>module</code> is not an instance of <code>nn.Module</code>.</li><li><strong>AttributeError</strong>: If at any point along the path resulting from the <code>target</code> string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="share-memory-self-self-source-2" tabindex="-1"><code>share_memory(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#share-memory-self-self-source-2" aria-label="Permalink to &quot;`share_memory(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877)&quot;">​</a></h4><p>See <code>tensorplay.Tensor.share_memory_</code>.</p><hr><h4 id="state-dict-self-args-destination-none-prefix-keep-vars-false-source-2" tabindex="-1"><code>state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#state-dict-self-args-destination-none-prefix-keep-vars-false-source-2" aria-label="Permalink to &quot;`state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105)&quot;">​</a></h4><p>Return a dictionary containing references to the whole state of the module.</p><p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>The returned object is a shallow copy. It contains references to the module&#39;s parameters and buffers.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Currently <code>state_dict()</code> also accepts positional arguments for <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However, this is being deprecated and keyword arguments will be enforced in future releases.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Please avoid the use of argument <code>destination</code> as it is not designed for end-users.</p></div><h4 id="args-78" tabindex="-1">Args <a class="header-anchor" href="#args-78" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>destination</strong> (<code>dict, optional</code>): If provided, the state of module will be updated into the dict and the same object is returned. Otherwise, an <code>OrderedDict</code> will be created and returned.</li><li><strong>Default</strong>: <code>None</code>.</li><li><strong>prefix</strong> (<code>str, optional</code>): a prefix added to parameter and buffer names to compose the keys in state_dict. 默认值： <code>&#39;&#39;</code>.</li><li><strong>keep_vars</strong> (<code>bool, optional</code>): by default the <code>~tensorplay.Tensor</code> s returned in the state dict are detached from autograd. If it&#39;s set to <code>True</code>, detaching will not be performed.</li><li><strong>Default</strong>: <code>False</code>.</li></ul><h4 id="returns-70" tabindex="-1">Returns <a class="header-anchor" href="#returns-70" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>dict</strong>: a dictionary containing a whole state of the module</li></ul><h4 id="example-29" tabindex="-1">Example <a class="header-anchor" href="#example-29" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">module.state_dict().keys()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;weight&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><hr><h4 id="to-self-args-kwargs-source-2" tabindex="-1"><code>to(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-self-args-kwargs-source-2" aria-label="Permalink to &quot;`to(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151)&quot;">​</a></h4><p>Move and/or cast the parameters and buffers.</p><p>This can be called as</p><p>.. function:: to(device=None, dtype=None, non_blocking=False) :noindex:</p><p>.. function:: to(dtype, non_blocking=False) :noindex:</p><p>.. function:: to(tensor, non_blocking=False) :noindex:</p><p>.. function:: to(memory_format=tensorplay.channels_last) :noindex:</p><p>Its signature is similar to <code>tensorplay.Tensor.to</code>, but only accepts floating point or complex <code>dtype</code>\ s. In addition, this method will only cast the floating point or complex parameters and buffers to <code>dtype</code> (if given). The integral parameters and buffers will be moved <code>device</code>, if that is given, but with dtypes unchanged. When <code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.</p><p>See below for examples.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="args-79" tabindex="-1">Args <a class="header-anchor" href="#args-79" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): the desired device of the parameters and buffers in this module</li><li><strong>dtype</strong> (<code>tensorplay.dtype</code>): the desired floating point or complex dtype of the parameters and buffers in this module</li><li><strong>tensor</strong> (<code>tensorplay.Tensor</code>): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module</li><li><strong>memory_format</strong> (<code>tensorplay.memory_format</code>): the desired memory format for 4D parameters and buffers in this module (keyword only argument)</li></ul><h4 id="returns-71" tabindex="-1">Returns <a class="header-anchor" href="#returns-71" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="examples-5" tabindex="-1">Examples <a class="header-anchor" href="#examples-5" aria-label="Permalink to &quot;Examples&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +IGNORE_WANT(&quot;non-deterministic&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(tensorplay.double)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float64)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +REQUIRES(env:TENSORPLAY_DOCTEST_CUDA1)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">gpu1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cuda:1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(gpu1, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.half, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">non_blocking</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;cuda:1&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cpu </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cpu&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(cpu)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).to(tensorplay.cdouble)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3741</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,  </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2382</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5593</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.4443</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear(tensorplay.ones(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.cdouble))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br></div></div><hr><h4 id="to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source-2" tabindex="-1"><code>to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source-2" aria-label="Permalink to &quot;`to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119)&quot;">​</a></h4><p>Move the parameters and buffers to the specified device without copying storage.</p><h4 id="args-80" tabindex="-1">Args <a class="header-anchor" href="#args-80" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): The desired device of the parameters and buffers in this module.</li><li><strong>recurse</strong> (<code>bool</code>): Whether parameters and buffers of submodules should be recursively moved to the specified device.</li></ul><h4 id="returns-72" tabindex="-1">Returns <a class="header-anchor" href="#returns-72" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="train-self-mode-bool-true-self-source-2" tabindex="-1"><code>train(self, mode: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#train-self-mode-bool-true-self-source-2" aria-label="Permalink to &quot;`train(self, mode: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786)&quot;">​</a></h4><p>Set the module in training mode.</p><p>这仅对某些模块有影响。请参阅特定模块的文档 了解它们在训练/评估模式下的行为细节， mode, i.e., whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, 等。</p><h4 id="args-81" tabindex="-1">Args <a class="header-anchor" href="#args-81" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>mode</strong> (<code>bool</code>): whether to set training mode (<code>True</code>) or evaluation mode (<code>False</code>). 默认值： <code>True</code>.</li></ul><h4 id="returns-73" tabindex="-1">Returns <a class="header-anchor" href="#returns-73" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="type-self-dst-type-union-tensorplay-dtype-str-self-source-2" tabindex="-1"><code>type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#type-self-dst-type-union-tensorplay-dtype-str-self-source-2" aria-label="Permalink to &quot;`type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061)&quot;">​</a></h4><p>Casts all parameters and buffers to <code>dst_type</code>.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="args-82" tabindex="-1">Args <a class="header-anchor" href="#args-82" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>dst_type</strong> (<code>type or string</code>): the desired type</li></ul><h4 id="returns-74" tabindex="-1">Returns <a class="header-anchor" href="#returns-74" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="zero-grad-self-set-to-none-bool-true-none-source-2" tabindex="-1"><code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#zero-grad-self-set-to-none-bool-true-none-source-2" aria-label="Permalink to &quot;`zero_grad(self, set_to_none: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849)&quot;">​</a></h4><p>Reset gradients of all model parameters.</p><p>See similar function under <code>tensorplay.optim.Optimizer</code> for more context.</p><h4 id="args-83" tabindex="-1">Args <a class="header-anchor" href="#args-83" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>set_to_none</strong> (<code>bool</code>): instead of setting to zero, set the grads to None. See <code>tensorplay.optim.Optimizer.zero_grad</code> for details.</li></ul><hr></details><h3 id="class-sigmoid-source" tabindex="-1"><code>class Sigmoid</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L193" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-sigmoid-source" aria-label="Permalink to &quot;`class Sigmoid` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L193)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Sigmoid(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">args, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">**</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">kwargs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>Module</code></p><p>Applies the Sigmoid function element-wise.</p><p>.. math</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">\</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">text{Sigmoid}(x) = \sigma(x) = \frac{1}{1 + \exp(-x)}</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="shape-3" tabindex="-1">Shape <a class="header-anchor" href="#shape-3" aria-label="Permalink to &quot;Shape&quot;">​</a></h4><ul><li>输入： <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="2.891ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1278 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(389,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(889,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mo>∗</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>, 其中 <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:0.079ex;" xmlns="http://www.w3.org/2000/svg" width="1.131ex" height="0.973ex" role="img" focusable="false" viewBox="0 -465 500 430" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>∗</mo></math></mjx-assistive-mml></mjx-container> 表示任意维数。</li><li>输出： <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="2.891ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1278 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(389,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(889,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mo>∗</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>, 与输入形状相同。</li></ul><h4 id="examples-6" tabindex="-1">Examples <a class="header-anchor" href="#examples-6" aria-label="Permalink to &quot;Examples&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sigmoid()</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.randn(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">output </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> m(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><details><summary>Methods</summary><h4 id="init-self-args-kwargs-none-source" tabindex="-1"><code>__init__(self, *args, **kwargs) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L507" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-args-kwargs-none-source" aria-label="Permalink to &quot;`__init__(self, *args, **kwargs) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L507)&quot;">​</a></h4><p>初始化内部 Module 状态，由 nn.Module 和 ScriptModule 共享。</p><hr><h4 id="add-module-self-name-str-module-optional-forwardref-module-none-source-3" tabindex="-1"><code>add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#add-module-self-name-str-module-optional-forwardref-module-none-source-3" aria-label="Permalink to &quot;`add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667)&quot;">​</a></h4><p>将子模块添加到当前模块。</p><p>可以使用给定名称作为属性访问该模块。</p><h4 id="args-84" tabindex="-1">Args <a class="header-anchor" href="#args-84" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): 子模块的名称。可以使用给定名称 从该模块访问子模块</li><li><strong>module</strong> (<code>Module</code>): 要添加到模块的子模块。</li></ul><hr><h4 id="apply-self-fn-callable-forwardref-module-nonetype-self-source-3" tabindex="-1"><code>apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#apply-self-fn-callable-forwardref-module-nonetype-self-source-3" aria-label="Permalink to &quot;`apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990)&quot;">​</a></h4><p>递归地将 <code>fn</code> 应用于每个子模块（由 <code>.children()</code> 返回）以及自身。</p><p>典型用途包括初始化模型的参数 (see also <code>nn-init-doc</code>).</p><h4 id="args-85" tabindex="-1">Args <a class="header-anchor" href="#args-85" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>fn</strong> (``Module<code> -&gt; None</code>): 应用于每个子模块的函数</li></ul><h4 id="returns-75" tabindex="-1">Returns <a class="header-anchor" href="#returns-75" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="example-30" tabindex="-1">Example <a class="header-anchor" href="#example-30" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@tensorplay.no_grad</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> init_weights</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        m.weight.fill_(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m.weight)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net.apply(init_weights)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><hr><h4 id="bfloat16-self-self-source-3" tabindex="-1"><code>bfloat16(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#bfloat16-self-self-source-3" aria-label="Permalink to &quot;`bfloat16(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108)&quot;">​</a></h4><p>将所有浮点参数和缓冲区转换为 <code>bfloat16</code> 数据类型。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="returns-76" tabindex="-1">Returns <a class="header-anchor" href="#returns-76" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source-3" tabindex="-1"><code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source-3" aria-label="Permalink to &quot;`buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627)&quot;">​</a></h4><p>返回模块缓冲区的迭代器。</p><h4 id="args-86" tabindex="-1">Args <a class="header-anchor" href="#args-86" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): 如果为 True，则生成此模块的缓冲区 以及所有子模块的缓冲区。否则，仅生成 此模块的直接成员缓冲区。</li></ul><h4 id="yields-24" tabindex="-1">Yields <a class="header-anchor" href="#yields-24" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>tensorplay.Tensor: 模块缓冲区
</code></pre><h4 id="example-31" tabindex="-1">Example <a class="header-anchor" href="#example-31" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.buffers():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf), buf.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="children-self-collections-abc-iterator-module-source-3" tabindex="-1"><code>children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#children-self-collections-abc-iterator-module-source-3" aria-label="Permalink to &quot;`children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681)&quot;">​</a></h4><p>返回直接子模块的迭代器。</p><h4 id="yields-25" tabindex="-1">Yields <a class="header-anchor" href="#yields-25" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: 一个子模块</li></ul><hr><h4 id="compile-self-args-kwargs-source-3" tabindex="-1"><code>compile(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#compile-self-args-kwargs-source-3" aria-label="Permalink to &quot;`compile(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944)&quot;">​</a></h4><p>使用 <code>tensorplay.compile</code> 编译此模块的前向传播。</p><p>此模块的 <code>__call__</code> 方法被编译，所有参数按原样传递 给 <code>tensorplay.compile</code>。</p><p>有关此函数的参数详情，请参阅 <code>tensorplay.compile</code>。</p><hr><h4 id="cpu-self-self-source-3" tabindex="-1"><code>cpu(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cpu-self-self-source-3" aria-label="Permalink to &quot;`cpu(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050)&quot;">​</a></h4><p>将所有模型参数和缓冲区移动到 CPU。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="returns-77" tabindex="-1">Returns <a class="header-anchor" href="#returns-77" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source-3" tabindex="-1"><code>cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source-3" aria-label="Permalink to &quot;`cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031)&quot;">​</a></h4><p>将所有模型参数和缓冲区移动到 GPU。</p><p>这也使得关联的参数和缓冲区成为不同的对象。所以 如果模块将在 GPU 上运行并进行优化， 应在构建优化器之前调用它。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="args-87" tabindex="-1">Args <a class="header-anchor" href="#args-87" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>int, optional</code>): 如果指定，所有参数将被 复制到该设备</li></ul><h4 id="returns-78" tabindex="-1">Returns <a class="header-anchor" href="#returns-78" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="double-self-self-source-3" tabindex="-1"><code>double(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#double-self-self-source-3" aria-label="Permalink to &quot;`double(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086)&quot;">​</a></h4><p>将所有浮点参数和缓冲区转换为 <code>double</code> 数据类型。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="returns-79" tabindex="-1">Returns <a class="header-anchor" href="#returns-79" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="eval-self-self-source-3" tabindex="-1"><code>eval(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#eval-self-self-source-3" aria-label="Permalink to &quot;`eval(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808)&quot;">​</a></h4><p>将模块设置为评估模式。</p><p>这仅对某些模块有影响。请参阅特定模块的文档 了解它们在训练/评估模式下的行为细节， 即它们是否受影响，例如 <code>Dropout</code>、<code>BatchNorm</code> 等。</p><p>这相当于 <code>self.train(False) &lt;tensorplay.nn.Module.train&gt;</code>。</p><p>有关 <code>.eval()</code> 和其他类似机制的比较， <code>.eval()</code> and several similar mechanisms that may be confused with it.</p><h4 id="returns-80" tabindex="-1">Returns <a class="header-anchor" href="#returns-80" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="extra-repr-self-str-source-3" tabindex="-1"><code>extra_repr(self) -&gt; str</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2884" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#extra-repr-self-str-source-3" aria-label="Permalink to &quot;`extra_repr(self) -&gt; str` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2884)&quot;">​</a></h4><p>返回模块的额外表示。</p><p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p><hr><h4 id="float-self-self-source-3" tabindex="-1"><code>float(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#float-self-self-source-3" aria-label="Permalink to &quot;`float(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075)&quot;">​</a></h4><p>将所有浮点参数和缓冲区转换为 <code>float</code> 数据类型。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="returns-81" tabindex="-1">Returns <a class="header-anchor" href="#returns-81" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="forward-self-input-tensorplay-c-tensorbase-tensorplay-c-tensorbase-source-3" tabindex="-1"><code>forward(self, input: tensorplay._C.TensorBase) -&gt; tensorplay._C.TensorBase</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L211" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#forward-self-input-tensorplay-c-tensorbase-tensorplay-c-tensorbase-source-3" aria-label="Permalink to &quot;`forward(self, input: tensorplay._C.TensorBase) -&gt; tensorplay._C.TensorBase` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L211)&quot;">​</a></h4><p>运行前向传播。</p><hr><h4 id="get-buffer-self-target-str-tensor-source-3" tabindex="-1"><code>get_buffer(self, target: str) -&gt; &#39;Tensor&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-buffer-self-target-str-tensor-source-3" aria-label="Permalink to &quot;`get_buffer(self, target: str) -&gt; &#39;Tensor&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881)&quot;">​</a></h4><p>如果存在，则返回由 <code>target</code> 指定的缓冲区，否则抛出错误。</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-88" tabindex="-1">Args <a class="header-anchor" href="#args-88" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: 缓冲区的完全限定字符串名称 to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-82" tabindex="-1">Returns <a class="header-anchor" href="#returns-82" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.Tensor: 由 ``target`` 引用的缓冲区
</code></pre><h4 id="raises-12" tabindex="-1">Raises <a class="header-anchor" href="#raises-12" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not a buffer</li></ul><hr><h4 id="get-extra-state-self-any-source-3" tabindex="-1"><code>get_extra_state(self) -&gt; Any</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-extra-state-self-any-source-3" aria-label="Permalink to &quot;`get_extra_state(self) -&gt; Any` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917)&quot;">​</a></h4><p>返回要包含在模块 state_dict 中的任何额外状态。</p><p>Implement this and a corresponding <code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module&#39;s <code>state_dict()</code>.</p><p>Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.</p><h4 id="returns-83" tabindex="-1">Returns <a class="header-anchor" href="#returns-83" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>object</strong>: Any extra state to store in the module&#39;s state_dict</li></ul><hr><h4 id="get-parameter-self-target-str-parameter-source-3" tabindex="-1"><code>get_parameter(self, target: str) -&gt; &#39;Parameter&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-parameter-self-target-str-parameter-source-3" aria-label="Permalink to &quot;`get_parameter(self, target: str) -&gt; &#39;Parameter&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845)&quot;">​</a></h4><p>如果存在，则返回由 <code>target</code> 指定的参数，否则抛出错误。</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-89" tabindex="-1">Args <a class="header-anchor" href="#args-89" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the Parameter to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-84" tabindex="-1">Returns <a class="header-anchor" href="#returns-84" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Parameter: The Parameter referenced by ``target``
</code></pre><h4 id="raises-13" tabindex="-1">Raises <a class="header-anchor" href="#raises-13" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not an <code>nn.Parameter</code></li></ul><hr><h4 id="get-submodule-self-target-str-module-source-3" tabindex="-1"><code>get_submodule(self, target: str) -&gt; &#39;Module&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-submodule-self-target-str-module-source-3" aria-label="Permalink to &quot;`get_submodule(self, target: str) -&gt; &#39;Module&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699)&quot;">​</a></h4><p>Return the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> which has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(&quot;net_b.linear&quot;)</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(&quot;net_b.net_c.conv&quot;)</code>.</p><p>The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.</p><h4 id="args-90" tabindex="-1">Args <a class="header-anchor" href="#args-90" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li></ul><h4 id="returns-85" tabindex="-1">Returns <a class="header-anchor" href="#returns-85" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Module: The submodule referenced by ``target``
</code></pre><h4 id="raises-14" tabindex="-1">Raises <a class="header-anchor" href="#raises-14" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If at any point along the path resulting from the target string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="half-self-self-source-3" tabindex="-1"><code>half(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#half-self-self-source-3" aria-label="Permalink to &quot;`half(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="returns-86" tabindex="-1">Returns <a class="header-anchor" href="#returns-86" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source-3" tabindex="-1"><code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source-3" aria-label="Permalink to &quot;`load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436)&quot;">​</a></h4><p>Copy parameters and buffers from <code>state_dict</code> into this module and its descendants.</p><p>If <code>strict</code> is <code>True</code>, then the keys of <code>state_dict</code> must exactly match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function.</p><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>If <code>assign</code> is <code>True</code> the optimizer must be created after the call to <code>load_state_dict</code> unless <code>~tensorplay.__future__.get_swap_module_params_on_conversion</code> is <code>True</code>.</p></div><h4 id="args-91" tabindex="-1">Args <a class="header-anchor" href="#args-91" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state_dict</strong> (<code>dict</code>): a dict containing parameters and persistent buffers.</li><li><strong>strict</strong> (<code>bool, optional</code>): whether to strictly enforce that the keys in <code>state_dict</code> match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function. 默认值： <code>True</code></li><li><strong>assign</strong> (<code>bool, optional</code>): When set to <code>False</code>, the properties of the tensors in the current module are preserved whereas setting it to <code>True</code> preserves properties of the Tensors in the state dict. The only exception is the <code>requires_grad</code> field of <code>~tensorplay.nn.Parameter</code> for which the value from the module is preserved. 默认值： <code>False</code></li></ul><h4 id="returns-87" tabindex="-1">Returns <a class="header-anchor" href="#returns-87" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:
    * ``missing_keys`` is a list of str containing any keys that are expected
        by this module but missing from the provided ``state_dict``.
    * ``unexpected_keys`` is a list of str containing the keys that are not
        expected by this module but present in the provided ``state_dict``.
</code></pre><h4 id="note-9" tabindex="-1">Note <a class="header-anchor" href="#note-9" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>If a parameter or buffer is registered as ``None`` and its corresponding key
exists in `state_dict`, `load_state_dict` will raise a
``RuntimeError``.
</code></pre><hr><h4 id="modules-self-collections-abc-iterator-module-source-3" tabindex="-1"><code>modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#modules-self-collections-abc-iterator-module-source-3" aria-label="Permalink to &quot;`modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710)&quot;">​</a></h4><p>Return an iterator over all modules in the network.</p><h4 id="yields-26" tabindex="-1">Yields <a class="header-anchor" href="#yields-26" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a module in the network</li></ul><h4 id="note-10" tabindex="-1">Note <a class="header-anchor" href="#note-10" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-32" tabindex="-1">Example <a class="header-anchor" href="#example-32" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source-3" tabindex="-1"><code>named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source-3" aria-label="Permalink to &quot;`named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650)&quot;">​</a></h4><p>Return an iterator over 模块缓冲区s, yielding both the name of the buffer as well as the buffer itself.</p><h4 id="args-92" tabindex="-1">Args <a class="header-anchor" href="#args-92" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all buffer names.</li><li><strong>recurse</strong> (<code>bool, optional</code>): 如果为 True，则生成此模块的缓冲区 以及所有子模块的缓冲区。否则，仅生成 此模块的直接成员缓冲区。 Defaults to True.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated buffers in the result. Defaults to True.</li></ul><h4 id="yields-27" tabindex="-1">Yields <a class="header-anchor" href="#yields-27" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, tensorplay.Tensor): Tuple containing the name and buffer
</code></pre><h4 id="example-33" tabindex="-1">Example <a class="header-anchor" href="#example-33" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_buffers():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_var&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-children-self-collections-abc-iterator-tuple-str-module-source-3" tabindex="-1"><code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-children-self-collections-abc-iterator-tuple-str-module-source-3" aria-label="Permalink to &quot;`named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690)&quot;">​</a></h4><p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p><h4 id="yields-28" tabindex="-1">Yields <a class="header-anchor" href="#yields-28" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple containing a name and child module
</code></pre><h4 id="example-34" tabindex="-1">Example <a class="header-anchor" href="#example-34" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, module </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.named_children():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv4&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv5&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(module)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source-3" tabindex="-1"><code>named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source-3" aria-label="Permalink to &quot;`named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737)&quot;">​</a></h4><p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p><h4 id="args-93" tabindex="-1">Args <a class="header-anchor" href="#args-93" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>memo</strong>: a memo to store the set of modules already added to the result</li><li><strong>prefix</strong>: a prefix that will be added to the name of the module</li><li><strong>remove_duplicate</strong>: whether to remove the duplicated module instances in the result or not</li></ul><h4 id="yields-29" tabindex="-1">Yields <a class="header-anchor" href="#yields-29" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple of name and module
</code></pre><h4 id="note-11" tabindex="-1">Note <a class="header-anchor" href="#note-11" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-35" tabindex="-1">Example <a class="header-anchor" href="#example-35" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.named_modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;0&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source-3" tabindex="-1"><code>named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source-3" aria-label="Permalink to &quot;`named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595)&quot;">​</a></h4><p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p><h4 id="args-94" tabindex="-1">Args <a class="header-anchor" href="#args-94" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all parameter names.</li><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that 此模块的直接成员缓冲区。</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated parameters in the result. Defaults to True.</li></ul><h4 id="yields-30" tabindex="-1">Yields <a class="header-anchor" href="#yields-30" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Parameter): Tuple containing the name and parameter
</code></pre><h4 id="example-36" tabindex="-1">Example <a class="header-anchor" href="#example-36" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_parameters():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source-3" tabindex="-1"><code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source-3" aria-label="Permalink to &quot;`parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570)&quot;">​</a></h4><p>Return an iterator over module parameters.</p><p>This is typically passed to an optimizer.</p><h4 id="args-95" tabindex="-1">Args <a class="header-anchor" href="#args-95" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that 此模块的直接成员缓冲区。</li></ul><h4 id="yields-31" tabindex="-1">Yields <a class="header-anchor" href="#yields-31" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Parameter</strong>: module parameter</li></ul><h4 id="example-37" tabindex="-1">Example <a class="header-anchor" href="#example-37" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.parameters():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param), param.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source-3" tabindex="-1"><code>register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source-3" aria-label="Permalink to &quot;`register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>This function is deprecated in favor of <code>~tensorplay.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.</p><h4 id="returns-88" tabindex="-1">Returns <a class="header-anchor" href="#returns-88" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source-3" tabindex="-1"><code>register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source-3" aria-label="Permalink to &quot;`register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553)&quot;">​</a></h4><p>Add a buffer to the module.</p><p>This is typically used to register a buffer that should not be considered a model parameter. For example, BatchNorm&#39;s <code>running_mean</code> is not a parameter, but is part of the module&#39;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting <code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&#39;s <code>state_dict</code>.</p><p>Buffers can be accessed as attributes using given names.</p><h4 id="args-96" tabindex="-1">Args <a class="header-anchor" href="#args-96" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the buffer. The buffer can be accessed from this module using the given name</li><li><strong>tensor</strong> (<code>Tensor or None</code>): buffer to be registered. If <code>None</code>, then operations that run on buffers, such as <code>cuda</code>, are ignored. If <code>None</code>, the buffer is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li><li><strong>persistent</strong> (<code>bool</code>): whether the buffer is part of this module&#39;s <code>state_dict</code>.</li></ul><h4 id="example-38" tabindex="-1">Example <a class="header-anchor" href="#example-38" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.register_buffer(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_mean&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, tensorplay.zeros(num_features))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><hr><h4 id="register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source-3" tabindex="-1"><code>register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source-3" aria-label="Permalink to &quot;`register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592)&quot;">​</a></h4><p>Register a forward hook on the module.</p><p>The hook will be called every time after <code>forward</code> has computed an output.</p><p>If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after <code>forward</code> is called. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-97" tabindex="-1">Args <a class="header-anchor" href="#args-97" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If <code>True</code>, the provided <code>hook</code> will be fired before all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward</code> hooks registered with <code>register_module_forward_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If <code>True</code>, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>always_call</strong> (<code>bool</code>): If <code>True</code> the <code>hook</code> will be run regardless of whether an exception is raised while calling the Module.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-89" tabindex="-1">Returns <a class="header-anchor" href="#returns-89" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source-3" tabindex="-1"><code>register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source-3" aria-label="Permalink to &quot;`register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526)&quot;">​</a></h4><p>Register a forward pre-hook on the module.</p><p>The hook will be called every time before <code>forward</code> is invoked.</p><p>If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> of modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> and</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> kwargs</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-98" tabindex="-1">Args <a class="header-anchor" href="#args-98" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward_pre</code> hooks registered with <code>register_module_forward_pre_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If true, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-90" tabindex="-1">Returns <a class="header-anchor" href="#returns-90" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-3" tabindex="-1"><code>register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-3" aria-label="Permalink to &quot;`register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>The hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:</p><pre><code>1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.
2. If none of the module inputs require gradients, the hook will fire when the gradients are computed
   with respect to module outputs.
3. If none of the module outputs require gradients, then the hooks will not fire.
</code></pre><p>The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_input, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Tensor) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_input</code> and <code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of <code>grad_input</code> in subsequent computations. <code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in <code>grad_input</code> and <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> outputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-99" tabindex="-1">Args <a class="header-anchor" href="#args-99" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward</code> hooks registered with <code>register_module_full_backward_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-91" tabindex="-1">Returns <a class="header-anchor" href="#returns-91" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-3" tabindex="-1"><code>register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-3" aria-label="Permalink to &quot;`register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287)&quot;">​</a></h4><p>Register a backward pre-hook on the module.</p><p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tuple[Tensor] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of <code>grad_output</code> in subsequent computations. Entries in <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-100" tabindex="-1">Args <a class="header-anchor" href="#args-100" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward_pre</code> hooks registered with <code>register_module_full_backward_pre_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-92" tabindex="-1">Returns <a class="header-anchor" href="#returns-92" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-post-hook-self-hook-source-3" tabindex="-1"><code>register_load_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-post-hook-self-hook-source-3" aria-label="Permalink to &quot;`register_load_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226)&quot;">​</a></h4><p>Register a post-hook to be run after module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, incompatible_keys) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.</p><p>The given incompatible_keys can be modified inplace if needed.</p><p>Note that the checks performed when calling <code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.</p><h4 id="returns-93" tabindex="-1">Returns <a class="header-anchor" href="#returns-93" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-pre-hook-self-hook-source-3" tabindex="-1"><code>register_load_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-pre-hook-self-hook-source-3" aria-label="Permalink to &quot;`register_load_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214)&quot;">​</a></h4><p>Register a pre-hook to be run before module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # noqa: B950</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="arguments-3" tabindex="-1">Arguments <a class="header-anchor" href="#arguments-3" aria-label="Permalink to &quot;Arguments&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): Callable hook that will be invoked before loading the state dict.</li></ul><hr><h4 id="register-module-self-name-str-module-optional-forwardref-module-none-source-3" tabindex="-1"><code>register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-module-self-name-str-module-optional-forwardref-module-none-source-3" aria-label="Permalink to &quot;`register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695)&quot;">​</a></h4><p>Alias for <code>add_module</code>.</p><hr><h4 id="register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source-3" tabindex="-1"><code>register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source-3" aria-label="Permalink to &quot;`register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617)&quot;">​</a></h4><p>Add a parameter to the module.</p><p>The parameter can be accessed as an attribute using given name.</p><h4 id="args-101" tabindex="-1">Args <a class="header-anchor" href="#args-101" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the parameter. The parameter can be accessed from this module using the given name</li><li><strong>param</strong> (<code>Parameter or None</code>): parameter to be added to the module. If <code>None</code>, then operations that run on parameters, such as <code>cuda</code>, are ignored. If <code>None</code>, the parameter is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li></ul><hr><h4 id="register-state-dict-post-hook-self-hook-source-3" tabindex="-1"><code>register_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-post-hook-self-hook-source-3" aria-label="Permalink to &quot;`register_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017)&quot;">​</a></h4><p>Register a post-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can modify the <code>state_dict</code> inplace.</p><hr><h4 id="register-state-dict-pre-hook-self-hook-source-3" tabindex="-1"><code>register_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-pre-hook-self-hook-source-3" aria-label="Permalink to &quot;`register_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041)&quot;">​</a></h4><p>Register a pre-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, prefix, keep_vars) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p><hr><h4 id="requires-grad-self-requires-grad-bool-true-self-source-3" tabindex="-1"><code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#requires-grad-self-requires-grad-bool-true-self-source-3" aria-label="Permalink to &quot;`requires_grad_(self, requires_grad: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826)&quot;">​</a></h4><p>Change if autograd should record operations on parameters in this module.</p><p>This method sets the parameters&#39; <code>requires_grad</code> attributes in-place.</p><p>This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).</p><p>有关 <code>.eval()</code> 和其他类似机制的比较， <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.</p><h4 id="args-102" tabindex="-1">Args <a class="header-anchor" href="#args-102" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>requires_grad</strong> (<code>bool</code>): whether autograd should record operations on parameters in this module. 默认值： <code>True</code>.</li></ul><h4 id="returns-94" tabindex="-1">Returns <a class="header-anchor" href="#returns-94" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="set-extra-state-self-state-any-none-source-3" tabindex="-1"><code>set_extra_state(self, state: Any) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-extra-state-self-state-any-none-source-3" aria-label="Permalink to &quot;`set_extra_state(self, state: Any) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938)&quot;">​</a></h4><p>Set extra state contained in the loaded <code>state_dict</code>.</p><p>This function is called from <code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding <code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.</p><h4 id="args-103" tabindex="-1">Args <a class="header-anchor" href="#args-103" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state</strong> (<code>dict</code>): Extra state from the <code>state_dict</code></li></ul><hr><h4 id="set-submodule-self-target-str-module-module-strict-bool-false-none-source-3" tabindex="-1"><code>set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-submodule-self-target-str-module-module-strict-bool-false-none-source-3" aria-label="Permalink to &quot;`set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764)&quot;">​</a></h4><p>Set the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>If <code>strict</code> is set to <code>False</code> (default), the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>strict</code> is set to <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule does not exist.</p></div><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(3, 3, 3) ) (linear): Linear(3, 3) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To override the <code>Conv2d</code> with a new submodule <code>Linear</code>, you could call <code>set_submodule(&quot;net_b.net_c.conv&quot;, nn.Linear(1, 1))</code> where <code>strict</code> could be <code>True</code> or <code>False</code></p><p>To add a new submodule <code>Conv2d</code> to the existing <code>net_b</code> module, you would call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1))</code>.</p><p>In the above if you set <code>strict=True</code> and call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1), strict=True)</code>, an AttributeError will be raised because <code>net_b</code> does not have a submodule named <code>conv</code>.</p><h4 id="args-104" tabindex="-1">Args <a class="header-anchor" href="#args-104" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li><li><strong>module</strong>: The module to set the submodule to.</li><li><strong>strict</strong>: If <code>False</code>, the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule doesn&#39;t already exist.</li></ul><h4 id="raises-15" tabindex="-1">Raises <a class="header-anchor" href="#raises-15" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>ValueError</strong>: If the <code>target</code> string is empty or if <code>module</code> is not an instance of <code>nn.Module</code>.</li><li><strong>AttributeError</strong>: If at any point along the path resulting from the <code>target</code> string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="share-memory-self-self-source-3" tabindex="-1"><code>share_memory(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#share-memory-self-self-source-3" aria-label="Permalink to &quot;`share_memory(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877)&quot;">​</a></h4><p>See <code>tensorplay.Tensor.share_memory_</code>.</p><hr><h4 id="state-dict-self-args-destination-none-prefix-keep-vars-false-source-3" tabindex="-1"><code>state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#state-dict-self-args-destination-none-prefix-keep-vars-false-source-3" aria-label="Permalink to &quot;`state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105)&quot;">​</a></h4><p>Return a dictionary containing references to the whole state of the module.</p><p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>The returned object is a shallow copy. It contains references to the module&#39;s parameters and buffers.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Currently <code>state_dict()</code> also accepts positional arguments for <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However, this is being deprecated and keyword arguments will be enforced in future releases.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Please avoid the use of argument <code>destination</code> as it is not designed for end-users.</p></div><h4 id="args-105" tabindex="-1">Args <a class="header-anchor" href="#args-105" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>destination</strong> (<code>dict, optional</code>): If provided, the state of module will be updated into the dict and the same object is returned. Otherwise, an <code>OrderedDict</code> will be created and returned.</li><li><strong>Default</strong>: <code>None</code>.</li><li><strong>prefix</strong> (<code>str, optional</code>): a prefix added to parameter and buffer names to compose the keys in state_dict. 默认值： <code>&#39;&#39;</code>.</li><li><strong>keep_vars</strong> (<code>bool, optional</code>): by default the <code>~tensorplay.Tensor</code> s returned in the state dict are detached from autograd. If it&#39;s set to <code>True</code>, detaching will not be performed.</li><li><strong>Default</strong>: <code>False</code>.</li></ul><h4 id="returns-95" tabindex="-1">Returns <a class="header-anchor" href="#returns-95" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>dict</strong>: a dictionary containing a whole state of the module</li></ul><h4 id="example-39" tabindex="-1">Example <a class="header-anchor" href="#example-39" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">module.state_dict().keys()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;weight&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><hr><h4 id="to-self-args-kwargs-source-3" tabindex="-1"><code>to(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-self-args-kwargs-source-3" aria-label="Permalink to &quot;`to(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151)&quot;">​</a></h4><p>Move and/or cast the parameters and buffers.</p><p>This can be called as</p><p>.. function:: to(device=None, dtype=None, non_blocking=False) :noindex:</p><p>.. function:: to(dtype, non_blocking=False) :noindex:</p><p>.. function:: to(tensor, non_blocking=False) :noindex:</p><p>.. function:: to(memory_format=tensorplay.channels_last) :noindex:</p><p>Its signature is similar to <code>tensorplay.Tensor.to</code>, but only accepts floating point or complex <code>dtype</code>\ s. In addition, this method will only cast the floating point or complex parameters and buffers to <code>dtype</code> (if given). The integral parameters and buffers will be moved <code>device</code>, if that is given, but with dtypes unchanged. When <code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.</p><p>See below for examples.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="args-106" tabindex="-1">Args <a class="header-anchor" href="#args-106" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): the desired device of the parameters and buffers in this module</li><li><strong>dtype</strong> (<code>tensorplay.dtype</code>): the desired floating point or complex dtype of the parameters and buffers in this module</li><li><strong>tensor</strong> (<code>tensorplay.Tensor</code>): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module</li><li><strong>memory_format</strong> (<code>tensorplay.memory_format</code>): the desired memory format for 4D parameters and buffers in this module (keyword only argument)</li></ul><h4 id="returns-96" tabindex="-1">Returns <a class="header-anchor" href="#returns-96" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="examples-7" tabindex="-1">Examples <a class="header-anchor" href="#examples-7" aria-label="Permalink to &quot;Examples&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +IGNORE_WANT(&quot;non-deterministic&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(tensorplay.double)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float64)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +REQUIRES(env:TENSORPLAY_DOCTEST_CUDA1)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">gpu1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cuda:1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(gpu1, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.half, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">non_blocking</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;cuda:1&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cpu </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cpu&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(cpu)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).to(tensorplay.cdouble)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3741</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,  </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2382</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5593</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.4443</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear(tensorplay.ones(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.cdouble))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br></div></div><hr><h4 id="to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source-3" tabindex="-1"><code>to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source-3" aria-label="Permalink to &quot;`to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119)&quot;">​</a></h4><p>Move the parameters and buffers to the specified device without copying storage.</p><h4 id="args-107" tabindex="-1">Args <a class="header-anchor" href="#args-107" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): The desired device of the parameters and buffers in this module.</li><li><strong>recurse</strong> (<code>bool</code>): Whether parameters and buffers of submodules should be recursively moved to the specified device.</li></ul><h4 id="returns-97" tabindex="-1">Returns <a class="header-anchor" href="#returns-97" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="train-self-mode-bool-true-self-source-3" tabindex="-1"><code>train(self, mode: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#train-self-mode-bool-true-self-source-3" aria-label="Permalink to &quot;`train(self, mode: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786)&quot;">​</a></h4><p>Set the module in training mode.</p><p>这仅对某些模块有影响。请参阅特定模块的文档 了解它们在训练/评估模式下的行为细节， mode, i.e., whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, 等。</p><h4 id="args-108" tabindex="-1">Args <a class="header-anchor" href="#args-108" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>mode</strong> (<code>bool</code>): whether to set training mode (<code>True</code>) or evaluation mode (<code>False</code>). 默认值： <code>True</code>.</li></ul><h4 id="returns-98" tabindex="-1">Returns <a class="header-anchor" href="#returns-98" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="type-self-dst-type-union-tensorplay-dtype-str-self-source-3" tabindex="-1"><code>type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#type-self-dst-type-union-tensorplay-dtype-str-self-source-3" aria-label="Permalink to &quot;`type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061)&quot;">​</a></h4><p>Casts all parameters and buffers to <code>dst_type</code>.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="args-109" tabindex="-1">Args <a class="header-anchor" href="#args-109" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>dst_type</strong> (<code>type or string</code>): the desired type</li></ul><h4 id="returns-99" tabindex="-1">Returns <a class="header-anchor" href="#returns-99" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="zero-grad-self-set-to-none-bool-true-none-source-3" tabindex="-1"><code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#zero-grad-self-set-to-none-bool-true-none-source-3" aria-label="Permalink to &quot;`zero_grad(self, set_to_none: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849)&quot;">​</a></h4><p>Reset gradients of all model parameters.</p><p>See similar function under <code>tensorplay.optim.Optimizer</code> for more context.</p><h4 id="args-110" tabindex="-1">Args <a class="header-anchor" href="#args-110" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>set_to_none</strong> (<code>bool</code>): instead of setting to zero, set the grads to None. See <code>tensorplay.optim.Optimizer.zero_grad</code> for details.</li></ul><hr></details><h3 id="class-tanh-source" tabindex="-1"><code>class Tanh</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L218" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-tanh-source" aria-label="Permalink to &quot;`class Tanh` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L218)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Tanh(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">args, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">**</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">kwargs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>Module</code></p><p>逐元素应用双曲正切 (Tanh) 函数。</p><p>Tanh is defined as:</p><p>.. math</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">\</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)} {\exp(x) + \exp(-x)}</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="shape-4" tabindex="-1">Shape <a class="header-anchor" href="#shape-4" aria-label="Permalink to &quot;Shape&quot;">​</a></h4><ul><li>输入： <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="2.891ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1278 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(389,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(889,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mo>∗</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>, 其中 <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:0.079ex;" xmlns="http://www.w3.org/2000/svg" width="1.131ex" height="0.973ex" role="img" focusable="false" viewBox="0 -465 500 430" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>∗</mo></math></mjx-assistive-mml></mjx-container> 表示任意维数。</li><li>输出： <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="2.891ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1278 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(389,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(889,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mo>∗</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>, 与输入形状相同。</li></ul><h4 id="examples-8" tabindex="-1">Examples <a class="header-anchor" href="#examples-8" aria-label="Permalink to &quot;Examples&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Tanh()</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.randn(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">output </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> m(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><details><summary>Methods</summary><h4 id="init-self-args-kwargs-none-source-1" tabindex="-1"><code>__init__(self, *args, **kwargs) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L507" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-args-kwargs-none-source-1" aria-label="Permalink to &quot;`__init__(self, *args, **kwargs) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L507)&quot;">​</a></h4><p>初始化内部 Module 状态，由 nn.Module 和 ScriptModule 共享。</p><hr><h4 id="add-module-self-name-str-module-optional-forwardref-module-none-source-4" tabindex="-1"><code>add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#add-module-self-name-str-module-optional-forwardref-module-none-source-4" aria-label="Permalink to &quot;`add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667)&quot;">​</a></h4><p>将子模块添加到当前模块。</p><p>可以使用给定名称作为属性访问该模块。</p><h4 id="args-111" tabindex="-1">Args <a class="header-anchor" href="#args-111" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): 子模块的名称。可以使用给定名称 从该模块访问子模块</li><li><strong>module</strong> (<code>Module</code>): 要添加到模块的子模块。</li></ul><hr><h4 id="apply-self-fn-callable-forwardref-module-nonetype-self-source-4" tabindex="-1"><code>apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#apply-self-fn-callable-forwardref-module-nonetype-self-source-4" aria-label="Permalink to &quot;`apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990)&quot;">​</a></h4><p>递归地将 <code>fn</code> 应用于每个子模块（由 <code>.children()</code> 返回）以及自身。</p><p>典型用途包括初始化模型的参数 (see also <code>nn-init-doc</code>).</p><h4 id="args-112" tabindex="-1">Args <a class="header-anchor" href="#args-112" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>fn</strong> (``Module<code> -&gt; None</code>): 应用于每个子模块的函数</li></ul><h4 id="returns-100" tabindex="-1">Returns <a class="header-anchor" href="#returns-100" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="example-40" tabindex="-1">Example <a class="header-anchor" href="#example-40" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@tensorplay.no_grad</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> init_weights</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        m.weight.fill_(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m.weight)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net.apply(init_weights)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><hr><h4 id="bfloat16-self-self-source-4" tabindex="-1"><code>bfloat16(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#bfloat16-self-self-source-4" aria-label="Permalink to &quot;`bfloat16(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108)&quot;">​</a></h4><p>将所有浮点参数和缓冲区转换为 <code>bfloat16</code> 数据类型。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="returns-101" tabindex="-1">Returns <a class="header-anchor" href="#returns-101" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source-4" tabindex="-1"><code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source-4" aria-label="Permalink to &quot;`buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627)&quot;">​</a></h4><p>返回模块缓冲区的迭代器。</p><h4 id="args-113" tabindex="-1">Args <a class="header-anchor" href="#args-113" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): 如果为 True，则生成此模块的缓冲区 以及所有子模块的缓冲区。否则，仅生成 此模块的直接成员缓冲区。</li></ul><h4 id="yields-32" tabindex="-1">Yields <a class="header-anchor" href="#yields-32" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>tensorplay.Tensor: 模块缓冲区
</code></pre><h4 id="example-41" tabindex="-1">Example <a class="header-anchor" href="#example-41" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.buffers():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf), buf.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="children-self-collections-abc-iterator-module-source-4" tabindex="-1"><code>children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#children-self-collections-abc-iterator-module-source-4" aria-label="Permalink to &quot;`children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681)&quot;">​</a></h4><p>返回直接子模块的迭代器。</p><h4 id="yields-33" tabindex="-1">Yields <a class="header-anchor" href="#yields-33" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: 一个子模块</li></ul><hr><h4 id="compile-self-args-kwargs-source-4" tabindex="-1"><code>compile(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#compile-self-args-kwargs-source-4" aria-label="Permalink to &quot;`compile(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944)&quot;">​</a></h4><p>使用 <code>tensorplay.compile</code> 编译此模块的前向传播。</p><p>此模块的 <code>__call__</code> 方法被编译，所有参数按原样传递 给 <code>tensorplay.compile</code>。</p><p>有关此函数的参数详情，请参阅 <code>tensorplay.compile</code>。</p><hr><h4 id="cpu-self-self-source-4" tabindex="-1"><code>cpu(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cpu-self-self-source-4" aria-label="Permalink to &quot;`cpu(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050)&quot;">​</a></h4><p>将所有模型参数和缓冲区移动到 CPU。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="returns-102" tabindex="-1">Returns <a class="header-anchor" href="#returns-102" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source-4" tabindex="-1"><code>cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source-4" aria-label="Permalink to &quot;`cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031)&quot;">​</a></h4><p>将所有模型参数和缓冲区移动到 GPU。</p><p>这也使得关联的参数和缓冲区成为不同的对象。所以 如果模块将在 GPU 上运行并进行优化， 应在构建优化器之前调用它。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="args-114" tabindex="-1">Args <a class="header-anchor" href="#args-114" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>int, optional</code>): 如果指定，所有参数将被 复制到该设备</li></ul><h4 id="returns-103" tabindex="-1">Returns <a class="header-anchor" href="#returns-103" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="double-self-self-source-4" tabindex="-1"><code>double(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#double-self-self-source-4" aria-label="Permalink to &quot;`double(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086)&quot;">​</a></h4><p>将所有浮点参数和缓冲区转换为 <code>double</code> 数据类型。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="returns-104" tabindex="-1">Returns <a class="header-anchor" href="#returns-104" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="eval-self-self-source-4" tabindex="-1"><code>eval(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#eval-self-self-source-4" aria-label="Permalink to &quot;`eval(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808)&quot;">​</a></h4><p>将模块设置为评估模式。</p><p>这仅对某些模块有影响。请参阅特定模块的文档 了解它们在训练/评估模式下的行为细节， 即它们是否受影响，例如 <code>Dropout</code>、<code>BatchNorm</code> 等。</p><p>这相当于 <code>self.train(False) &lt;tensorplay.nn.Module.train&gt;</code>。</p><p>有关 <code>.eval()</code> 和其他类似机制的比较， <code>.eval()</code> and several similar mechanisms that may be confused with it.</p><h4 id="returns-105" tabindex="-1">Returns <a class="header-anchor" href="#returns-105" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="extra-repr-self-str-source-4" tabindex="-1"><code>extra_repr(self) -&gt; str</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2884" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#extra-repr-self-str-source-4" aria-label="Permalink to &quot;`extra_repr(self) -&gt; str` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2884)&quot;">​</a></h4><p>返回模块的额外表示。</p><p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p><hr><h4 id="float-self-self-source-4" tabindex="-1"><code>float(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#float-self-self-source-4" aria-label="Permalink to &quot;`float(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075)&quot;">​</a></h4><p>将所有浮点参数和缓冲区转换为 <code>float</code> 数据类型。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="returns-106" tabindex="-1">Returns <a class="header-anchor" href="#returns-106" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="forward-self-input-tensorplay-c-tensorbase-tensorplay-c-tensorbase-source-4" tabindex="-1"><code>forward(self, input: tensorplay._C.TensorBase) -&gt; tensorplay._C.TensorBase</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L237" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#forward-self-input-tensorplay-c-tensorbase-tensorplay-c-tensorbase-source-4" aria-label="Permalink to &quot;`forward(self, input: tensorplay._C.TensorBase) -&gt; tensorplay._C.TensorBase` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L237)&quot;">​</a></h4><p>运行前向传播。</p><hr><h4 id="get-buffer-self-target-str-tensor-source-4" tabindex="-1"><code>get_buffer(self, target: str) -&gt; &#39;Tensor&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-buffer-self-target-str-tensor-source-4" aria-label="Permalink to &quot;`get_buffer(self, target: str) -&gt; &#39;Tensor&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881)&quot;">​</a></h4><p>如果存在，则返回由 <code>target</code> 指定的缓冲区，否则抛出错误。</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-115" tabindex="-1">Args <a class="header-anchor" href="#args-115" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: 缓冲区的完全限定字符串名称 to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-107" tabindex="-1">Returns <a class="header-anchor" href="#returns-107" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.Tensor: 由 ``target`` 引用的缓冲区
</code></pre><h4 id="raises-16" tabindex="-1">Raises <a class="header-anchor" href="#raises-16" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not a buffer</li></ul><hr><h4 id="get-extra-state-self-any-source-4" tabindex="-1"><code>get_extra_state(self) -&gt; Any</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-extra-state-self-any-source-4" aria-label="Permalink to &quot;`get_extra_state(self) -&gt; Any` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917)&quot;">​</a></h4><p>返回要包含在模块 state_dict 中的任何额外状态。</p><p>Implement this and a corresponding <code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module&#39;s <code>state_dict()</code>.</p><p>Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.</p><h4 id="returns-108" tabindex="-1">Returns <a class="header-anchor" href="#returns-108" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>object</strong>: Any extra state to store in the module&#39;s state_dict</li></ul><hr><h4 id="get-parameter-self-target-str-parameter-source-4" tabindex="-1"><code>get_parameter(self, target: str) -&gt; &#39;Parameter&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-parameter-self-target-str-parameter-source-4" aria-label="Permalink to &quot;`get_parameter(self, target: str) -&gt; &#39;Parameter&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845)&quot;">​</a></h4><p>如果存在，则返回由 <code>target</code> 指定的参数，否则抛出错误。</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-116" tabindex="-1">Args <a class="header-anchor" href="#args-116" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the Parameter to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-109" tabindex="-1">Returns <a class="header-anchor" href="#returns-109" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Parameter: The Parameter referenced by ``target``
</code></pre><h4 id="raises-17" tabindex="-1">Raises <a class="header-anchor" href="#raises-17" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not an <code>nn.Parameter</code></li></ul><hr><h4 id="get-submodule-self-target-str-module-source-4" tabindex="-1"><code>get_submodule(self, target: str) -&gt; &#39;Module&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-submodule-self-target-str-module-source-4" aria-label="Permalink to &quot;`get_submodule(self, target: str) -&gt; &#39;Module&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699)&quot;">​</a></h4><p>Return the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> which has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(&quot;net_b.linear&quot;)</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(&quot;net_b.net_c.conv&quot;)</code>.</p><p>The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.</p><h4 id="args-117" tabindex="-1">Args <a class="header-anchor" href="#args-117" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li></ul><h4 id="returns-110" tabindex="-1">Returns <a class="header-anchor" href="#returns-110" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Module: The submodule referenced by ``target``
</code></pre><h4 id="raises-18" tabindex="-1">Raises <a class="header-anchor" href="#raises-18" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If at any point along the path resulting from the target string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="half-self-self-source-4" tabindex="-1"><code>half(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#half-self-self-source-4" aria-label="Permalink to &quot;`half(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="returns-111" tabindex="-1">Returns <a class="header-anchor" href="#returns-111" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source-4" tabindex="-1"><code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source-4" aria-label="Permalink to &quot;`load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436)&quot;">​</a></h4><p>Copy parameters and buffers from <code>state_dict</code> into this module and its descendants.</p><p>If <code>strict</code> is <code>True</code>, then the keys of <code>state_dict</code> must exactly match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function.</p><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>If <code>assign</code> is <code>True</code> the optimizer must be created after the call to <code>load_state_dict</code> unless <code>~tensorplay.__future__.get_swap_module_params_on_conversion</code> is <code>True</code>.</p></div><h4 id="args-118" tabindex="-1">Args <a class="header-anchor" href="#args-118" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state_dict</strong> (<code>dict</code>): a dict containing parameters and persistent buffers.</li><li><strong>strict</strong> (<code>bool, optional</code>): whether to strictly enforce that the keys in <code>state_dict</code> match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function. 默认值： <code>True</code></li><li><strong>assign</strong> (<code>bool, optional</code>): When set to <code>False</code>, the properties of the tensors in the current module are preserved whereas setting it to <code>True</code> preserves properties of the Tensors in the state dict. The only exception is the <code>requires_grad</code> field of <code>~tensorplay.nn.Parameter</code> for which the value from the module is preserved. 默认值： <code>False</code></li></ul><h4 id="returns-112" tabindex="-1">Returns <a class="header-anchor" href="#returns-112" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:
    * ``missing_keys`` is a list of str containing any keys that are expected
        by this module but missing from the provided ``state_dict``.
    * ``unexpected_keys`` is a list of str containing the keys that are not
        expected by this module but present in the provided ``state_dict``.
</code></pre><h4 id="note-12" tabindex="-1">Note <a class="header-anchor" href="#note-12" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>If a parameter or buffer is registered as ``None`` and its corresponding key
exists in `state_dict`, `load_state_dict` will raise a
``RuntimeError``.
</code></pre><hr><h4 id="modules-self-collections-abc-iterator-module-source-4" tabindex="-1"><code>modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#modules-self-collections-abc-iterator-module-source-4" aria-label="Permalink to &quot;`modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710)&quot;">​</a></h4><p>Return an iterator over all modules in the network.</p><h4 id="yields-34" tabindex="-1">Yields <a class="header-anchor" href="#yields-34" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a module in the network</li></ul><h4 id="note-13" tabindex="-1">Note <a class="header-anchor" href="#note-13" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-42" tabindex="-1">Example <a class="header-anchor" href="#example-42" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source-4" tabindex="-1"><code>named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source-4" aria-label="Permalink to &quot;`named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650)&quot;">​</a></h4><p>Return an iterator over 模块缓冲区s, yielding both the name of the buffer as well as the buffer itself.</p><h4 id="args-119" tabindex="-1">Args <a class="header-anchor" href="#args-119" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all buffer names.</li><li><strong>recurse</strong> (<code>bool, optional</code>): 如果为 True，则生成此模块的缓冲区 以及所有子模块的缓冲区。否则，仅生成 此模块的直接成员缓冲区。 Defaults to True.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated buffers in the result. Defaults to True.</li></ul><h4 id="yields-35" tabindex="-1">Yields <a class="header-anchor" href="#yields-35" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, tensorplay.Tensor): Tuple containing the name and buffer
</code></pre><h4 id="example-43" tabindex="-1">Example <a class="header-anchor" href="#example-43" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_buffers():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_var&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-children-self-collections-abc-iterator-tuple-str-module-source-4" tabindex="-1"><code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-children-self-collections-abc-iterator-tuple-str-module-source-4" aria-label="Permalink to &quot;`named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690)&quot;">​</a></h4><p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p><h4 id="yields-36" tabindex="-1">Yields <a class="header-anchor" href="#yields-36" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple containing a name and child module
</code></pre><h4 id="example-44" tabindex="-1">Example <a class="header-anchor" href="#example-44" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, module </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.named_children():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv4&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv5&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(module)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source-4" tabindex="-1"><code>named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source-4" aria-label="Permalink to &quot;`named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737)&quot;">​</a></h4><p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p><h4 id="args-120" tabindex="-1">Args <a class="header-anchor" href="#args-120" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>memo</strong>: a memo to store the set of modules already added to the result</li><li><strong>prefix</strong>: a prefix that will be added to the name of the module</li><li><strong>remove_duplicate</strong>: whether to remove the duplicated module instances in the result or not</li></ul><h4 id="yields-37" tabindex="-1">Yields <a class="header-anchor" href="#yields-37" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple of name and module
</code></pre><h4 id="note-14" tabindex="-1">Note <a class="header-anchor" href="#note-14" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-45" tabindex="-1">Example <a class="header-anchor" href="#example-45" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.named_modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;0&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source-4" tabindex="-1"><code>named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source-4" aria-label="Permalink to &quot;`named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595)&quot;">​</a></h4><p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p><h4 id="args-121" tabindex="-1">Args <a class="header-anchor" href="#args-121" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all parameter names.</li><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that 此模块的直接成员缓冲区。</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated parameters in the result. Defaults to True.</li></ul><h4 id="yields-38" tabindex="-1">Yields <a class="header-anchor" href="#yields-38" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Parameter): Tuple containing the name and parameter
</code></pre><h4 id="example-46" tabindex="-1">Example <a class="header-anchor" href="#example-46" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_parameters():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source-4" tabindex="-1"><code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source-4" aria-label="Permalink to &quot;`parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570)&quot;">​</a></h4><p>Return an iterator over module parameters.</p><p>This is typically passed to an optimizer.</p><h4 id="args-122" tabindex="-1">Args <a class="header-anchor" href="#args-122" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that 此模块的直接成员缓冲区。</li></ul><h4 id="yields-39" tabindex="-1">Yields <a class="header-anchor" href="#yields-39" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Parameter</strong>: module parameter</li></ul><h4 id="example-47" tabindex="-1">Example <a class="header-anchor" href="#example-47" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.parameters():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param), param.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source-4" tabindex="-1"><code>register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source-4" aria-label="Permalink to &quot;`register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>This function is deprecated in favor of <code>~tensorplay.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.</p><h4 id="returns-113" tabindex="-1">Returns <a class="header-anchor" href="#returns-113" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source-4" tabindex="-1"><code>register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source-4" aria-label="Permalink to &quot;`register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553)&quot;">​</a></h4><p>Add a buffer to the module.</p><p>This is typically used to register a buffer that should not be considered a model parameter. For example, BatchNorm&#39;s <code>running_mean</code> is not a parameter, but is part of the module&#39;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting <code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&#39;s <code>state_dict</code>.</p><p>Buffers can be accessed as attributes using given names.</p><h4 id="args-123" tabindex="-1">Args <a class="header-anchor" href="#args-123" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the buffer. The buffer can be accessed from this module using the given name</li><li><strong>tensor</strong> (<code>Tensor or None</code>): buffer to be registered. If <code>None</code>, then operations that run on buffers, such as <code>cuda</code>, are ignored. If <code>None</code>, the buffer is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li><li><strong>persistent</strong> (<code>bool</code>): whether the buffer is part of this module&#39;s <code>state_dict</code>.</li></ul><h4 id="example-48" tabindex="-1">Example <a class="header-anchor" href="#example-48" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.register_buffer(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_mean&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, tensorplay.zeros(num_features))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><hr><h4 id="register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source-4" tabindex="-1"><code>register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source-4" aria-label="Permalink to &quot;`register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592)&quot;">​</a></h4><p>Register a forward hook on the module.</p><p>The hook will be called every time after <code>forward</code> has computed an output.</p><p>If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after <code>forward</code> is called. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-124" tabindex="-1">Args <a class="header-anchor" href="#args-124" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If <code>True</code>, the provided <code>hook</code> will be fired before all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward</code> hooks registered with <code>register_module_forward_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If <code>True</code>, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>always_call</strong> (<code>bool</code>): If <code>True</code> the <code>hook</code> will be run regardless of whether an exception is raised while calling the Module.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-114" tabindex="-1">Returns <a class="header-anchor" href="#returns-114" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source-4" tabindex="-1"><code>register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source-4" aria-label="Permalink to &quot;`register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526)&quot;">​</a></h4><p>Register a forward pre-hook on the module.</p><p>The hook will be called every time before <code>forward</code> is invoked.</p><p>If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> of modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> and</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> kwargs</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-125" tabindex="-1">Args <a class="header-anchor" href="#args-125" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward_pre</code> hooks registered with <code>register_module_forward_pre_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If true, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-115" tabindex="-1">Returns <a class="header-anchor" href="#returns-115" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-4" tabindex="-1"><code>register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-4" aria-label="Permalink to &quot;`register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>The hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:</p><pre><code>1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.
2. If none of the module inputs require gradients, the hook will fire when the gradients are computed
   with respect to module outputs.
3. If none of the module outputs require gradients, then the hooks will not fire.
</code></pre><p>The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_input, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Tensor) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_input</code> and <code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of <code>grad_input</code> in subsequent computations. <code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in <code>grad_input</code> and <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> outputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-126" tabindex="-1">Args <a class="header-anchor" href="#args-126" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward</code> hooks registered with <code>register_module_full_backward_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-116" tabindex="-1">Returns <a class="header-anchor" href="#returns-116" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-4" tabindex="-1"><code>register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-4" aria-label="Permalink to &quot;`register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287)&quot;">​</a></h4><p>Register a backward pre-hook on the module.</p><p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tuple[Tensor] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of <code>grad_output</code> in subsequent computations. Entries in <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-127" tabindex="-1">Args <a class="header-anchor" href="#args-127" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward_pre</code> hooks registered with <code>register_module_full_backward_pre_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-117" tabindex="-1">Returns <a class="header-anchor" href="#returns-117" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-post-hook-self-hook-source-4" tabindex="-1"><code>register_load_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-post-hook-self-hook-source-4" aria-label="Permalink to &quot;`register_load_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226)&quot;">​</a></h4><p>Register a post-hook to be run after module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, incompatible_keys) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.</p><p>The given incompatible_keys can be modified inplace if needed.</p><p>Note that the checks performed when calling <code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.</p><h4 id="returns-118" tabindex="-1">Returns <a class="header-anchor" href="#returns-118" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-pre-hook-self-hook-source-4" tabindex="-1"><code>register_load_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-pre-hook-self-hook-source-4" aria-label="Permalink to &quot;`register_load_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214)&quot;">​</a></h4><p>Register a pre-hook to be run before module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # noqa: B950</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="arguments-4" tabindex="-1">Arguments <a class="header-anchor" href="#arguments-4" aria-label="Permalink to &quot;Arguments&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): Callable hook that will be invoked before loading the state dict.</li></ul><hr><h4 id="register-module-self-name-str-module-optional-forwardref-module-none-source-4" tabindex="-1"><code>register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-module-self-name-str-module-optional-forwardref-module-none-source-4" aria-label="Permalink to &quot;`register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695)&quot;">​</a></h4><p>Alias for <code>add_module</code>.</p><hr><h4 id="register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source-4" tabindex="-1"><code>register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source-4" aria-label="Permalink to &quot;`register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617)&quot;">​</a></h4><p>Add a parameter to the module.</p><p>The parameter can be accessed as an attribute using given name.</p><h4 id="args-128" tabindex="-1">Args <a class="header-anchor" href="#args-128" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the parameter. The parameter can be accessed from this module using the given name</li><li><strong>param</strong> (<code>Parameter or None</code>): parameter to be added to the module. If <code>None</code>, then operations that run on parameters, such as <code>cuda</code>, are ignored. If <code>None</code>, the parameter is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li></ul><hr><h4 id="register-state-dict-post-hook-self-hook-source-4" tabindex="-1"><code>register_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-post-hook-self-hook-source-4" aria-label="Permalink to &quot;`register_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017)&quot;">​</a></h4><p>Register a post-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can modify the <code>state_dict</code> inplace.</p><hr><h4 id="register-state-dict-pre-hook-self-hook-source-4" tabindex="-1"><code>register_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-pre-hook-self-hook-source-4" aria-label="Permalink to &quot;`register_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041)&quot;">​</a></h4><p>Register a pre-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, prefix, keep_vars) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p><hr><h4 id="requires-grad-self-requires-grad-bool-true-self-source-4" tabindex="-1"><code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#requires-grad-self-requires-grad-bool-true-self-source-4" aria-label="Permalink to &quot;`requires_grad_(self, requires_grad: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826)&quot;">​</a></h4><p>Change if autograd should record operations on parameters in this module.</p><p>This method sets the parameters&#39; <code>requires_grad</code> attributes in-place.</p><p>This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).</p><p>有关 <code>.eval()</code> 和其他类似机制的比较， <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.</p><h4 id="args-129" tabindex="-1">Args <a class="header-anchor" href="#args-129" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>requires_grad</strong> (<code>bool</code>): whether autograd should record operations on parameters in this module. 默认值： <code>True</code>.</li></ul><h4 id="returns-119" tabindex="-1">Returns <a class="header-anchor" href="#returns-119" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="set-extra-state-self-state-any-none-source-4" tabindex="-1"><code>set_extra_state(self, state: Any) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-extra-state-self-state-any-none-source-4" aria-label="Permalink to &quot;`set_extra_state(self, state: Any) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938)&quot;">​</a></h4><p>Set extra state contained in the loaded <code>state_dict</code>.</p><p>This function is called from <code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding <code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.</p><h4 id="args-130" tabindex="-1">Args <a class="header-anchor" href="#args-130" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state</strong> (<code>dict</code>): Extra state from the <code>state_dict</code></li></ul><hr><h4 id="set-submodule-self-target-str-module-module-strict-bool-false-none-source-4" tabindex="-1"><code>set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-submodule-self-target-str-module-module-strict-bool-false-none-source-4" aria-label="Permalink to &quot;`set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764)&quot;">​</a></h4><p>Set the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>If <code>strict</code> is set to <code>False</code> (default), the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>strict</code> is set to <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule does not exist.</p></div><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(3, 3, 3) ) (linear): Linear(3, 3) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To override the <code>Conv2d</code> with a new submodule <code>Linear</code>, you could call <code>set_submodule(&quot;net_b.net_c.conv&quot;, nn.Linear(1, 1))</code> where <code>strict</code> could be <code>True</code> or <code>False</code></p><p>To add a new submodule <code>Conv2d</code> to the existing <code>net_b</code> module, you would call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1))</code>.</p><p>In the above if you set <code>strict=True</code> and call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1), strict=True)</code>, an AttributeError will be raised because <code>net_b</code> does not have a submodule named <code>conv</code>.</p><h4 id="args-131" tabindex="-1">Args <a class="header-anchor" href="#args-131" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li><li><strong>module</strong>: The module to set the submodule to.</li><li><strong>strict</strong>: If <code>False</code>, the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule doesn&#39;t already exist.</li></ul><h4 id="raises-19" tabindex="-1">Raises <a class="header-anchor" href="#raises-19" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>ValueError</strong>: If the <code>target</code> string is empty or if <code>module</code> is not an instance of <code>nn.Module</code>.</li><li><strong>AttributeError</strong>: If at any point along the path resulting from the <code>target</code> string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="share-memory-self-self-source-4" tabindex="-1"><code>share_memory(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#share-memory-self-self-source-4" aria-label="Permalink to &quot;`share_memory(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877)&quot;">​</a></h4><p>See <code>tensorplay.Tensor.share_memory_</code>.</p><hr><h4 id="state-dict-self-args-destination-none-prefix-keep-vars-false-source-4" tabindex="-1"><code>state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#state-dict-self-args-destination-none-prefix-keep-vars-false-source-4" aria-label="Permalink to &quot;`state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105)&quot;">​</a></h4><p>Return a dictionary containing references to the whole state of the module.</p><p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>The returned object is a shallow copy. It contains references to the module&#39;s parameters and buffers.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Currently <code>state_dict()</code> also accepts positional arguments for <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However, this is being deprecated and keyword arguments will be enforced in future releases.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Please avoid the use of argument <code>destination</code> as it is not designed for end-users.</p></div><h4 id="args-132" tabindex="-1">Args <a class="header-anchor" href="#args-132" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>destination</strong> (<code>dict, optional</code>): If provided, the state of module will be updated into the dict and the same object is returned. Otherwise, an <code>OrderedDict</code> will be created and returned.</li><li><strong>Default</strong>: <code>None</code>.</li><li><strong>prefix</strong> (<code>str, optional</code>): a prefix added to parameter and buffer names to compose the keys in state_dict. 默认值： <code>&#39;&#39;</code>.</li><li><strong>keep_vars</strong> (<code>bool, optional</code>): by default the <code>~tensorplay.Tensor</code> s returned in the state dict are detached from autograd. If it&#39;s set to <code>True</code>, detaching will not be performed.</li><li><strong>Default</strong>: <code>False</code>.</li></ul><h4 id="returns-120" tabindex="-1">Returns <a class="header-anchor" href="#returns-120" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>dict</strong>: a dictionary containing a whole state of the module</li></ul><h4 id="example-49" tabindex="-1">Example <a class="header-anchor" href="#example-49" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">module.state_dict().keys()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;weight&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><hr><h4 id="to-self-args-kwargs-source-4" tabindex="-1"><code>to(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-self-args-kwargs-source-4" aria-label="Permalink to &quot;`to(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151)&quot;">​</a></h4><p>Move and/or cast the parameters and buffers.</p><p>This can be called as</p><p>.. function:: to(device=None, dtype=None, non_blocking=False) :noindex:</p><p>.. function:: to(dtype, non_blocking=False) :noindex:</p><p>.. function:: to(tensor, non_blocking=False) :noindex:</p><p>.. function:: to(memory_format=tensorplay.channels_last) :noindex:</p><p>Its signature is similar to <code>tensorplay.Tensor.to</code>, but only accepts floating point or complex <code>dtype</code>\ s. In addition, this method will only cast the floating point or complex parameters and buffers to <code>dtype</code> (if given). The integral parameters and buffers will be moved <code>device</code>, if that is given, but with dtypes unchanged. When <code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.</p><p>See below for examples.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="args-133" tabindex="-1">Args <a class="header-anchor" href="#args-133" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): the desired device of the parameters and buffers in this module</li><li><strong>dtype</strong> (<code>tensorplay.dtype</code>): the desired floating point or complex dtype of the parameters and buffers in this module</li><li><strong>tensor</strong> (<code>tensorplay.Tensor</code>): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module</li><li><strong>memory_format</strong> (<code>tensorplay.memory_format</code>): the desired memory format for 4D parameters and buffers in this module (keyword only argument)</li></ul><h4 id="returns-121" tabindex="-1">Returns <a class="header-anchor" href="#returns-121" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="examples-9" tabindex="-1">Examples <a class="header-anchor" href="#examples-9" aria-label="Permalink to &quot;Examples&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +IGNORE_WANT(&quot;non-deterministic&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(tensorplay.double)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float64)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +REQUIRES(env:TENSORPLAY_DOCTEST_CUDA1)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">gpu1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cuda:1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(gpu1, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.half, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">non_blocking</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;cuda:1&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cpu </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cpu&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(cpu)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).to(tensorplay.cdouble)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3741</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,  </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2382</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5593</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.4443</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear(tensorplay.ones(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.cdouble))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br></div></div><hr><h4 id="to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source-4" tabindex="-1"><code>to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source-4" aria-label="Permalink to &quot;`to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119)&quot;">​</a></h4><p>Move the parameters and buffers to the specified device without copying storage.</p><h4 id="args-134" tabindex="-1">Args <a class="header-anchor" href="#args-134" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): The desired device of the parameters and buffers in this module.</li><li><strong>recurse</strong> (<code>bool</code>): Whether parameters and buffers of submodules should be recursively moved to the specified device.</li></ul><h4 id="returns-122" tabindex="-1">Returns <a class="header-anchor" href="#returns-122" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="train-self-mode-bool-true-self-source-4" tabindex="-1"><code>train(self, mode: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#train-self-mode-bool-true-self-source-4" aria-label="Permalink to &quot;`train(self, mode: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786)&quot;">​</a></h4><p>Set the module in training mode.</p><p>这仅对某些模块有影响。请参阅特定模块的文档 了解它们在训练/评估模式下的行为细节， mode, i.e., whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, 等。</p><h4 id="args-135" tabindex="-1">Args <a class="header-anchor" href="#args-135" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>mode</strong> (<code>bool</code>): whether to set training mode (<code>True</code>) or evaluation mode (<code>False</code>). 默认值： <code>True</code>.</li></ul><h4 id="returns-123" tabindex="-1">Returns <a class="header-anchor" href="#returns-123" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="type-self-dst-type-union-tensorplay-dtype-str-self-source-4" tabindex="-1"><code>type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#type-self-dst-type-union-tensorplay-dtype-str-self-source-4" aria-label="Permalink to &quot;`type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061)&quot;">​</a></h4><p>Casts all parameters and buffers to <code>dst_type</code>.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="args-136" tabindex="-1">Args <a class="header-anchor" href="#args-136" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>dst_type</strong> (<code>type or string</code>): the desired type</li></ul><h4 id="returns-124" tabindex="-1">Returns <a class="header-anchor" href="#returns-124" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="zero-grad-self-set-to-none-bool-true-none-source-4" tabindex="-1"><code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#zero-grad-self-set-to-none-bool-true-none-source-4" aria-label="Permalink to &quot;`zero_grad(self, set_to_none: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849)&quot;">​</a></h4><p>Reset gradients of all model parameters.</p><p>See similar function under <code>tensorplay.optim.Optimizer</code> for more context.</p><h4 id="args-137" tabindex="-1">Args <a class="header-anchor" href="#args-137" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>set_to_none</strong> (<code>bool</code>): instead of setting to zero, set the grads to None. See <code>tensorplay.optim.Optimizer.zero_grad</code> for details.</li></ul><hr></details><h3 id="class-threshold-source" tabindex="-1"><code>class Threshold</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L19" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-threshold-source" aria-label="Permalink to &quot;`class Threshold` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L19)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Threshold(threshold: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">float</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, value: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">float</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, inplace: </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bool</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>Module</code></p><p>对输入张量的每个元素进行阈值处理。</p><p>Threshold is defined as:</p><p>.. math</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">\</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">begin{cases}</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">x, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&amp;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">\</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">text{ if } x &gt; \text{threshold} \\</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">\</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">text{value}, &amp;\text{ otherwise }</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">\</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">end{cases}</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><h4 id="args-138" tabindex="-1">Args <a class="header-anchor" href="#args-138" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>threshold</strong>: The value to threshold at</li><li><strong>value</strong>: The value to replace with</li><li><strong>inplace</strong>: can optionally do the operation in-place. 默认值： <code>False</code></li></ul><h4 id="shape-5" tabindex="-1">Shape <a class="header-anchor" href="#shape-5" aria-label="Permalink to &quot;Shape&quot;">​</a></h4><ul><li>输入： <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="2.891ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1278 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(389,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(889,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mo>∗</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>, 其中 <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:0.079ex;" xmlns="http://www.w3.org/2000/svg" width="1.131ex" height="0.973ex" role="img" focusable="false" viewBox="0 -465 500 430" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>∗</mo></math></mjx-assistive-mml></mjx-container> 表示任意维数。</li><li>输出： <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="2.891ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1278 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(389,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(889,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mo>∗</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>, 与输入形状相同。</li></ul><h4 id="examples-10" tabindex="-1">Examples <a class="header-anchor" href="#examples-10" aria-label="Permalink to &quot;Examples&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.nn.Threshold(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.arange(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">output </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> m(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><details><summary>Methods</summary><h4 id="init-self-threshold-float-value-float-inplace-bool-false-none-source" tabindex="-1"><code>__init__(self, threshold: float, value: float, inplace: bool = False) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L53" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-threshold-float-value-float-inplace-bool-false-none-source" aria-label="Permalink to &quot;`__init__(self, threshold: float, value: float, inplace: bool = False) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L53)&quot;">​</a></h4><p>初始化内部 Module 状态，由 nn.Module 和 ScriptModule 共享。</p><hr><h4 id="add-module-self-name-str-module-optional-forwardref-module-none-source-5" tabindex="-1"><code>add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#add-module-self-name-str-module-optional-forwardref-module-none-source-5" aria-label="Permalink to &quot;`add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667)&quot;">​</a></h4><p>将子模块添加到当前模块。</p><p>可以使用给定名称作为属性访问该模块。</p><h4 id="args-139" tabindex="-1">Args <a class="header-anchor" href="#args-139" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): 子模块的名称。可以使用给定名称 从该模块访问子模块</li><li><strong>module</strong> (<code>Module</code>): 要添加到模块的子模块。</li></ul><hr><h4 id="apply-self-fn-callable-forwardref-module-nonetype-self-source-5" tabindex="-1"><code>apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#apply-self-fn-callable-forwardref-module-nonetype-self-source-5" aria-label="Permalink to &quot;`apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990)&quot;">​</a></h4><p>递归地将 <code>fn</code> 应用于每个子模块（由 <code>.children()</code> 返回）以及自身。</p><p>典型用途包括初始化模型的参数 (see also <code>nn-init-doc</code>).</p><h4 id="args-140" tabindex="-1">Args <a class="header-anchor" href="#args-140" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>fn</strong> (``Module<code> -&gt; None</code>): 应用于每个子模块的函数</li></ul><h4 id="returns-125" tabindex="-1">Returns <a class="header-anchor" href="#returns-125" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="example-50" tabindex="-1">Example <a class="header-anchor" href="#example-50" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@tensorplay.no_grad</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> init_weights</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        m.weight.fill_(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m.weight)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net.apply(init_weights)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><hr><h4 id="bfloat16-self-self-source-5" tabindex="-1"><code>bfloat16(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#bfloat16-self-self-source-5" aria-label="Permalink to &quot;`bfloat16(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108)&quot;">​</a></h4><p>将所有浮点参数和缓冲区转换为 <code>bfloat16</code> 数据类型。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="returns-126" tabindex="-1">Returns <a class="header-anchor" href="#returns-126" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source-5" tabindex="-1"><code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source-5" aria-label="Permalink to &quot;`buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627)&quot;">​</a></h4><p>返回模块缓冲区的迭代器。</p><h4 id="args-141" tabindex="-1">Args <a class="header-anchor" href="#args-141" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): 如果为 True，则生成此模块的缓冲区 以及所有子模块的缓冲区。否则，仅生成 此模块的直接成员缓冲区。</li></ul><h4 id="yields-40" tabindex="-1">Yields <a class="header-anchor" href="#yields-40" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>tensorplay.Tensor: 模块缓冲区
</code></pre><h4 id="example-51" tabindex="-1">Example <a class="header-anchor" href="#example-51" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.buffers():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf), buf.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="children-self-collections-abc-iterator-module-source-5" tabindex="-1"><code>children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#children-self-collections-abc-iterator-module-source-5" aria-label="Permalink to &quot;`children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681)&quot;">​</a></h4><p>返回直接子模块的迭代器。</p><h4 id="yields-41" tabindex="-1">Yields <a class="header-anchor" href="#yields-41" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: 一个子模块</li></ul><hr><h4 id="compile-self-args-kwargs-source-5" tabindex="-1"><code>compile(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#compile-self-args-kwargs-source-5" aria-label="Permalink to &quot;`compile(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944)&quot;">​</a></h4><p>使用 <code>tensorplay.compile</code> 编译此模块的前向传播。</p><p>此模块的 <code>__call__</code> 方法被编译，所有参数按原样传递 给 <code>tensorplay.compile</code>。</p><p>有关此函数的参数详情，请参阅 <code>tensorplay.compile</code>。</p><hr><h4 id="cpu-self-self-source-5" tabindex="-1"><code>cpu(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cpu-self-self-source-5" aria-label="Permalink to &quot;`cpu(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050)&quot;">​</a></h4><p>将所有模型参数和缓冲区移动到 CPU。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="returns-127" tabindex="-1">Returns <a class="header-anchor" href="#returns-127" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source-5" tabindex="-1"><code>cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source-5" aria-label="Permalink to &quot;`cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031)&quot;">​</a></h4><p>将所有模型参数和缓冲区移动到 GPU。</p><p>这也使得关联的参数和缓冲区成为不同的对象。所以 如果模块将在 GPU 上运行并进行优化， 应在构建优化器之前调用它。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="args-142" tabindex="-1">Args <a class="header-anchor" href="#args-142" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>int, optional</code>): 如果指定，所有参数将被 复制到该设备</li></ul><h4 id="returns-128" tabindex="-1">Returns <a class="header-anchor" href="#returns-128" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="double-self-self-source-5" tabindex="-1"><code>double(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#double-self-self-source-5" aria-label="Permalink to &quot;`double(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086)&quot;">​</a></h4><p>将所有浮点参数和缓冲区转换为 <code>double</code> 数据类型。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="returns-129" tabindex="-1">Returns <a class="header-anchor" href="#returns-129" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="eval-self-self-source-5" tabindex="-1"><code>eval(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#eval-self-self-source-5" aria-label="Permalink to &quot;`eval(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808)&quot;">​</a></h4><p>将模块设置为评估模式。</p><p>这仅对某些模块有影响。请参阅特定模块的文档 了解它们在训练/评估模式下的行为细节， 即它们是否受影响，例如 <code>Dropout</code>、<code>BatchNorm</code> 等。</p><p>这相当于 <code>self.train(False) &lt;tensorplay.nn.Module.train&gt;</code>。</p><p>有关 <code>.eval()</code> 和其他类似机制的比较， <code>.eval()</code> and several similar mechanisms that may be confused with it.</p><h4 id="returns-130" tabindex="-1">Returns <a class="header-anchor" href="#returns-130" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="extra-repr-self-str-source-5" tabindex="-1"><code>extra_repr(self) -&gt; str</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2884" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#extra-repr-self-str-source-5" aria-label="Permalink to &quot;`extra_repr(self) -&gt; str` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2884)&quot;">​</a></h4><p>返回模块的额外表示。</p><p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p><hr><h4 id="float-self-self-source-5" tabindex="-1"><code>float(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#float-self-self-source-5" aria-label="Permalink to &quot;`float(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075)&quot;">​</a></h4><p>将所有浮点参数和缓冲区转换为 <code>float</code> 数据类型。</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="returns-131" tabindex="-1">Returns <a class="header-anchor" href="#returns-131" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="forward-self-input-tensorplay-c-tensorbase-tensorplay-c-tensorbase-source-5" tabindex="-1"><code>forward(self, input: tensorplay._C.TensorBase) -&gt; tensorplay._C.TensorBase</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L59" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#forward-self-input-tensorplay-c-tensorbase-tensorplay-c-tensorbase-source-5" aria-label="Permalink to &quot;`forward(self, input: tensorplay._C.TensorBase) -&gt; tensorplay._C.TensorBase` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/activation.py#L59)&quot;">​</a></h4><p>运行前向传播。</p><hr><h4 id="get-buffer-self-target-str-tensor-source-5" tabindex="-1"><code>get_buffer(self, target: str) -&gt; &#39;Tensor&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-buffer-self-target-str-tensor-source-5" aria-label="Permalink to &quot;`get_buffer(self, target: str) -&gt; &#39;Tensor&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881)&quot;">​</a></h4><p>如果存在，则返回由 <code>target</code> 指定的缓冲区，否则抛出错误。</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-143" tabindex="-1">Args <a class="header-anchor" href="#args-143" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: 缓冲区的完全限定字符串名称 to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-132" tabindex="-1">Returns <a class="header-anchor" href="#returns-132" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.Tensor: 由 ``target`` 引用的缓冲区
</code></pre><h4 id="raises-20" tabindex="-1">Raises <a class="header-anchor" href="#raises-20" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not a buffer</li></ul><hr><h4 id="get-extra-state-self-any-source-5" tabindex="-1"><code>get_extra_state(self) -&gt; Any</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-extra-state-self-any-source-5" aria-label="Permalink to &quot;`get_extra_state(self) -&gt; Any` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917)&quot;">​</a></h4><p>返回要包含在模块 state_dict 中的任何额外状态。</p><p>Implement this and a corresponding <code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module&#39;s <code>state_dict()</code>.</p><p>Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.</p><h4 id="returns-133" tabindex="-1">Returns <a class="header-anchor" href="#returns-133" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>object</strong>: Any extra state to store in the module&#39;s state_dict</li></ul><hr><h4 id="get-parameter-self-target-str-parameter-source-5" tabindex="-1"><code>get_parameter(self, target: str) -&gt; &#39;Parameter&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-parameter-self-target-str-parameter-source-5" aria-label="Permalink to &quot;`get_parameter(self, target: str) -&gt; &#39;Parameter&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845)&quot;">​</a></h4><p>如果存在，则返回由 <code>target</code> 指定的参数，否则抛出错误。</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-144" tabindex="-1">Args <a class="header-anchor" href="#args-144" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the Parameter to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-134" tabindex="-1">Returns <a class="header-anchor" href="#returns-134" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Parameter: The Parameter referenced by ``target``
</code></pre><h4 id="raises-21" tabindex="-1">Raises <a class="header-anchor" href="#raises-21" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not an <code>nn.Parameter</code></li></ul><hr><h4 id="get-submodule-self-target-str-module-source-5" tabindex="-1"><code>get_submodule(self, target: str) -&gt; &#39;Module&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-submodule-self-target-str-module-source-5" aria-label="Permalink to &quot;`get_submodule(self, target: str) -&gt; &#39;Module&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699)&quot;">​</a></h4><p>Return the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> which has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(&quot;net_b.linear&quot;)</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(&quot;net_b.net_c.conv&quot;)</code>.</p><p>The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.</p><h4 id="args-145" tabindex="-1">Args <a class="header-anchor" href="#args-145" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li></ul><h4 id="returns-135" tabindex="-1">Returns <a class="header-anchor" href="#returns-135" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Module: The submodule referenced by ``target``
</code></pre><h4 id="raises-22" tabindex="-1">Raises <a class="header-anchor" href="#raises-22" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If at any point along the path resulting from the target string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="half-self-self-source-5" tabindex="-1"><code>half(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#half-self-self-source-5" aria-label="Permalink to &quot;`half(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="returns-136" tabindex="-1">Returns <a class="header-anchor" href="#returns-136" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source-5" tabindex="-1"><code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source-5" aria-label="Permalink to &quot;`load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436)&quot;">​</a></h4><p>Copy parameters and buffers from <code>state_dict</code> into this module and its descendants.</p><p>If <code>strict</code> is <code>True</code>, then the keys of <code>state_dict</code> must exactly match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function.</p><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>If <code>assign</code> is <code>True</code> the optimizer must be created after the call to <code>load_state_dict</code> unless <code>~tensorplay.__future__.get_swap_module_params_on_conversion</code> is <code>True</code>.</p></div><h4 id="args-146" tabindex="-1">Args <a class="header-anchor" href="#args-146" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state_dict</strong> (<code>dict</code>): a dict containing parameters and persistent buffers.</li><li><strong>strict</strong> (<code>bool, optional</code>): whether to strictly enforce that the keys in <code>state_dict</code> match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function. 默认值： <code>True</code></li><li><strong>assign</strong> (<code>bool, optional</code>): When set to <code>False</code>, the properties of the tensors in the current module are preserved whereas setting it to <code>True</code> preserves properties of the Tensors in the state dict. The only exception is the <code>requires_grad</code> field of <code>~tensorplay.nn.Parameter</code> for which the value from the module is preserved. 默认值： <code>False</code></li></ul><h4 id="returns-137" tabindex="-1">Returns <a class="header-anchor" href="#returns-137" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:
    * ``missing_keys`` is a list of str containing any keys that are expected
        by this module but missing from the provided ``state_dict``.
    * ``unexpected_keys`` is a list of str containing the keys that are not
        expected by this module but present in the provided ``state_dict``.
</code></pre><h4 id="note-15" tabindex="-1">Note <a class="header-anchor" href="#note-15" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>If a parameter or buffer is registered as ``None`` and its corresponding key
exists in `state_dict`, `load_state_dict` will raise a
``RuntimeError``.
</code></pre><hr><h4 id="modules-self-collections-abc-iterator-module-source-5" tabindex="-1"><code>modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#modules-self-collections-abc-iterator-module-source-5" aria-label="Permalink to &quot;`modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710)&quot;">​</a></h4><p>Return an iterator over all modules in the network.</p><h4 id="yields-42" tabindex="-1">Yields <a class="header-anchor" href="#yields-42" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a module in the network</li></ul><h4 id="note-16" tabindex="-1">Note <a class="header-anchor" href="#note-16" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-52" tabindex="-1">Example <a class="header-anchor" href="#example-52" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source-5" tabindex="-1"><code>named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source-5" aria-label="Permalink to &quot;`named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650)&quot;">​</a></h4><p>Return an iterator over 模块缓冲区s, yielding both the name of the buffer as well as the buffer itself.</p><h4 id="args-147" tabindex="-1">Args <a class="header-anchor" href="#args-147" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all buffer names.</li><li><strong>recurse</strong> (<code>bool, optional</code>): 如果为 True，则生成此模块的缓冲区 以及所有子模块的缓冲区。否则，仅生成 此模块的直接成员缓冲区。 Defaults to True.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated buffers in the result. Defaults to True.</li></ul><h4 id="yields-43" tabindex="-1">Yields <a class="header-anchor" href="#yields-43" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, tensorplay.Tensor): Tuple containing the name and buffer
</code></pre><h4 id="example-53" tabindex="-1">Example <a class="header-anchor" href="#example-53" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_buffers():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_var&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-children-self-collections-abc-iterator-tuple-str-module-source-5" tabindex="-1"><code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-children-self-collections-abc-iterator-tuple-str-module-source-5" aria-label="Permalink to &quot;`named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690)&quot;">​</a></h4><p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p><h4 id="yields-44" tabindex="-1">Yields <a class="header-anchor" href="#yields-44" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple containing a name and child module
</code></pre><h4 id="example-54" tabindex="-1">Example <a class="header-anchor" href="#example-54" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, module </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.named_children():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv4&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv5&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(module)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source-5" tabindex="-1"><code>named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source-5" aria-label="Permalink to &quot;`named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737)&quot;">​</a></h4><p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p><h4 id="args-148" tabindex="-1">Args <a class="header-anchor" href="#args-148" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>memo</strong>: a memo to store the set of modules already added to the result</li><li><strong>prefix</strong>: a prefix that will be added to the name of the module</li><li><strong>remove_duplicate</strong>: whether to remove the duplicated module instances in the result or not</li></ul><h4 id="yields-45" tabindex="-1">Yields <a class="header-anchor" href="#yields-45" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple of name and module
</code></pre><h4 id="note-17" tabindex="-1">Note <a class="header-anchor" href="#note-17" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-55" tabindex="-1">Example <a class="header-anchor" href="#example-55" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.named_modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;0&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source-5" tabindex="-1"><code>named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source-5" aria-label="Permalink to &quot;`named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595)&quot;">​</a></h4><p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p><h4 id="args-149" tabindex="-1">Args <a class="header-anchor" href="#args-149" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all parameter names.</li><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that 此模块的直接成员缓冲区。</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated parameters in the result. Defaults to True.</li></ul><h4 id="yields-46" tabindex="-1">Yields <a class="header-anchor" href="#yields-46" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Parameter): Tuple containing the name and parameter
</code></pre><h4 id="example-56" tabindex="-1">Example <a class="header-anchor" href="#example-56" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_parameters():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source-5" tabindex="-1"><code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source-5" aria-label="Permalink to &quot;`parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570)&quot;">​</a></h4><p>Return an iterator over module parameters.</p><p>This is typically passed to an optimizer.</p><h4 id="args-150" tabindex="-1">Args <a class="header-anchor" href="#args-150" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that 此模块的直接成员缓冲区。</li></ul><h4 id="yields-47" tabindex="-1">Yields <a class="header-anchor" href="#yields-47" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Parameter</strong>: module parameter</li></ul><h4 id="example-57" tabindex="-1">Example <a class="header-anchor" href="#example-57" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.parameters():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param), param.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source-5" tabindex="-1"><code>register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source-5" aria-label="Permalink to &quot;`register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>This function is deprecated in favor of <code>~tensorplay.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.</p><h4 id="returns-138" tabindex="-1">Returns <a class="header-anchor" href="#returns-138" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source-5" tabindex="-1"><code>register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source-5" aria-label="Permalink to &quot;`register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553)&quot;">​</a></h4><p>Add a buffer to the module.</p><p>This is typically used to register a buffer that should not be considered a model parameter. For example, BatchNorm&#39;s <code>running_mean</code> is not a parameter, but is part of the module&#39;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting <code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&#39;s <code>state_dict</code>.</p><p>Buffers can be accessed as attributes using given names.</p><h4 id="args-151" tabindex="-1">Args <a class="header-anchor" href="#args-151" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the buffer. The buffer can be accessed from this module using the given name</li><li><strong>tensor</strong> (<code>Tensor or None</code>): buffer to be registered. If <code>None</code>, then operations that run on buffers, such as <code>cuda</code>, are ignored. If <code>None</code>, the buffer is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li><li><strong>persistent</strong> (<code>bool</code>): whether the buffer is part of this module&#39;s <code>state_dict</code>.</li></ul><h4 id="example-58" tabindex="-1">Example <a class="header-anchor" href="#example-58" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.register_buffer(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_mean&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, tensorplay.zeros(num_features))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><hr><h4 id="register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source-5" tabindex="-1"><code>register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source-5" aria-label="Permalink to &quot;`register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592)&quot;">​</a></h4><p>Register a forward hook on the module.</p><p>The hook will be called every time after <code>forward</code> has computed an output.</p><p>If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after <code>forward</code> is called. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-152" tabindex="-1">Args <a class="header-anchor" href="#args-152" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If <code>True</code>, the provided <code>hook</code> will be fired before all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward</code> hooks registered with <code>register_module_forward_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If <code>True</code>, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>always_call</strong> (<code>bool</code>): If <code>True</code> the <code>hook</code> will be run regardless of whether an exception is raised while calling the Module.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-139" tabindex="-1">Returns <a class="header-anchor" href="#returns-139" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source-5" tabindex="-1"><code>register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source-5" aria-label="Permalink to &quot;`register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526)&quot;">​</a></h4><p>Register a forward pre-hook on the module.</p><p>The hook will be called every time before <code>forward</code> is invoked.</p><p>If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> of modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> and</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> kwargs</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-153" tabindex="-1">Args <a class="header-anchor" href="#args-153" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward_pre</code> hooks registered with <code>register_module_forward_pre_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If true, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-140" tabindex="-1">Returns <a class="header-anchor" href="#returns-140" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-5" tabindex="-1"><code>register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-5" aria-label="Permalink to &quot;`register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>The hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:</p><pre><code>1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.
2. If none of the module inputs require gradients, the hook will fire when the gradients are computed
   with respect to module outputs.
3. If none of the module outputs require gradients, then the hooks will not fire.
</code></pre><p>The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_input, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Tensor) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_input</code> and <code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of <code>grad_input</code> in subsequent computations. <code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in <code>grad_input</code> and <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> outputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-154" tabindex="-1">Args <a class="header-anchor" href="#args-154" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward</code> hooks registered with <code>register_module_full_backward_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-141" tabindex="-1">Returns <a class="header-anchor" href="#returns-141" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-5" tabindex="-1"><code>register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-5" aria-label="Permalink to &quot;`register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287)&quot;">​</a></h4><p>Register a backward pre-hook on the module.</p><p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tuple[Tensor] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of <code>grad_output</code> in subsequent computations. Entries in <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-155" tabindex="-1">Args <a class="header-anchor" href="#args-155" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward_pre</code> hooks registered with <code>register_module_full_backward_pre_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-142" tabindex="-1">Returns <a class="header-anchor" href="#returns-142" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-post-hook-self-hook-source-5" tabindex="-1"><code>register_load_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-post-hook-self-hook-source-5" aria-label="Permalink to &quot;`register_load_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226)&quot;">​</a></h4><p>Register a post-hook to be run after module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, incompatible_keys) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.</p><p>The given incompatible_keys can be modified inplace if needed.</p><p>Note that the checks performed when calling <code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.</p><h4 id="returns-143" tabindex="-1">Returns <a class="header-anchor" href="#returns-143" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-pre-hook-self-hook-source-5" tabindex="-1"><code>register_load_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-pre-hook-self-hook-source-5" aria-label="Permalink to &quot;`register_load_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214)&quot;">​</a></h4><p>Register a pre-hook to be run before module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # noqa: B950</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="arguments-5" tabindex="-1">Arguments <a class="header-anchor" href="#arguments-5" aria-label="Permalink to &quot;Arguments&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): Callable hook that will be invoked before loading the state dict.</li></ul><hr><h4 id="register-module-self-name-str-module-optional-forwardref-module-none-source-5" tabindex="-1"><code>register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-module-self-name-str-module-optional-forwardref-module-none-source-5" aria-label="Permalink to &quot;`register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695)&quot;">​</a></h4><p>Alias for <code>add_module</code>.</p><hr><h4 id="register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source-5" tabindex="-1"><code>register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source-5" aria-label="Permalink to &quot;`register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617)&quot;">​</a></h4><p>Add a parameter to the module.</p><p>The parameter can be accessed as an attribute using given name.</p><h4 id="args-156" tabindex="-1">Args <a class="header-anchor" href="#args-156" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the parameter. The parameter can be accessed from this module using the given name</li><li><strong>param</strong> (<code>Parameter or None</code>): parameter to be added to the module. If <code>None</code>, then operations that run on parameters, such as <code>cuda</code>, are ignored. If <code>None</code>, the parameter is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li></ul><hr><h4 id="register-state-dict-post-hook-self-hook-source-5" tabindex="-1"><code>register_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-post-hook-self-hook-source-5" aria-label="Permalink to &quot;`register_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017)&quot;">​</a></h4><p>Register a post-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can modify the <code>state_dict</code> inplace.</p><hr><h4 id="register-state-dict-pre-hook-self-hook-source-5" tabindex="-1"><code>register_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-pre-hook-self-hook-source-5" aria-label="Permalink to &quot;`register_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041)&quot;">​</a></h4><p>Register a pre-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, prefix, keep_vars) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p><hr><h4 id="requires-grad-self-requires-grad-bool-true-self-source-5" tabindex="-1"><code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#requires-grad-self-requires-grad-bool-true-self-source-5" aria-label="Permalink to &quot;`requires_grad_(self, requires_grad: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826)&quot;">​</a></h4><p>Change if autograd should record operations on parameters in this module.</p><p>This method sets the parameters&#39; <code>requires_grad</code> attributes in-place.</p><p>This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).</p><p>有关 <code>.eval()</code> 和其他类似机制的比较， <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.</p><h4 id="args-157" tabindex="-1">Args <a class="header-anchor" href="#args-157" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>requires_grad</strong> (<code>bool</code>): whether autograd should record operations on parameters in this module. 默认值： <code>True</code>.</li></ul><h4 id="returns-144" tabindex="-1">Returns <a class="header-anchor" href="#returns-144" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="set-extra-state-self-state-any-none-source-5" tabindex="-1"><code>set_extra_state(self, state: Any) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-extra-state-self-state-any-none-source-5" aria-label="Permalink to &quot;`set_extra_state(self, state: Any) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938)&quot;">​</a></h4><p>Set extra state contained in the loaded <code>state_dict</code>.</p><p>This function is called from <code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding <code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.</p><h4 id="args-158" tabindex="-1">Args <a class="header-anchor" href="#args-158" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state</strong> (<code>dict</code>): Extra state from the <code>state_dict</code></li></ul><hr><h4 id="set-submodule-self-target-str-module-module-strict-bool-false-none-source-5" tabindex="-1"><code>set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-submodule-self-target-str-module-module-strict-bool-false-none-source-5" aria-label="Permalink to &quot;`set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764)&quot;">​</a></h4><p>Set the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>If <code>strict</code> is set to <code>False</code> (default), the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>strict</code> is set to <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule does not exist.</p></div><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(3, 3, 3) ) (linear): Linear(3, 3) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To override the <code>Conv2d</code> with a new submodule <code>Linear</code>, you could call <code>set_submodule(&quot;net_b.net_c.conv&quot;, nn.Linear(1, 1))</code> where <code>strict</code> could be <code>True</code> or <code>False</code></p><p>To add a new submodule <code>Conv2d</code> to the existing <code>net_b</code> module, you would call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1))</code>.</p><p>In the above if you set <code>strict=True</code> and call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1), strict=True)</code>, an AttributeError will be raised because <code>net_b</code> does not have a submodule named <code>conv</code>.</p><h4 id="args-159" tabindex="-1">Args <a class="header-anchor" href="#args-159" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li><li><strong>module</strong>: The module to set the submodule to.</li><li><strong>strict</strong>: If <code>False</code>, the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule doesn&#39;t already exist.</li></ul><h4 id="raises-23" tabindex="-1">Raises <a class="header-anchor" href="#raises-23" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>ValueError</strong>: If the <code>target</code> string is empty or if <code>module</code> is not an instance of <code>nn.Module</code>.</li><li><strong>AttributeError</strong>: If at any point along the path resulting from the <code>target</code> string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="share-memory-self-self-source-5" tabindex="-1"><code>share_memory(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#share-memory-self-self-source-5" aria-label="Permalink to &quot;`share_memory(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877)&quot;">​</a></h4><p>See <code>tensorplay.Tensor.share_memory_</code>.</p><hr><h4 id="state-dict-self-args-destination-none-prefix-keep-vars-false-source-5" tabindex="-1"><code>state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#state-dict-self-args-destination-none-prefix-keep-vars-false-source-5" aria-label="Permalink to &quot;`state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105)&quot;">​</a></h4><p>Return a dictionary containing references to the whole state of the module.</p><p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>The returned object is a shallow copy. It contains references to the module&#39;s parameters and buffers.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Currently <code>state_dict()</code> also accepts positional arguments for <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However, this is being deprecated and keyword arguments will be enforced in future releases.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Please avoid the use of argument <code>destination</code> as it is not designed for end-users.</p></div><h4 id="args-160" tabindex="-1">Args <a class="header-anchor" href="#args-160" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>destination</strong> (<code>dict, optional</code>): If provided, the state of module will be updated into the dict and the same object is returned. Otherwise, an <code>OrderedDict</code> will be created and returned.</li><li><strong>Default</strong>: <code>None</code>.</li><li><strong>prefix</strong> (<code>str, optional</code>): a prefix added to parameter and buffer names to compose the keys in state_dict. 默认值： <code>&#39;&#39;</code>.</li><li><strong>keep_vars</strong> (<code>bool, optional</code>): by default the <code>~tensorplay.Tensor</code> s returned in the state dict are detached from autograd. If it&#39;s set to <code>True</code>, detaching will not be performed.</li><li><strong>Default</strong>: <code>False</code>.</li></ul><h4 id="returns-145" tabindex="-1">Returns <a class="header-anchor" href="#returns-145" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>dict</strong>: a dictionary containing a whole state of the module</li></ul><h4 id="example-59" tabindex="-1">Example <a class="header-anchor" href="#example-59" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">module.state_dict().keys()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;weight&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><hr><h4 id="to-self-args-kwargs-source-5" tabindex="-1"><code>to(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-self-args-kwargs-source-5" aria-label="Permalink to &quot;`to(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151)&quot;">​</a></h4><p>Move and/or cast the parameters and buffers.</p><p>This can be called as</p><p>.. function:: to(device=None, dtype=None, non_blocking=False) :noindex:</p><p>.. function:: to(dtype, non_blocking=False) :noindex:</p><p>.. function:: to(tensor, non_blocking=False) :noindex:</p><p>.. function:: to(memory_format=tensorplay.channels_last) :noindex:</p><p>Its signature is similar to <code>tensorplay.Tensor.to</code>, but only accepts floating point or complex <code>dtype</code>\ s. In addition, this method will only cast the floating point or complex parameters and buffers to <code>dtype</code> (if given). The integral parameters and buffers will be moved <code>device</code>, if that is given, but with dtypes unchanged. When <code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.</p><p>See below for examples.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="args-161" tabindex="-1">Args <a class="header-anchor" href="#args-161" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): the desired device of the parameters and buffers in this module</li><li><strong>dtype</strong> (<code>tensorplay.dtype</code>): the desired floating point or complex dtype of the parameters and buffers in this module</li><li><strong>tensor</strong> (<code>tensorplay.Tensor</code>): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module</li><li><strong>memory_format</strong> (<code>tensorplay.memory_format</code>): the desired memory format for 4D parameters and buffers in this module (keyword only argument)</li></ul><h4 id="returns-146" tabindex="-1">Returns <a class="header-anchor" href="#returns-146" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="examples-11" tabindex="-1">Examples <a class="header-anchor" href="#examples-11" aria-label="Permalink to &quot;Examples&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +IGNORE_WANT(&quot;non-deterministic&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(tensorplay.double)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float64)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +REQUIRES(env:TENSORPLAY_DOCTEST_CUDA1)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">gpu1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cuda:1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(gpu1, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.half, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">non_blocking</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;cuda:1&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cpu </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cpu&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(cpu)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).to(tensorplay.cdouble)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3741</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,  </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2382</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5593</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.4443</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear(tensorplay.ones(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.cdouble))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br></div></div><hr><h4 id="to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source-5" tabindex="-1"><code>to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source-5" aria-label="Permalink to &quot;`to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119)&quot;">​</a></h4><p>Move the parameters and buffers to the specified device without copying storage.</p><h4 id="args-162" tabindex="-1">Args <a class="header-anchor" href="#args-162" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): The desired device of the parameters and buffers in this module.</li><li><strong>recurse</strong> (<code>bool</code>): Whether parameters and buffers of submodules should be recursively moved to the specified device.</li></ul><h4 id="returns-147" tabindex="-1">Returns <a class="header-anchor" href="#returns-147" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="train-self-mode-bool-true-self-source-5" tabindex="-1"><code>train(self, mode: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#train-self-mode-bool-true-self-source-5" aria-label="Permalink to &quot;`train(self, mode: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786)&quot;">​</a></h4><p>Set the module in training mode.</p><p>这仅对某些模块有影响。请参阅特定模块的文档 了解它们在训练/评估模式下的行为细节， mode, i.e., whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, 等。</p><h4 id="args-163" tabindex="-1">Args <a class="header-anchor" href="#args-163" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>mode</strong> (<code>bool</code>): whether to set training mode (<code>True</code>) or evaluation mode (<code>False</code>). 默认值： <code>True</code>.</li></ul><h4 id="returns-148" tabindex="-1">Returns <a class="header-anchor" href="#returns-148" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="type-self-dst-type-union-tensorplay-dtype-str-self-source-5" tabindex="-1"><code>type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#type-self-dst-type-union-tensorplay-dtype-str-self-source-5" aria-label="Permalink to &quot;`type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061)&quot;">​</a></h4><p>Casts all parameters and buffers to <code>dst_type</code>.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>此方法会就地修改模块。</p></div><h4 id="args-164" tabindex="-1">Args <a class="header-anchor" href="#args-164" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>dst_type</strong> (<code>type or string</code>): the desired type</li></ul><h4 id="returns-149" tabindex="-1">Returns <a class="header-anchor" href="#returns-149" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="zero-grad-self-set-to-none-bool-true-none-source-5" tabindex="-1"><code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#zero-grad-self-set-to-none-bool-true-none-source-5" aria-label="Permalink to &quot;`zero_grad(self, set_to_none: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849)&quot;">​</a></h4><p>Reset gradients of all model parameters.</p><p>See similar function under <code>tensorplay.optim.Optimizer</code> for more context.</p><h4 id="args-165" tabindex="-1">Args <a class="header-anchor" href="#args-165" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>set_to_none</strong> (<code>bool</code>): instead of setting to zero, set the grads to None. See <code>tensorplay.optim.Optimizer.zero_grad</code> for details.</li></ul><hr></details></div></div></main><footer class="VPDocFooter" data-v-39a288b8 data-v-e257564d><!--[--><!--]--><!----><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-e257564d><span class="visually-hidden" id="doc-footer-aria-label" data-v-e257564d>Pager</span><div class="pager" data-v-e257564d><a class="VPLink link pager-link prev" href="/zh/api/pooling" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>Previous page</span><span class="title" data-v-e257564d>Pooling Layers</span><!--]--></a></div><div class="pager" data-v-e257564d><a class="VPLink link pager-link next" href="/zh/api/normalization" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>Next page</span><span class="title" data-v-e257564d>Normalization Layers</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-5d98c3a5 data-v-e315a0ad><div class="container" data-v-e315a0ad><p class="message" data-v-e315a0ad>基于 Apache 2.0 许可发布。</p><p class="copyright" data-v-e315a0ad>版权所有 © 2025 zlx。保留所有权利。</p></div></footer><!--[--><a href="https://deepwiki.com/bluemoon-o2/TensorPlay" target="_blank" rel="noopener noreferrer" class="deepwiki-floating-badge" title="View on DeepWiki" data-v-5bf2a798><div class="badge-content" data-v-5bf2a798><span class="badge-icon" data-v-5bf2a798>📚</span><span class="badge-text" data-v-5bf2a798>DeepWiki</span></div></a><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"about_index.md\":\"CIIINTJo\",\"api__c.md\":\"CqosplcF\",\"api__reduction.md\":\"4CT0Th-w\",\"api_activation.md\":\"D6ga1LTB\",\"api_adagrad.md\":\"DLbERoZa\",\"api_adam.md\":\"RHJKi90L\",\"api_adamw.md\":\"CwVxav7q\",\"api_alexnet.md\":\"MpdCIREt\",\"api_amp.md\":\"DwxpVi7g\",\"api_audio.md\":\"Bh-isKk9\",\"api_autocast_mode.md\":\"Upx07om-\",\"api_autograd.md\":\"CT9D3RMp\",\"api_backend.md\":\"B-rtwINW\",\"api_backends.md\":\"CZwYzPim\",\"api_batchnorm.md\":\"DiZgGllz\",\"api_collate.md\":\"D7C1FWND\",\"api_common_types.md\":\"D___UsnV\",\"api_comparison.md\":\"CreVyI7m\",\"api_container.md\":\"DHHM_Bnu\",\"api_conv.md\":\"DVE_N5Pv\",\"api_cpp.md\":\"BzQeMOx5\",\"api_cpu.md\":\"CRrVscpA\",\"api_cuda.md\":\"CfDgBLV4\",\"api_data.md\":\"X1JkS3Ea\",\"api_dataloader.md\":\"C7Xq7oGq\",\"api_dataset.md\":\"B_1fKkbj\",\"api_datasets.md\":\"X5IH5o-c\",\"api_dropout.md\":\"4oOM6zi3\",\"api_flatten.md\":\"CazrUAyd\",\"api_folder.md\":\"iJO9XhTd\",\"api_function.md\":\"Cifia0I5\",\"api_functional.md\":\"vzxgI2qA\",\"api_grad_mode.md\":\"pynV7unl\",\"api_grad_scaler.md\":\"B2e1EVkK\",\"api_hooks.md\":\"oWg4LFLb\",\"api_hub.md\":\"LAmAFqTQ\",\"api_index.md\":\"7k9z3nEC\",\"api_init.md\":\"CRGsoW-4\",\"api_instancenorm.md\":\"DLI9QpOZ\",\"api_io.md\":\"B4QJ8i4z\",\"api_lazy.md\":\"CncGuZCr\",\"api_linear.md\":\"oksodNM4\",\"api_loss.md\":\"j_BqCfCJ\",\"api_lr_scheduler.md\":\"X8HZdXpB\",\"api_mkl.md\":\"BjC9gPeB\",\"api_mkldnn.md\":\"vxGeiDLe\",\"api_mnist.md\":\"y5npcclL\",\"api_models.md\":\"SRUf7VVL\",\"api_module.md\":\"BwrS1O16\",\"api_modules.md\":\"QlSgpybu\",\"api_multiprocessing.md\":\"DQUtvEWY\",\"api_nn.md\":\"COPAhDKB\",\"api_normalization.md\":\"ByrPVG7H\",\"api_onnx.md\":\"SEclrq_a\",\"api_openmp.md\":\"BT-Xb62x\",\"api_ops.md\":\"BQq94cRn\",\"api_optim.md\":\"Dn-RoM6r\",\"api_optimizer.md\":\"C2B0c4Z4\",\"api_parameter.md\":\"BKqpCoGr\",\"api_pooling.md\":\"D0thcWXT\",\"api_primitives.md\":\"D9obA82D\",\"api_rmsprop.md\":\"5QmwGE08\",\"api_sampler.md\":\"FxRjpOCf\",\"api_serialization.md\":\"DHRYYFog\",\"api_sgd.md\":\"C_yFSfpK\",\"api_sparse.md\":\"JEGqpwR5\",\"api_stax.md\":\"FE2KuGtx\",\"api_tensorplay.md\":\"CKZuHRE1\",\"api_transforms.md\":\"2p8MgsrV\",\"api_triton.md\":\"Cjx-7JMC\",\"api_types.md\":\"CXoOl-31\",\"api_utils.md\":\"D3F7mqqh\",\"api_vision.md\":\"DLc3G25z\",\"api_viz.md\":\"B2hlZ2Gl\",\"api_worker.md\":\"B5lclh1L\",\"blog_index.md\":\"D21wAZT6\",\"blog_posts_deep-dive-p10-dispatcher.md\":\"Ca8YFPud\",\"blog_posts_tensorplay-architecture-design.md\":\"B8x6QRfn\",\"blog_posts_tpx-autograd-decoupling.md\":\"D7yIFGv8\",\"changelog.md\":\"CfqvsU-D\",\"community_index.md\":\"DsvJKdP0\",\"contributing.md\":\"reV6tBNP\",\"ecosystem_index.md\":\"C5wl-ECg\",\"guide_architecture.md\":\"dtwAcikV\",\"guide_getting-started.md\":\"DA8shq5l\",\"guide_install.md\":\"CFhD7ThC\",\"guide_quickstart.md\":\"DmWxuskA\",\"guide_resources.md\":\"CvwzxFzL\",\"guide_tutorials.md\":\"DIGxDztk\",\"guide_tutorials_cnn-classification.md\":\"BW6UVjdS\",\"guide_tutorials_custom-autograd.md\":\"CisYHV9c\",\"guide_tutorials_linear-regression.md\":\"CE8n7LhM\",\"guide_what-is-tensorplay.md\":\"EVupmN6e\",\"index.md\":\"BLDTU91p\",\"join_index.md\":\"gMCm4TUT\",\"privacy_index.md\":\"CJIqUJMr\",\"subscribe_index.md\":\"DULaWXfI\",\"zh_about_index.md\":\"-3pTOuqZ\",\"zh_api__c.md\":\"MPrSxP_X\",\"zh_api__reduction.md\":\"qV6ouOlq\",\"zh_api_activation.md\":\"DhmY2P-X\",\"zh_api_adagrad.md\":\"DBlRXNHF\",\"zh_api_adam.md\":\"C0nw-mF4\",\"zh_api_adamw.md\":\"D-lYkaTv\",\"zh_api_alexnet.md\":\"BWVWH3KT\",\"zh_api_amp.md\":\"DmfeKcbR\",\"zh_api_audio.md\":\"DV8jfVkL\",\"zh_api_autocast_mode.md\":\"CA5EsxSP\",\"zh_api_autograd.md\":\"D2gfxv4i\",\"zh_api_backend.md\":\"D7d0LjCA\",\"zh_api_backends.md\":\"DI7dZt-1\",\"zh_api_batchnorm.md\":\"QaaaODQC\",\"zh_api_collate.md\":\"CkgzFnj7\",\"zh_api_common_types.md\":\"Bi-t-p8_\",\"zh_api_comparison.md\":\"8mWeXUIV\",\"zh_api_container.md\":\"3WPRuOa5\",\"zh_api_conv.md\":\"FT4rQ48Y\",\"zh_api_cpp.md\":\"Dg31fkWU\",\"zh_api_cuda.md\":\"Ftjr_r5j\",\"zh_api_data.md\":\"DXafuWdu\",\"zh_api_dataloader.md\":\"9lTMqcrA\",\"zh_api_dataset.md\":\"BjaAZvTL\",\"zh_api_datasets.md\":\"rRe-_lwa\",\"zh_api_dropout.md\":\"DicyjWrW\",\"zh_api_flatten.md\":\"CiGze1E1\",\"zh_api_folder.md\":\"C0ip8jxS\",\"zh_api_function.md\":\"CC0XVXv7\",\"zh_api_functional.md\":\"CKRoagcS\",\"zh_api_grad_mode.md\":\"BAdK9C3A\",\"zh_api_grad_scaler.md\":\"ayjcYf_m\",\"zh_api_hooks.md\":\"lgFFJBfG\",\"zh_api_hub.md\":\"BU3mLuXZ\",\"zh_api_index.md\":\"XELyvdx1\",\"zh_api_init.md\":\"BpuF-xTb\",\"zh_api_instancenorm.md\":\"Bx8asyFc\",\"zh_api_io.md\":\"CKfyH_yr\",\"zh_api_lazy.md\":\"Dh-43yXH\",\"zh_api_linear.md\":\"DAJwSbeo\",\"zh_api_loss.md\":\"BtTzGqB8\",\"zh_api_lr_scheduler.md\":\"Bx57Gcb3\",\"zh_api_mkl.md\":\"BTQ2a__w\",\"zh_api_mkldnn.md\":\"CXkzUfQN\",\"zh_api_mnist.md\":\"DRdX7jO1\",\"zh_api_models.md\":\"FzTgqXB3\",\"zh_api_module.md\":\"yXevpwKU\",\"zh_api_modules.md\":\"4fFY20ZD\",\"zh_api_multiprocessing.md\":\"um9au9b1\",\"zh_api_nn.md\":\"CplqfocR\",\"zh_api_normalization.md\":\"hobNBqmi\",\"zh_api_onnx.md\":\"Ddz87Msq\",\"zh_api_openmp.md\":\"i1AKxeou\",\"zh_api_ops.md\":\"9Jj9Wr54\",\"zh_api_optim.md\":\"Dw0vGXeU\",\"zh_api_optimizer.md\":\"BqVotpYJ\",\"zh_api_parameter.md\":\"C3WaBDRJ\",\"zh_api_pooling.md\":\"KQjmVa32\",\"zh_api_primitives.md\":\"asKXZhlA\",\"zh_api_rmsprop.md\":\"CZe0TE5A\",\"zh_api_sampler.md\":\"B5rPkeW0\",\"zh_api_serialization.md\":\"DkXFDqSw\",\"zh_api_sgd.md\":\"sm0U3Vw6\",\"zh_api_sparse.md\":\"CejENDG3\",\"zh_api_stax.md\":\"CGuqno6N\",\"zh_api_tensorplay.md\":\"CNtf_oqw\",\"zh_api_transforms.md\":\"zevP9TO2\",\"zh_api_types.md\":\"DU2x97wA\",\"zh_api_utils.md\":\"DPEFHCAQ\",\"zh_api_vision.md\":\"DLR640Rq\",\"zh_api_viz.md\":\"B512LDgq\",\"zh_api_worker.md\":\"BUorbrDl\",\"zh_blog_index.md\":\"DVTzOKJL\",\"zh_blog_posts_deep-dive-p10-dispatcher.md\":\"DC8lY5o1\",\"zh_blog_posts_tensorplay-architecture-design.md\":\"CSq3TUdr\",\"zh_blog_posts_tpx-autograd-decoupling.md\":\"DLZzwXhu\",\"zh_changelog.md\":\"6PPzigGg\",\"zh_community_index.md\":\"EwEOzVj_\",\"zh_contributing.md\":\"CGGMcTSp\",\"zh_ecosystem_index.md\":\"yUKOvCqU\",\"zh_guide_architecture.md\":\"DCtlxIRq\",\"zh_guide_getting-started.md\":\"BKIhLfTl\",\"zh_guide_install.md\":\"D4h-Og8N\",\"zh_guide_quickstart.md\":\"CDQ5vuiK\",\"zh_guide_resources.md\":\"C7rFLh8M\",\"zh_guide_tutorials.md\":\"ZnE-wF46\",\"zh_guide_tutorials_cnn-classification.md\":\"B4Ud8y-D\",\"zh_guide_tutorials_custom-autograd.md\":\"BYkwBgzh\",\"zh_guide_tutorials_linear-regression.md\":\"BnHJUJXt\",\"zh_guide_what-is-tensorplay.md\":\"DqKe5Ta0\",\"zh_index.md\":\"CZTb8-i1\",\"zh_join_index.md\":\"CiJpMZ-N\",\"zh_privacy_index.md\":\"sYKLed_z\",\"zh_subscribe_index.md\":\"Cb68I94F\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"TensorPlay\",\"description\":\"A transparent, educational, and PyTorch-compatible deep learning framework.\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"logo\":\"/images/logo-0.png\",\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/bluemoon-o2/tensorplay\"}],\"search\":{\"provider\":\"local\",\"options\":{\"locales\":{\"zh\":{\"translations\":{\"button\":{\"buttonText\":\"搜索文档\",\"buttonAriaLabel\":\"搜索文档\"},\"modal\":{\"noResultsText\":\"无法找到相关结果\",\"resetButtonTitle\":\"清除查询条件\",\"footer\":{\"selectText\":\"选择\",\"navigateText\":\"切换\",\"closeText\":\"关闭\"}}}}}}}},\"locales\":{\"root\":{\"label\":\"English\",\"lang\":\"en\",\"description\":\"A simple deep learning framework designed for educational purposes and small-scale experiments.\",\"themeConfig\":{\"nav\":[{\"text\":\"Learn\",\"items\":[{\"text\":\"Getting Started\",\"link\":\"/guide/getting-started\"},{\"text\":\"Tutorials\",\"link\":\"/guide/tutorials\"}]},{\"text\":\"Community\",\"link\":\"/community/\"},{\"text\":\"Projects\",\"link\":\"/ecosystem/\"},{\"text\":\"Docs\",\"link\":\"/api/\"},{\"text\":\"Blog & News\",\"link\":\"/blog/\"},{\"text\":\"About\",\"link\":\"/about/\"},{\"text\":\"JOIN\",\"link\":\"/join/\"}],\"sidebar\":{\"/guide/\":[{\"text\":\"Introduction\",\"items\":[{\"text\":\"What is TensorPlay?\",\"link\":\"/guide/what-is-tensorplay\"},{\"text\":\"Getting Started\",\"link\":\"/guide/getting-started\"},{\"text\":\"Installation\",\"link\":\"/guide/install\"},{\"text\":\"Quickstart\",\"link\":\"/guide/quickstart\"},{\"text\":\"Architecture\",\"link\":\"/guide/architecture\"}]},{\"text\":\"Guides & Resources\",\"items\":[{\"text\":\"Tutorials\",\"link\":\"/guide/tutorials\",\"items\":[{\"text\":\"Image Classification\",\"link\":\"/guide/tutorials/cnn-classification\"},{\"text\":\"Linear Regression\",\"link\":\"/guide/tutorials/linear-regression\"},{\"text\":\"Custom Autograd\",\"link\":\"/guide/tutorials/custom-autograd\"}]},{\"text\":\"Resources\",\"link\":\"/guide/resources\"},{\"text\":\"API Reference\",\"link\":\"/api/\"}]}],\"/api/\":[{\"text\":\"Core API\",\"items\":[{\"text\":\"Overview\",\"link\":\"/api/\"},{\"text\":\"tensorplay\",\"link\":\"/api/tensorplay\"},{\"text\":\"autograd\",\"link\":\"/api/autograd\"},{\"text\":\"functional\",\"link\":\"/api/functional\"},{\"text\":\"optim\",\"link\":\"/api/optim\"},{\"text\":\"cuda\",\"link\":\"/api/cuda\"}]},{\"text\":\"Neural Networks (nn)\",\"collapsed\":false,\"items\":[{\"text\":\"nn\",\"link\":\"/api/nn\"},{\"text\":\"Modules\",\"link\":\"/api/modules\"},{\"text\":\"Linear Layers\",\"link\":\"/api/linear\"},{\"text\":\"Convolution Layers\",\"link\":\"/api/conv\"},{\"text\":\"Pooling Layers\",\"link\":\"/api/pooling\"},{\"text\":\"Activation Functions\",\"link\":\"/api/activation\"},{\"text\":\"Normalization Layers\",\"link\":\"/api/normalization\"},{\"text\":\"Loss Functions\",\"link\":\"/api/loss\"},{\"text\":\"Dropout Layers\",\"link\":\"/api/dropout\"},{\"text\":\"Container\",\"link\":\"/api/container\"}]}]},\"footer\":{\"message\":\"Released under the Apache 2.0 License.\",\"copyright\":\"Copyright © 2025 zlx. All rights reserved.\"}}},\"zh\":{\"label\":\"简体中文\",\"lang\":\"zh\",\"link\":\"/zh/\",\"description\":\"一个适合学习者、兼容 PyTorch 的深度学习框架。\",\"themeConfig\":{\"nav\":[{\"text\":\"学习\",\"items\":[{\"text\":\"快速开始\",\"link\":\"/zh/guide/getting-started\"},{\"text\":\"教程\",\"link\":\"/zh/guide/tutorials\"}]},{\"text\":\"社区\",\"link\":\"/zh/community/\"},{\"text\":\"项目\",\"link\":\"/zh/ecosystem/\"},{\"text\":\"文档\",\"link\":\"/zh/api/\"},{\"text\":\"博客与新闻\",\"link\":\"/zh/blog/\"},{\"text\":\"关于\",\"link\":\"/zh/about/\"},{\"text\":\"加入我们\",\"link\":\"/zh/join/\"}],\"sidebar\":{\"/zh/guide/\":[{\"text\":\"介绍\",\"items\":[{\"text\":\"什么是 TensorPlay?\",\"link\":\"/zh/guide/what-is-tensorplay\"},{\"text\":\"快速开始\",\"link\":\"/zh/guide/getting-started\"},{\"text\":\"安装\",\"link\":\"/zh/guide/install\"},{\"text\":\"快速入门\",\"link\":\"/zh/guide/quickstart\"},{\"text\":\"架构设计\",\"link\":\"/zh/guide/architecture\"}]},{\"text\":\"指南与资源\",\"items\":[{\"text\":\"教程\",\"link\":\"/zh/guide/tutorials\",\"items\":[{\"text\":\"图像分类\",\"link\":\"/zh/guide/tutorials/cnn-classification\"},{\"text\":\"线性回归\",\"link\":\"/zh/guide/tutorials/linear-regression\"},{\"text\":\"自定义自动微分\",\"link\":\"/zh/guide/tutorials/custom-autograd\"}]},{\"text\":\"资源\",\"link\":\"/zh/guide/resources\"},{\"text\":\"API 参考\",\"link\":\"/zh/api/\"}]}],\"/zh/api/\":[{\"text\":\"核心 API\",\"items\":[{\"text\":\"概览\",\"link\":\"/zh/api/\"},{\"text\":\"tensorplay\",\"link\":\"/zh/api/tensorplay\"},{\"text\":\"autograd\",\"link\":\"/zh/api/autograd\"},{\"text\":\"functional\",\"link\":\"/zh/api/functional\"},{\"text\":\"optim\",\"link\":\"/zh/api/optim\"},{\"text\":\"cuda\",\"link\":\"/zh/api/cuda\"}]},{\"text\":\"神经网络 (nn)\",\"collapsed\":false,\"items\":[{\"text\":\"nn\",\"link\":\"/zh/api/nn\"},{\"text\":\"Modules\",\"link\":\"/zh/api/modules\"},{\"text\":\"Linear Layers\",\"link\":\"/zh/api/linear\"},{\"text\":\"Convolution Layers\",\"link\":\"/zh/api/conv\"},{\"text\":\"Pooling Layers\",\"link\":\"/zh/api/pooling\"},{\"text\":\"Activation Functions\",\"link\":\"/zh/api/activation\"},{\"text\":\"Normalization Layers\",\"link\":\"/zh/api/normalization\"},{\"text\":\"Loss Functions\",\"link\":\"/zh/api/loss\"},{\"text\":\"Dropout Layers\",\"link\":\"/zh/api/dropout\"},{\"text\":\"Container\",\"link\":\"/zh/api/container\"}]}]},\"footer\":{\"message\":\"基于 Apache 2.0 许可发布。\",\"copyright\":\"版权所有 © 2025 zlx。保留所有权利。\"}}}},\"scrollOffset\":134,\"cleanUrls\":true}");</script>
    
  </body>
</html>