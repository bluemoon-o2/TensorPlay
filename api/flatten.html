<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    
    <title>tensorplay.nn.modules.flatten | TensorPlay</title>
    <meta name="description" content="A simple deep learning framework designed for educational purposes and small-scale experiments.">
    <meta name="generator" content="VitePress v1.6.4">
    <link rel="preload stylesheet" href="/assets/style.CpKfxB8V.css" as="style">
    <link rel="preload stylesheet" href="/vp-icons.css" as="style">
    
    <script type="module" src="/assets/app.CTcvRb4v.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/theme.DKh4-qDF.js">
    <link rel="modulepreload" href="/assets/chunks/framework.t-GQSDtj.js">
    <link rel="modulepreload" href="/assets/api_flatten.md.kF2VdI-B.lean.js">
    <link rel="icon" type="image/png" href="/images/logo-0.png">
    <link rel="apple-touch-icon" href="/images/logo-0.png">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <meta name="keywords" content="deep learning, machine learning, ai framework, pytorch compatible, educational, c++, python, tensorplay, 深度学习, 机器学习, 自动微分">
    <meta name="author" content="TensorPlay Team">
    <meta property="og:type" content="website">
    <meta property="og:title" content="TensorPlay | Transparent &amp; Compatible AI Framework">
    <meta property="og:description" content="A transparent, educational, and PyTorch-compatible deep learning framework for the modern AI era.">
    <meta property="og:site_name" content="TensorPlay">
    <meta property="og:url" content="https://www.tensorplay.cn">
    <meta property="og:image" content="https://www.tensorplay.cn/images/logo-1.png">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="TensorPlay | Transparent &amp; Compatible AI Framework">
    <meta name="twitter:description" content="A transparent, educational, and PyTorch-compatible deep learning framework for the modern AI era.">
    <meta name="twitter:image" content="https://www.tensorplay.cn/images/logo-1.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@300;400;500;600;700&amp;display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-5d98c3a5><!--[--><canvas id="cyberpunk-particles" data-v-36680640></canvas><!----><!--]--><!--[--><span tabindex="-1" data-v-0b0ada53></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-0b0ada53>Skip to content</a><!--]--><!----><header class="VPNav" data-v-5d98c3a5 data-v-ae24b3ad><div class="VPNavBar" data-v-ae24b3ad data-v-6aa21345><div class="wrapper" data-v-6aa21345><div class="container" data-v-6aa21345><div class="title" data-v-6aa21345><div class="VPNavBarTitle has-sidebar" data-v-6aa21345 data-v-1168a8e4><a class="title" href="/" data-v-1168a8e4><!--[--><!--]--><!--[--><img class="VPImage logo" src="/images/logo-0.png" alt data-v-8426fc1a><!--]--><span data-v-1168a8e4>TensorPlay</span><!--[--><!--]--></a></div></div><div class="content" data-v-6aa21345><div class="content-body" data-v-6aa21345><!--[--><!--]--><div class="VPNavBarSearch search" data-v-6aa21345><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-6aa21345 data-v-dc692963><span id="main-nav-aria-label" class="visually-hidden" data-v-dc692963> Main Navigation </span><!--[--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-dc692963 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-cf11d7a2><span class="text" data-v-cf11d7a2><!----><span data-v-cf11d7a2>Learn</span><span class="vpi-chevron-down text-icon" data-v-cf11d7a2></span></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><div class="items" data-v-b98bc113><!--[--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/guide/getting-started" data-v-35975db6><!--[--><span data-v-35975db6>Getting Started</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/guide/tutorials" data-v-35975db6><!--[--><span data-v-35975db6>Tutorials</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/community/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>Community</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/ecosystem/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>Projects</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/api/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>Docs</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/blog/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>Blog & News</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/about/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>About</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/join/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>JOIN</span><!--]--></a><!--]--><!--]--></nav><div class="VPFlyout VPNavBarTranslations translations" data-v-6aa21345 data-v-88af2de4 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="Change language" data-v-cf11d7a2><span class="text" data-v-cf11d7a2><span class="vpi-languages option-icon" data-v-cf11d7a2></span><!----><span class="vpi-chevron-down text-icon" data-v-cf11d7a2></span></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><!----><!--[--><!--[--><div class="items" data-v-88af2de4><p class="title" data-v-88af2de4>English</p><!--[--><div class="VPMenuLink" data-v-88af2de4 data-v-35975db6><a class="VPLink link" href="/zh/api/flatten" data-v-35975db6><!--[--><span data-v-35975db6>简体中文</span><!--]--></a></div><!--]--></div><!--]--><!--]--></div></div></div><div class="VPNavBarAppearance appearance" data-v-6aa21345 data-v-6c893767><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-6c893767 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-6aa21345 data-v-0394ad82 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/bluemoon-o2/tensorplay" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-6aa21345 data-v-bb2aa2f0 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-cf11d7a2><span class="vpi-more-horizontal icon" data-v-cf11d7a2></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><!----><!--[--><!--[--><div class="group translations" data-v-bb2aa2f0><p class="trans-title" data-v-bb2aa2f0>English</p><!--[--><div class="VPMenuLink" data-v-bb2aa2f0 data-v-35975db6><a class="VPLink link" href="/zh/api/flatten" data-v-35975db6><!--[--><span data-v-35975db6>简体中文</span><!--]--></a></div><!--]--></div><div class="group" data-v-bb2aa2f0><div class="item appearance" data-v-bb2aa2f0><p class="label" data-v-bb2aa2f0>Appearance</p><div class="appearance-action" data-v-bb2aa2f0><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-bb2aa2f0 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div></div></div><div class="group" data-v-bb2aa2f0><div class="item social-links" data-v-bb2aa2f0><div class="VPSocialLinks social-links-list" data-v-bb2aa2f0 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/bluemoon-o2/tensorplay" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-6aa21345 data-v-e5dd9c1c><span class="container" data-v-e5dd9c1c><span class="top" data-v-e5dd9c1c></span><span class="middle" data-v-e5dd9c1c></span><span class="bottom" data-v-e5dd9c1c></span></span></button></div></div></div></div><div class="divider" data-v-6aa21345><div class="divider-line" data-v-6aa21345></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-5d98c3a5 data-v-a6f0e41e><div class="container" data-v-a6f0e41e><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-a6f0e41e><span class="vpi-align-left menu-icon" data-v-a6f0e41e></span><span class="menu-text" data-v-a6f0e41e>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-a6f0e41e data-v-8a42e2b4><button data-v-8a42e2b4>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-5d98c3a5 data-v-319d5ca6><div class="curtain" data-v-319d5ca6></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-319d5ca6><span class="visually-hidden" id="sidebar-aria-label" data-v-319d5ca6> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-c40bc020><section class="VPSidebarItem level-0" data-v-c40bc020 data-v-b3fd67f8><div class="item" role="button" tabindex="0" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><h2 class="text" data-v-b3fd67f8>Core API</h2><!----></div><div class="items" data-v-b3fd67f8><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/api/" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Overview</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/api/tensorplay" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>tensorplay</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/api/autograd" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>autograd</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/api/functional" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>functional</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/api/optim" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>optim</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/api/cuda" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>cuda</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-c40bc020><section class="VPSidebarItem level-0 collapsible" data-v-c40bc020 data-v-b3fd67f8><div class="item" role="button" tabindex="0" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><h2 class="text" data-v-b3fd67f8>Neural Networks (nn)</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-b3fd67f8><span class="vpi-chevron-right caret-icon" data-v-b3fd67f8></span></div></div><div class="items" data-v-b3fd67f8><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/api/nn" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>nn</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/api/modules" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Modules</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/api/linear" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Linear Layers</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/api/conv" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Convolution Layers</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/api/pooling" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Pooling Layers</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/api/activation" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Activation Functions</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/api/normalization" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Normalization Layers</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/api/loss" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Loss Functions</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/api/dropout" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Dropout Layers</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/api/container" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>Container</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-5d98c3a5 data-v-1428d186><div class="VPDoc has-sidebar has-aside" data-v-1428d186 data-v-39a288b8><!--[--><!--]--><div class="container" data-v-39a288b8><div class="aside" data-v-39a288b8><div class="aside-curtain" data-v-39a288b8></div><div class="aside-container" data-v-39a288b8><div class="aside-content" data-v-39a288b8><div class="VPDocAside" data-v-39a288b8 data-v-3f215769><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-3f215769 data-v-a5bbad30><div class="content" data-v-a5bbad30><div class="outline-marker" data-v-a5bbad30></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-a5bbad30>On this page</div><ul class="VPDocOutlineItem root" data-v-a5bbad30 data-v-b933a997><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-3f215769></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-39a288b8><div class="content-container" data-v-39a288b8><!--[--><!--[--><!--[--><!----><!--]--><!--]--><!--]--><main class="main" data-v-39a288b8><div style="position:relative;" class="vp-doc _api_flatten" data-v-39a288b8><div><h1 id="tensorplay-nn-modules-flatten" tabindex="-1">tensorplay.nn.modules.flatten <a class="header-anchor" href="#tensorplay-nn-modules-flatten" aria-label="Permalink to &quot;tensorplay.nn.modules.flatten&quot;">​</a></h1><h2 id="classes" tabindex="-1">Classes <a class="header-anchor" href="#classes" aria-label="Permalink to &quot;Classes&quot;">​</a></h2><h3 id="class-flatten-source" tabindex="-1"><code>class Flatten</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/flatten.py#L12" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-flatten-source" aria-label="Permalink to &quot;`class Flatten` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/flatten.py#L12)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Flatten(start_dim: </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">int</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, end_dim: </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">int</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> -</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>Module</code></p><p>Flattens a contiguous range of dims into a tensor.</p><p>For use with <code>~nn.Sequential</code>, see <code>tensorplay.flatten</code> for details.</p><h4 id="shape" tabindex="-1">Shape <a class="header-anchor" href="#shape" aria-label="Permalink to &quot;Shape&quot;">​</a></h4><ul><li>Input: <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="27.162ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 12005.7 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(389,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(889,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(1333.7,0)"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(646,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mtext"><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" style="stroke-width:3;"></path><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(394,0)" style="stroke-width:3;"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(783,0)" style="stroke-width:3;"></path><path data-c="72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z" transform="translate(1283,0)" style="stroke-width:3;"></path><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(1675,0)" style="stroke-width:3;"></path></g></g></g><g data-mml-node="mo" transform="translate(3489.1,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3933.8,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(4378.5,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(4823.1,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5267.8,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(5712.5,0)"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(646,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g></g><g data-mml-node="mo" transform="translate(6652.4,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(7097.1,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(7541.8,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(7986.4,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(8431.1,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(8875.8,0)"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(646,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mtext"><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" style="stroke-width:3;"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(444,0)" style="stroke-width:3;"></path><path data-c="64" d="M376 495Q376 511 376 535T377 568Q377 613 367 624T316 637H298V660Q298 683 300 683L310 684Q320 685 339 686T376 688Q393 689 413 690T443 693T454 694H457V390Q457 84 458 81Q461 61 472 55T517 46H535V0Q533 0 459 -5T380 -11H373V44L365 37Q307 -11 235 -11Q158 -11 96 50T34 215Q34 315 97 378T244 442Q319 442 376 393V495ZM373 342Q328 405 260 405Q211 405 173 369Q146 341 139 305T131 211Q131 155 138 120T173 59Q203 26 251 26Q322 26 373 103V342Z" transform="translate(1000,0)" style="stroke-width:3;"></path></g></g></g><g data-mml-node="mo" transform="translate(10672,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(11116.7,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(11616.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mo>∗</mo><mo>,</mo><msub><mi>S</mi><mrow data-mjx-texclass="ORD"><mtext>start</mtext></mrow></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>S</mi><mrow data-mjx-texclass="ORD"><mi>i</mi></mrow></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>S</mi><mrow data-mjx-texclass="ORD"><mtext>end</mtext></mrow></msub><mo>,</mo><mo>∗</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>,&#39; where <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.127ex" height="1.952ex" role="img" focusable="false" viewBox="0 -705 940 862.8" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(646,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>S</mi><mrow data-mjx-texclass="ORD"><mi>i</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> is the size at dimension <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>i</mi></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:0.079ex;" xmlns="http://www.w3.org/2000/svg" width="1.131ex" height="0.973ex" role="img" focusable="false" viewBox="0 -465 500 430" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>∗</mo></math></mjx-assistive-mml></mjx-container> means any number of dimensions including none.</li><li>Output: <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.777ex;" xmlns="http://www.w3.org/2000/svg" width="15.96ex" height="2.966ex" role="img" focusable="false" viewBox="0 -967.8 7054.5 1311.1" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(389,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(889,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="munderover" transform="translate(1333.7,0)"><g data-mml-node="mo"><path data-c="220F" d="M158 656Q147 684 131 694Q110 707 69 710H55V750H888V710H874Q840 708 820 698T795 678T786 656V-155Q798 -206 874 -210H888V-250H570V-210H584Q618 -208 638 -197T663 -178T673 -155V710H270V277L271 -155Q283 -206 359 -210H373V-250H55V-210H69Q103 -208 123 -197T148 -178T158 -155V656Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(977,477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mtext"><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" style="stroke-width:3;"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(444,0)" style="stroke-width:3;"></path><path data-c="64" d="M376 495Q376 511 376 535T377 568Q377 613 367 624T316 637H298V660Q298 683 300 683L310 684Q320 685 339 686T376 688Q393 689 413 690T443 693T454 694H457V390Q457 84 458 81Q461 61 472 55T517 46H535V0Q533 0 459 -5T380 -11H373V44L365 37Q307 -11 235 -11Q158 -11 96 50T34 215Q34 315 97 378T244 442Q319 442 376 393V495ZM373 342Q328 405 260 405Q211 405 173 369Q146 341 139 305T131 211Q131 155 138 120T173 59Q203 26 251 26Q322 26 373 103V342Z" transform="translate(1000,0)" style="stroke-width:3;"></path></g></g><g data-mml-node="TeXAtom" transform="translate(977,-285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3;"></path></g><g data-mml-node="mtext" transform="translate(1123,0)"><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" style="stroke-width:3;"></path><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(394,0)" style="stroke-width:3;"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(783,0)" style="stroke-width:3;"></path><path data-c="72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z" transform="translate(1283,0)" style="stroke-width:3;"></path><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(1675,0)" style="stroke-width:3;"></path></g></g></g><g data-mml-node="msub" transform="translate(4780.9,0)"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(646,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g></g><g data-mml-node="mo" transform="translate(5720.8,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(6165.5,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(6665.5,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mo>∗</mo><mo>,</mo><munderover><mo data-mjx-texclass="OP">∏</mo><mrow data-mjx-texclass="ORD"><mi>i</mi><mo>=</mo><mtext>start</mtext></mrow><mrow data-mjx-texclass="ORD"><mtext>end</mtext></mrow></munderover><msub><mi>S</mi><mrow data-mjx-texclass="ORD"><mi>i</mi></mrow></msub><mo>,</mo><mo>∗</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>.</li></ul><h4 id="args" tabindex="-1">Args <a class="header-anchor" href="#args" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>start_dim</strong>: first dim to flatten (default = 1).</li><li><strong>end_dim</strong>: last dim to flatten (default = -1).</li></ul><h4 id="examples" tabindex="-1">Examples <a class="header-anchor" href="#examples" aria-label="Permalink to &quot;Examples&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.randn(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">32</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># With default parameters</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Flatten()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">output </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> m(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">output.size()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.Size([</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">32</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">25</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># With non-default parameters</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Flatten(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">output </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> m(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">output.size()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.Size([</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">160</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><details><summary>Methods</summary><h4 id="init-self-start-dim-int-1-end-dim-int-1-none-source" tabindex="-1"><code>__init__(self, start_dim: int = 1, end_dim: int = -1) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/flatten.py#L46" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-start-dim-int-1-end-dim-int-1-none-source" aria-label="Permalink to &quot;`__init__(self, start_dim: int = 1, end_dim: int = -1) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/flatten.py#L46)&quot;">​</a></h4><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p><hr><h4 id="add-module-self-name-str-module-optional-forwardref-module-none-source" tabindex="-1"><code>add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#add-module-self-name-str-module-optional-forwardref-module-none-source" aria-label="Permalink to &quot;`add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667)&quot;">​</a></h4><p>Add a child module to the current module.</p><p>The module can be accessed as an attribute using the given name.</p><h4 id="args-1" tabindex="-1">Args <a class="header-anchor" href="#args-1" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the child module. The child module can be accessed from this module using the given name</li><li><strong>module</strong> (<code>Module</code>): child module to be added to the module.</li></ul><hr><h4 id="apply-self-fn-callable-forwardref-module-nonetype-self-source" tabindex="-1"><code>apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#apply-self-fn-callable-forwardref-module-nonetype-self-source" aria-label="Permalink to &quot;`apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990)&quot;">​</a></h4><p>Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self.</p><p>Typical use includes initializing the parameters of a model (see also <code>nn-init-doc</code>).</p><h4 id="args-2" tabindex="-1">Args <a class="header-anchor" href="#args-2" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>fn</strong> (``Module<code> -&gt; None</code>): function to be applied to each submodule</li></ul><h4 id="returns" tabindex="-1">Returns <a class="header-anchor" href="#returns" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="example" tabindex="-1">Example <a class="header-anchor" href="#example" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@tensorplay.no_grad</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> init_weights</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        m.weight.fill_(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m.weight)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net.apply(init_weights)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><hr><h4 id="bfloat16-self-self-source" tabindex="-1"><code>bfloat16(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#bfloat16-self-self-source" aria-label="Permalink to &quot;`bfloat16(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-1" tabindex="-1">Returns <a class="header-anchor" href="#returns-1" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source" tabindex="-1"><code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source" aria-label="Permalink to &quot;`buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627)&quot;">​</a></h4><p>Return an iterator over module buffers.</p><h4 id="args-3" tabindex="-1">Args <a class="header-anchor" href="#args-3" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.</li></ul><h4 id="yields" tabindex="-1">Yields <a class="header-anchor" href="#yields" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>tensorplay.Tensor: module buffer
</code></pre><h4 id="example-1" tabindex="-1">Example <a class="header-anchor" href="#example-1" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.buffers():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf), buf.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="children-self-collections-abc-iterator-module-source" tabindex="-1"><code>children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#children-self-collections-abc-iterator-module-source" aria-label="Permalink to &quot;`children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681)&quot;">​</a></h4><p>Return an iterator over immediate children modules.</p><h4 id="yields-1" tabindex="-1">Yields <a class="header-anchor" href="#yields-1" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a child module</li></ul><hr><h4 id="compile-self-args-kwargs-source" tabindex="-1"><code>compile(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#compile-self-args-kwargs-source" aria-label="Permalink to &quot;`compile(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944)&quot;">​</a></h4><p>Compile this Module&#39;s forward using <code>tensorplay.compile</code>.</p><p>This Module&#39;s <code>__call__</code> method is compiled and all arguments are passed as-is to <code>tensorplay.compile</code>.</p><p>See <code>tensorplay.compile</code> for details on the arguments for this function.</p><hr><h4 id="cpu-self-self-source" tabindex="-1"><code>cpu(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cpu-self-self-source" aria-label="Permalink to &quot;`cpu(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050)&quot;">​</a></h4><p>Move all model parameters and buffers to the CPU.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-2" tabindex="-1">Returns <a class="header-anchor" href="#returns-2" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source" tabindex="-1"><code>cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source" aria-label="Permalink to &quot;`cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031)&quot;">​</a></h4><p>Move all model parameters and buffers to the GPU.</p><p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on GPU while being optimized.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-4" tabindex="-1">Args <a class="header-anchor" href="#args-4" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>int, optional</code>): if specified, all parameters will be copied to that device</li></ul><h4 id="returns-3" tabindex="-1">Returns <a class="header-anchor" href="#returns-3" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="double-self-self-source" tabindex="-1"><code>double(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#double-self-self-source" aria-label="Permalink to &quot;`double(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-4" tabindex="-1">Returns <a class="header-anchor" href="#returns-4" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="eval-self-self-source" tabindex="-1"><code>eval(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#eval-self-self-source" aria-label="Permalink to &quot;`eval(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808)&quot;">​</a></h4><p>Set the module in evaluation mode.</p><p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e. whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, etc.</p><p>This is equivalent with <code>self.train(False) &lt;tensorplay.nn.Module.train&gt;</code>.</p><p>See <code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.</p><h4 id="returns-5" tabindex="-1">Returns <a class="header-anchor" href="#returns-5" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="extra-repr-self-str-source" tabindex="-1"><code>extra_repr(self) -&gt; str</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/flatten.py#L57" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#extra-repr-self-str-source" aria-label="Permalink to &quot;`extra_repr(self) -&gt; str` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/flatten.py#L57)&quot;">​</a></h4><p>Returns the extra representation of the module.</p><hr><h4 id="float-self-self-source" tabindex="-1"><code>float(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#float-self-self-source" aria-label="Permalink to &quot;`float(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-6" tabindex="-1">Returns <a class="header-anchor" href="#returns-6" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="forward-self-input-tensorplay-c-tensorbase-tensorplay-c-tensorbase-source" tabindex="-1"><code>forward(self, input: tensorplay._C.TensorBase) -&gt; tensorplay._C.TensorBase</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/flatten.py#L51" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#forward-self-input-tensorplay-c-tensorbase-tensorplay-c-tensorbase-source" aria-label="Permalink to &quot;`forward(self, input: tensorplay._C.TensorBase) -&gt; tensorplay._C.TensorBase` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/flatten.py#L51)&quot;">​</a></h4><p>Runs the forward pass.</p><hr><h4 id="get-buffer-self-target-str-tensor-source" tabindex="-1"><code>get_buffer(self, target: str) -&gt; &#39;Tensor&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-buffer-self-target-str-tensor-source" aria-label="Permalink to &quot;`get_buffer(self, target: str) -&gt; &#39;Tensor&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881)&quot;">​</a></h4><p>Return the buffer given by <code>target</code> if it exists, otherwise throw an error.</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-5" tabindex="-1">Args <a class="header-anchor" href="#args-5" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the buffer to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-7" tabindex="-1">Returns <a class="header-anchor" href="#returns-7" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.Tensor: The buffer referenced by ``target``
</code></pre><h4 id="raises" tabindex="-1">Raises <a class="header-anchor" href="#raises" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not a buffer</li></ul><hr><h4 id="get-extra-state-self-any-source" tabindex="-1"><code>get_extra_state(self) -&gt; Any</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-extra-state-self-any-source" aria-label="Permalink to &quot;`get_extra_state(self) -&gt; Any` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917)&quot;">​</a></h4><p>Return any extra state to include in the module&#39;s state_dict.</p><p>Implement this and a corresponding <code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module&#39;s <code>state_dict()</code>.</p><p>Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.</p><h4 id="returns-8" tabindex="-1">Returns <a class="header-anchor" href="#returns-8" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>object</strong>: Any extra state to store in the module&#39;s state_dict</li></ul><hr><h4 id="get-parameter-self-target-str-parameter-source" tabindex="-1"><code>get_parameter(self, target: str) -&gt; &#39;Parameter&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-parameter-self-target-str-parameter-source" aria-label="Permalink to &quot;`get_parameter(self, target: str) -&gt; &#39;Parameter&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845)&quot;">​</a></h4><p>Return the parameter given by <code>target</code> if it exists, otherwise throw an error.</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-6" tabindex="-1">Args <a class="header-anchor" href="#args-6" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the Parameter to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-9" tabindex="-1">Returns <a class="header-anchor" href="#returns-9" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Parameter: The Parameter referenced by ``target``
</code></pre><h4 id="raises-1" tabindex="-1">Raises <a class="header-anchor" href="#raises-1" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not an <code>nn.Parameter</code></li></ul><hr><h4 id="get-submodule-self-target-str-module-source" tabindex="-1"><code>get_submodule(self, target: str) -&gt; &#39;Module&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-submodule-self-target-str-module-source" aria-label="Permalink to &quot;`get_submodule(self, target: str) -&gt; &#39;Module&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699)&quot;">​</a></h4><p>Return the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> which has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(&quot;net_b.linear&quot;)</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(&quot;net_b.net_c.conv&quot;)</code>.</p><p>The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.</p><h4 id="args-7" tabindex="-1">Args <a class="header-anchor" href="#args-7" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li></ul><h4 id="returns-10" tabindex="-1">Returns <a class="header-anchor" href="#returns-10" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Module: The submodule referenced by ``target``
</code></pre><h4 id="raises-2" tabindex="-1">Raises <a class="header-anchor" href="#raises-2" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If at any point along the path resulting from the target string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="half-self-self-source" tabindex="-1"><code>half(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#half-self-self-source" aria-label="Permalink to &quot;`half(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-11" tabindex="-1">Returns <a class="header-anchor" href="#returns-11" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source" tabindex="-1"><code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source" aria-label="Permalink to &quot;`load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436)&quot;">​</a></h4><p>Copy parameters and buffers from <code>state_dict</code> into this module and its descendants.</p><p>If <code>strict</code> is <code>True</code>, then the keys of <code>state_dict</code> must exactly match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function.</p><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>If <code>assign</code> is <code>True</code> the optimizer must be created after the call to <code>load_state_dict</code> unless <code>~tensorplay.__future__.get_swap_module_params_on_conversion</code> is <code>True</code>.</p></div><h4 id="args-8" tabindex="-1">Args <a class="header-anchor" href="#args-8" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state_dict</strong> (<code>dict</code>): a dict containing parameters and persistent buffers.</li><li><strong>strict</strong> (<code>bool, optional</code>): whether to strictly enforce that the keys in <code>state_dict</code> match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function. Default: <code>True</code></li><li><strong>assign</strong> (<code>bool, optional</code>): When set to <code>False</code>, the properties of the tensors in the current module are preserved whereas setting it to <code>True</code> preserves properties of the Tensors in the state dict. The only exception is the <code>requires_grad</code> field of <code>~tensorplay.nn.Parameter</code> for which the value from the module is preserved. Default: <code>False</code></li></ul><h4 id="returns-12" tabindex="-1">Returns <a class="header-anchor" href="#returns-12" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:
    * ``missing_keys`` is a list of str containing any keys that are expected
        by this module but missing from the provided ``state_dict``.
    * ``unexpected_keys`` is a list of str containing the keys that are not
        expected by this module but present in the provided ``state_dict``.
</code></pre><h4 id="note" tabindex="-1">Note <a class="header-anchor" href="#note" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>If a parameter or buffer is registered as ``None`` and its corresponding key
exists in `state_dict`, `load_state_dict` will raise a
``RuntimeError``.
</code></pre><hr><h4 id="modules-self-collections-abc-iterator-module-source" tabindex="-1"><code>modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#modules-self-collections-abc-iterator-module-source" aria-label="Permalink to &quot;`modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710)&quot;">​</a></h4><p>Return an iterator over all modules in the network.</p><h4 id="yields-2" tabindex="-1">Yields <a class="header-anchor" href="#yields-2" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a module in the network</li></ul><h4 id="note-1" tabindex="-1">Note <a class="header-anchor" href="#note-1" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-2" tabindex="-1">Example <a class="header-anchor" href="#example-2" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source" tabindex="-1"><code>named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source" aria-label="Permalink to &quot;`named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650)&quot;">​</a></h4><p>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p><h4 id="args-9" tabindex="-1">Args <a class="header-anchor" href="#args-9" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all buffer names.</li><li><strong>recurse</strong> (<code>bool, optional</code>): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Defaults to True.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated buffers in the result. Defaults to True.</li></ul><h4 id="yields-3" tabindex="-1">Yields <a class="header-anchor" href="#yields-3" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, tensorplay.Tensor): Tuple containing the name and buffer
</code></pre><h4 id="example-3" tabindex="-1">Example <a class="header-anchor" href="#example-3" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_buffers():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_var&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-children-self-collections-abc-iterator-tuple-str-module-source" tabindex="-1"><code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-children-self-collections-abc-iterator-tuple-str-module-source" aria-label="Permalink to &quot;`named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690)&quot;">​</a></h4><p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p><h4 id="yields-4" tabindex="-1">Yields <a class="header-anchor" href="#yields-4" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple containing a name and child module
</code></pre><h4 id="example-4" tabindex="-1">Example <a class="header-anchor" href="#example-4" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, module </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.named_children():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv4&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv5&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(module)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source" tabindex="-1"><code>named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source" aria-label="Permalink to &quot;`named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737)&quot;">​</a></h4><p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p><h4 id="args-10" tabindex="-1">Args <a class="header-anchor" href="#args-10" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>memo</strong>: a memo to store the set of modules already added to the result</li><li><strong>prefix</strong>: a prefix that will be added to the name of the module</li><li><strong>remove_duplicate</strong>: whether to remove the duplicated module instances in the result or not</li></ul><h4 id="yields-5" tabindex="-1">Yields <a class="header-anchor" href="#yields-5" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple of name and module
</code></pre><h4 id="note-2" tabindex="-1">Note <a class="header-anchor" href="#note-2" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-5" tabindex="-1">Example <a class="header-anchor" href="#example-5" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.named_modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;0&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source" tabindex="-1"><code>named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source" aria-label="Permalink to &quot;`named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595)&quot;">​</a></h4><p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p><h4 id="args-11" tabindex="-1">Args <a class="header-anchor" href="#args-11" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all parameter names.</li><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated parameters in the result. Defaults to True.</li></ul><h4 id="yields-6" tabindex="-1">Yields <a class="header-anchor" href="#yields-6" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Parameter): Tuple containing the name and parameter
</code></pre><h4 id="example-6" tabindex="-1">Example <a class="header-anchor" href="#example-6" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_parameters():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source" tabindex="-1"><code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source" aria-label="Permalink to &quot;`parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570)&quot;">​</a></h4><p>Return an iterator over module parameters.</p><p>This is typically passed to an optimizer.</p><h4 id="args-12" tabindex="-1">Args <a class="header-anchor" href="#args-12" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</li></ul><h4 id="yields-7" tabindex="-1">Yields <a class="header-anchor" href="#yields-7" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Parameter</strong>: module parameter</li></ul><h4 id="example-7" tabindex="-1">Example <a class="header-anchor" href="#example-7" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.parameters():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param), param.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source" tabindex="-1"><code>register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source" aria-label="Permalink to &quot;`register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>This function is deprecated in favor of <code>~tensorplay.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.</p><h4 id="returns-13" tabindex="-1">Returns <a class="header-anchor" href="#returns-13" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source" tabindex="-1"><code>register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source" aria-label="Permalink to &quot;`register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553)&quot;">​</a></h4><p>Add a buffer to the module.</p><p>This is typically used to register a buffer that should not be considered a model parameter. For example, BatchNorm&#39;s <code>running_mean</code> is not a parameter, but is part of the module&#39;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting <code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&#39;s <code>state_dict</code>.</p><p>Buffers can be accessed as attributes using given names.</p><h4 id="args-13" tabindex="-1">Args <a class="header-anchor" href="#args-13" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the buffer. The buffer can be accessed from this module using the given name</li><li><strong>tensor</strong> (<code>Tensor or None</code>): buffer to be registered. If <code>None</code>, then operations that run on buffers, such as <code>cuda</code>, are ignored. If <code>None</code>, the buffer is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li><li><strong>persistent</strong> (<code>bool</code>): whether the buffer is part of this module&#39;s <code>state_dict</code>.</li></ul><h4 id="example-8" tabindex="-1">Example <a class="header-anchor" href="#example-8" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.register_buffer(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_mean&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, tensorplay.zeros(num_features))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><hr><h4 id="register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source" tabindex="-1"><code>register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source" aria-label="Permalink to &quot;`register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592)&quot;">​</a></h4><p>Register a forward hook on the module.</p><p>The hook will be called every time after <code>forward</code> has computed an output.</p><p>If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after <code>forward</code> is called. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-14" tabindex="-1">Args <a class="header-anchor" href="#args-14" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If <code>True</code>, the provided <code>hook</code> will be fired before all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward</code> hooks registered with <code>register_module_forward_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If <code>True</code>, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>always_call</strong> (<code>bool</code>): If <code>True</code> the <code>hook</code> will be run regardless of whether an exception is raised while calling the Module.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-14" tabindex="-1">Returns <a class="header-anchor" href="#returns-14" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source" tabindex="-1"><code>register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source" aria-label="Permalink to &quot;`register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526)&quot;">​</a></h4><p>Register a forward pre-hook on the module.</p><p>The hook will be called every time before <code>forward</code> is invoked.</p><p>If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> of modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> and</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> kwargs</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-15" tabindex="-1">Args <a class="header-anchor" href="#args-15" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward_pre</code> hooks registered with <code>register_module_forward_pre_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If true, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-15" tabindex="-1">Returns <a class="header-anchor" href="#returns-15" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source" tabindex="-1"><code>register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source" aria-label="Permalink to &quot;`register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>The hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:</p><pre><code>1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.
2. If none of the module inputs require gradients, the hook will fire when the gradients are computed
   with respect to module outputs.
3. If none of the module outputs require gradients, then the hooks will not fire.
</code></pre><p>The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_input, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Tensor) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_input</code> and <code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of <code>grad_input</code> in subsequent computations. <code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in <code>grad_input</code> and <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> outputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-16" tabindex="-1">Args <a class="header-anchor" href="#args-16" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward</code> hooks registered with <code>register_module_full_backward_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-16" tabindex="-1">Returns <a class="header-anchor" href="#returns-16" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source" tabindex="-1"><code>register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source" aria-label="Permalink to &quot;`register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287)&quot;">​</a></h4><p>Register a backward pre-hook on the module.</p><p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tuple[Tensor] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of <code>grad_output</code> in subsequent computations. Entries in <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-17" tabindex="-1">Args <a class="header-anchor" href="#args-17" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward_pre</code> hooks registered with <code>register_module_full_backward_pre_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-17" tabindex="-1">Returns <a class="header-anchor" href="#returns-17" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-post-hook-self-hook-source" tabindex="-1"><code>register_load_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-post-hook-self-hook-source" aria-label="Permalink to &quot;`register_load_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226)&quot;">​</a></h4><p>Register a post-hook to be run after module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, incompatible_keys) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.</p><p>The given incompatible_keys can be modified inplace if needed.</p><p>Note that the checks performed when calling <code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.</p><h4 id="returns-18" tabindex="-1">Returns <a class="header-anchor" href="#returns-18" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-pre-hook-self-hook-source" tabindex="-1"><code>register_load_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-pre-hook-self-hook-source" aria-label="Permalink to &quot;`register_load_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214)&quot;">​</a></h4><p>Register a pre-hook to be run before module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # noqa: B950</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="arguments" tabindex="-1">Arguments <a class="header-anchor" href="#arguments" aria-label="Permalink to &quot;Arguments&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): Callable hook that will be invoked before loading the state dict.</li></ul><hr><h4 id="register-module-self-name-str-module-optional-forwardref-module-none-source" tabindex="-1"><code>register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-module-self-name-str-module-optional-forwardref-module-none-source" aria-label="Permalink to &quot;`register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695)&quot;">​</a></h4><p>Alias for <code>add_module</code>.</p><hr><h4 id="register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source" tabindex="-1"><code>register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source" aria-label="Permalink to &quot;`register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617)&quot;">​</a></h4><p>Add a parameter to the module.</p><p>The parameter can be accessed as an attribute using given name.</p><h4 id="args-18" tabindex="-1">Args <a class="header-anchor" href="#args-18" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the parameter. The parameter can be accessed from this module using the given name</li><li><strong>param</strong> (<code>Parameter or None</code>): parameter to be added to the module. If <code>None</code>, then operations that run on parameters, such as <code>cuda</code>, are ignored. If <code>None</code>, the parameter is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li></ul><hr><h4 id="register-state-dict-post-hook-self-hook-source" tabindex="-1"><code>register_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-post-hook-self-hook-source" aria-label="Permalink to &quot;`register_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017)&quot;">​</a></h4><p>Register a post-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can modify the <code>state_dict</code> inplace.</p><hr><h4 id="register-state-dict-pre-hook-self-hook-source" tabindex="-1"><code>register_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-pre-hook-self-hook-source" aria-label="Permalink to &quot;`register_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041)&quot;">​</a></h4><p>Register a pre-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, prefix, keep_vars) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p><hr><h4 id="requires-grad-self-requires-grad-bool-true-self-source" tabindex="-1"><code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#requires-grad-self-requires-grad-bool-true-self-source" aria-label="Permalink to &quot;`requires_grad_(self, requires_grad: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826)&quot;">​</a></h4><p>Change if autograd should record operations on parameters in this module.</p><p>This method sets the parameters&#39; <code>requires_grad</code> attributes in-place.</p><p>This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).</p><p>See <code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.</p><h4 id="args-19" tabindex="-1">Args <a class="header-anchor" href="#args-19" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>requires_grad</strong> (<code>bool</code>): whether autograd should record operations on parameters in this module. Default: <code>True</code>.</li></ul><h4 id="returns-19" tabindex="-1">Returns <a class="header-anchor" href="#returns-19" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="set-extra-state-self-state-any-none-source" tabindex="-1"><code>set_extra_state(self, state: Any) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-extra-state-self-state-any-none-source" aria-label="Permalink to &quot;`set_extra_state(self, state: Any) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938)&quot;">​</a></h4><p>Set extra state contained in the loaded <code>state_dict</code>.</p><p>This function is called from <code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding <code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.</p><h4 id="args-20" tabindex="-1">Args <a class="header-anchor" href="#args-20" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state</strong> (<code>dict</code>): Extra state from the <code>state_dict</code></li></ul><hr><h4 id="set-submodule-self-target-str-module-module-strict-bool-false-none-source" tabindex="-1"><code>set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-submodule-self-target-str-module-module-strict-bool-false-none-source" aria-label="Permalink to &quot;`set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764)&quot;">​</a></h4><p>Set the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>If <code>strict</code> is set to <code>False</code> (default), the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>strict</code> is set to <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule does not exist.</p></div><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(3, 3, 3) ) (linear): Linear(3, 3) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To override the <code>Conv2d</code> with a new submodule <code>Linear</code>, you could call <code>set_submodule(&quot;net_b.net_c.conv&quot;, nn.Linear(1, 1))</code> where <code>strict</code> could be <code>True</code> or <code>False</code></p><p>To add a new submodule <code>Conv2d</code> to the existing <code>net_b</code> module, you would call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1))</code>.</p><p>In the above if you set <code>strict=True</code> and call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1), strict=True)</code>, an AttributeError will be raised because <code>net_b</code> does not have a submodule named <code>conv</code>.</p><h4 id="args-21" tabindex="-1">Args <a class="header-anchor" href="#args-21" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li><li><strong>module</strong>: The module to set the submodule to.</li><li><strong>strict</strong>: If <code>False</code>, the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule doesn&#39;t already exist.</li></ul><h4 id="raises-3" tabindex="-1">Raises <a class="header-anchor" href="#raises-3" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>ValueError</strong>: If the <code>target</code> string is empty or if <code>module</code> is not an instance of <code>nn.Module</code>.</li><li><strong>AttributeError</strong>: If at any point along the path resulting from the <code>target</code> string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="share-memory-self-self-source" tabindex="-1"><code>share_memory(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#share-memory-self-self-source" aria-label="Permalink to &quot;`share_memory(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877)&quot;">​</a></h4><p>See <code>tensorplay.Tensor.share_memory_</code>.</p><hr><h4 id="state-dict-self-args-destination-none-prefix-keep-vars-false-source" tabindex="-1"><code>state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#state-dict-self-args-destination-none-prefix-keep-vars-false-source" aria-label="Permalink to &quot;`state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105)&quot;">​</a></h4><p>Return a dictionary containing references to the whole state of the module.</p><p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>The returned object is a shallow copy. It contains references to the module&#39;s parameters and buffers.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Currently <code>state_dict()</code> also accepts positional arguments for <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However, this is being deprecated and keyword arguments will be enforced in future releases.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Please avoid the use of argument <code>destination</code> as it is not designed for end-users.</p></div><h4 id="args-22" tabindex="-1">Args <a class="header-anchor" href="#args-22" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>destination</strong> (<code>dict, optional</code>): If provided, the state of module will be updated into the dict and the same object is returned. Otherwise, an <code>OrderedDict</code> will be created and returned.</li><li><strong>Default</strong>: <code>None</code>.</li><li><strong>prefix</strong> (<code>str, optional</code>): a prefix added to parameter and buffer names to compose the keys in state_dict. Default: <code>&#39;&#39;</code>.</li><li><strong>keep_vars</strong> (<code>bool, optional</code>): by default the <code>~tensorplay.Tensor</code> s returned in the state dict are detached from autograd. If it&#39;s set to <code>True</code>, detaching will not be performed.</li><li><strong>Default</strong>: <code>False</code>.</li></ul><h4 id="returns-20" tabindex="-1">Returns <a class="header-anchor" href="#returns-20" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>dict</strong>: a dictionary containing a whole state of the module</li></ul><h4 id="example-9" tabindex="-1">Example <a class="header-anchor" href="#example-9" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">module.state_dict().keys()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;weight&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><hr><h4 id="to-self-args-kwargs-source" tabindex="-1"><code>to(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-self-args-kwargs-source" aria-label="Permalink to &quot;`to(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151)&quot;">​</a></h4><p>Move and/or cast the parameters and buffers.</p><p>This can be called as</p><p>.. function:: to(device=None, dtype=None, non_blocking=False) :noindex:</p><p>.. function:: to(dtype, non_blocking=False) :noindex:</p><p>.. function:: to(tensor, non_blocking=False) :noindex:</p><p>.. function:: to(memory_format=tensorplay.channels_last) :noindex:</p><p>Its signature is similar to <code>tensorplay.Tensor.to</code>, but only accepts floating point or complex <code>dtype</code>\ s. In addition, this method will only cast the floating point or complex parameters and buffers to <code>dtype</code> (if given). The integral parameters and buffers will be moved <code>device</code>, if that is given, but with dtypes unchanged. When <code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.</p><p>See below for examples.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-23" tabindex="-1">Args <a class="header-anchor" href="#args-23" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): the desired device of the parameters and buffers in this module</li><li><strong>dtype</strong> (<code>tensorplay.dtype</code>): the desired floating point or complex dtype of the parameters and buffers in this module</li><li><strong>tensor</strong> (<code>tensorplay.Tensor</code>): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module</li><li><strong>memory_format</strong> (<code>tensorplay.memory_format</code>): the desired memory format for 4D parameters and buffers in this module (keyword only argument)</li></ul><h4 id="returns-21" tabindex="-1">Returns <a class="header-anchor" href="#returns-21" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="examples-1" tabindex="-1">Examples <a class="header-anchor" href="#examples-1" aria-label="Permalink to &quot;Examples&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +IGNORE_WANT(&quot;non-deterministic&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(tensorplay.double)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float64)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +REQUIRES(env:TENSORPLAY_DOCTEST_CUDA1)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">gpu1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cuda:1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(gpu1, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.half, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">non_blocking</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;cuda:1&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cpu </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cpu&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(cpu)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).to(tensorplay.cdouble)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3741</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,  </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2382</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5593</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.4443</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear(tensorplay.ones(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.cdouble))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br></div></div><hr><h4 id="to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source" tabindex="-1"><code>to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source" aria-label="Permalink to &quot;`to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119)&quot;">​</a></h4><p>Move the parameters and buffers to the specified device without copying storage.</p><h4 id="args-24" tabindex="-1">Args <a class="header-anchor" href="#args-24" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): The desired device of the parameters and buffers in this module.</li><li><strong>recurse</strong> (<code>bool</code>): Whether parameters and buffers of submodules should be recursively moved to the specified device.</li></ul><h4 id="returns-22" tabindex="-1">Returns <a class="header-anchor" href="#returns-22" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="train-self-mode-bool-true-self-source" tabindex="-1"><code>train(self, mode: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#train-self-mode-bool-true-self-source" aria-label="Permalink to &quot;`train(self, mode: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786)&quot;">​</a></h4><p>Set the module in training mode.</p><p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e., whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, etc.</p><h4 id="args-25" tabindex="-1">Args <a class="header-anchor" href="#args-25" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>mode</strong> (<code>bool</code>): whether to set training mode (<code>True</code>) or evaluation mode (<code>False</code>). Default: <code>True</code>.</li></ul><h4 id="returns-23" tabindex="-1">Returns <a class="header-anchor" href="#returns-23" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="type-self-dst-type-union-tensorplay-dtype-str-self-source" tabindex="-1"><code>type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#type-self-dst-type-union-tensorplay-dtype-str-self-source" aria-label="Permalink to &quot;`type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061)&quot;">​</a></h4><p>Casts all parameters and buffers to <code>dst_type</code>.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-26" tabindex="-1">Args <a class="header-anchor" href="#args-26" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>dst_type</strong> (<code>type or string</code>): the desired type</li></ul><h4 id="returns-24" tabindex="-1">Returns <a class="header-anchor" href="#returns-24" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="zero-grad-self-set-to-none-bool-true-none-source" tabindex="-1"><code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#zero-grad-self-set-to-none-bool-true-none-source" aria-label="Permalink to &quot;`zero_grad(self, set_to_none: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849)&quot;">​</a></h4><p>Reset gradients of all model parameters.</p><p>See similar function under <code>tensorplay.optim.Optimizer</code> for more context.</p><h4 id="args-27" tabindex="-1">Args <a class="header-anchor" href="#args-27" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>set_to_none</strong> (<code>bool</code>): instead of setting to zero, set the grads to None. See <code>tensorplay.optim.Optimizer.zero_grad</code> for details.</li></ul><hr></details><h3 id="class-unflatten-source" tabindex="-1"><code>class Unflatten</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/flatten.py#L64" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#class-unflatten-source" aria-label="Permalink to &quot;`class Unflatten` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/flatten.py#L64)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Unflatten(dim: Union[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">str</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], unflattened_size: Union[tensorplay.Size, list[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], tuple[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">...</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], tuple[tuple[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">str</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]]]) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Bases:</strong> <code>Module</code></p><p>Unflattens a tensor dim expanding it to a desired shape. For use with <code>~nn.Sequential</code>.</p><ul><li><p><code>dim</code> specifies the dimension of the input tensor to be unflattened, and it can be either <code>int</code> or <code>str</code> when <code>Tensor</code> or <code>NamedTensor</code> is used, respectively.</p></li><li><p><code>unflattened_size</code> is the new shape of the unflattened dimension of the tensor and it can be a <code>tuple</code> of ints or a <code>list</code> of ints or <code>tensorplay.Size</code> for <code>Tensor</code> input; a <code>NamedShape</code> (tuple of <code>(name, size)</code> tuples) for <code>NamedTensor</code> input.</p></li></ul><h4 id="shape-1" tabindex="-1">Shape <a class="header-anchor" href="#shape-1" aria-label="Permalink to &quot;Shape&quot;">​</a></h4><ul><li>Input: <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="10.276ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4542.1 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(389,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(889,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(1333.7,0)"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(646,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mtext"><path data-c="64" d="M376 495Q376 511 376 535T377 568Q377 613 367 624T316 637H298V660Q298 683 300 683L310 684Q320 685 339 686T376 688Q393 689 413 690T443 693T454 694H457V390Q457 84 458 81Q461 61 472 55T517 46H535V0Q533 0 459 -5T380 -11H373V44L365 37Q307 -11 235 -11Q158 -11 96 50T34 215Q34 315 97 378T244 442Q319 442 376 393V495ZM373 342Q328 405 260 405Q211 405 173 369Q146 341 139 305T131 211Q131 155 138 120T173 59Q203 26 251 26Q322 26 373 103V342Z" style="stroke-width:3;"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(556,0)" style="stroke-width:3;"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(834,0)" style="stroke-width:3;"></path></g></g></g><g data-mml-node="mo" transform="translate(3208.4,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3653.1,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(4153.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mo>∗</mo><mo>,</mo><msub><mi>S</mi><mrow data-mjx-texclass="ORD"><mtext>dim</mtext></mrow></msub><mo>,</mo><mo>∗</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>, where <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.357ex;" xmlns="http://www.w3.org/2000/svg" width="4.242ex" height="1.952ex" role="img" focusable="false" viewBox="0 -705 1874.7 862.8" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(646,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mtext"><path data-c="64" d="M376 495Q376 511 376 535T377 568Q377 613 367 624T316 637H298V660Q298 683 300 683L310 684Q320 685 339 686T376 688Q393 689 413 690T443 693T454 694H457V390Q457 84 458 81Q461 61 472 55T517 46H535V0Q533 0 459 -5T380 -11H373V44L365 37Q307 -11 235 -11Q158 -11 96 50T34 215Q34 315 97 378T244 442Q319 442 376 393V495ZM373 342Q328 405 260 405Q211 405 173 369Q146 341 139 305T131 211Q131 155 138 120T173 59Q203 26 251 26Q322 26 373 103V342Z" style="stroke-width:3;"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(556,0)" style="stroke-width:3;"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(834,0)" style="stroke-width:3;"></path></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>S</mi><mrow data-mjx-texclass="ORD"><mtext>dim</mtext></mrow></msub></math></mjx-assistive-mml></mjx-container> is the size at dimension <code>dim</code> and <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:0.079ex;" xmlns="http://www.w3.org/2000/svg" width="1.131ex" height="0.973ex" role="img" focusable="false" viewBox="0 -465 500 430" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>∗</mo></math></mjx-assistive-mml></mjx-container> means any number of dimensions including none.</li><li>Output: <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="16.291ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 7200.5 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(389,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(889,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(1333.7,0)"><g data-mml-node="mi"><path data-c="1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(716,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(2453.2,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2897.9,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3342.6,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3787.2,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(4231.9,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(4676.6,0)"><g data-mml-node="mi"><path data-c="1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(716,-150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(5866.8,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(6311.5,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(6811.5,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mo>∗</mo><mo>,</mo><msub><mi>U</mi><mn>1</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>U</mi><mi>n</mi></msub><mo>,</mo><mo>∗</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>, where <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.735ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 767 705" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>U</mi></math></mjx-assistive-mml></mjx-container> = <code>unflattened_size</code> and <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.777ex;" xmlns="http://www.w3.org/2000/svg" width="14.841ex" height="2.563ex" role="img" focusable="false" viewBox="0 -789.6 6559.6 1132.9" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="munderover"><g data-mml-node="mo"><path data-c="220F" d="M158 656Q147 684 131 694Q110 707 69 710H55V750H888V710H874Q840 708 820 698T795 678T786 656V-155Q798 -206 874 -210H888V-250H570V-210H584Q618 -208 638 -197T663 -178T673 -155V710H270V277L271 -155Q283 -206 359 -210H373V-250H55V-210H69Q103 -208 123 -197T148 -178T158 -155V656Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(977,477.1) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(977,-285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g></g></g><g data-mml-node="msub" transform="translate(2341.3,0)"><g data-mml-node="mi"><path data-c="1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(716,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(3629,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(4684.8,0)"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(646,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mtext"><path data-c="64" d="M376 495Q376 511 376 535T377 568Q377 613 367 624T316 637H298V660Q298 683 300 683L310 684Q320 685 339 686T376 688Q393 689 413 690T443 693T454 694H457V390Q457 84 458 81Q461 61 472 55T517 46H535V0Q533 0 459 -5T380 -11H373V44L365 37Q307 -11 235 -11Q158 -11 96 50T34 215Q34 315 97 378T244 442Q319 442 376 393V495ZM373 342Q328 405 260 405Q211 405 173 369Q146 341 139 305T131 211Q131 155 138 120T173 59Q203 26 251 26Q322 26 373 103V342Z" style="stroke-width:3;"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(556,0)" style="stroke-width:3;"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(834,0)" style="stroke-width:3;"></path></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><munderover><mo data-mjx-texclass="OP">∏</mo><mrow data-mjx-texclass="ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>U</mi><mi>i</mi></msub><mo>=</mo><msub><mi>S</mi><mrow data-mjx-texclass="ORD"><mtext>dim</mtext></mrow></msub></math></mjx-assistive-mml></mjx-container>.</li></ul><h4 id="args-28" tabindex="-1">Args <a class="header-anchor" href="#args-28" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>dim</strong> (<code>Union[int, str]</code>): Dimension to be unflattened</li><li><strong>unflattened_size</strong> (<code>Union[tensorplay.Size, Tuple, List, NamedShape]</code>): New shape of the unflattened dimension</li></ul><h4 id="examples-2" tabindex="-1">Examples <a class="header-anchor" href="#examples-2" aria-label="Permalink to &quot;Examples&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.randn(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">50</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># With tuple of ints</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">50</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">50</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">),</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    nn.Unflatten(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">output </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> m(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">output.size()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    tensorplay.Size([</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># With tensorplay.Size</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">50</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">50</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">),</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    nn.Unflatten(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, tensorplay.Size([</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">output </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> m(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">output.size()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    tensorplay.Size([</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># With namedshape (tuple of tuples)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.randn(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">50</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">names</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;N&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;features&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">unflatten </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Unflatten(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;features&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, ((</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;C&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;H&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;W&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">output </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> unflatten(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">output.size()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    tensorplay.Size([</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br></div></div><details><summary>Methods</summary><h4 id="init-self-dim-union-int-str-unflattened-size-union-tensorplay-size-list-int-tuple-int-tuple-tuple-str-int-none-source" tabindex="-1"><code>__init__(self, dim: Union[int, str], unflattened_size: Union[tensorplay.Size, list[int], tuple[int, ...], tuple[tuple[str, int]]]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/flatten.py#L117" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#init-self-dim-union-int-str-unflattened-size-union-tensorplay-size-list-int-tuple-int-tuple-tuple-str-int-none-source" aria-label="Permalink to &quot;`__init__(self, dim: Union[int, str], unflattened_size: Union[tensorplay.Size, list[int], tuple[int, ...], tuple[tuple[str, int]]]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/flatten.py#L117)&quot;">​</a></h4><p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p><hr><h4 id="add-module-self-name-str-module-optional-forwardref-module-none-source-1" tabindex="-1"><code>add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#add-module-self-name-str-module-optional-forwardref-module-none-source-1" aria-label="Permalink to &quot;`add_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L667)&quot;">​</a></h4><p>Add a child module to the current module.</p><p>The module can be accessed as an attribute using the given name.</p><h4 id="args-29" tabindex="-1">Args <a class="header-anchor" href="#args-29" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the child module. The child module can be accessed from this module using the given name</li><li><strong>module</strong> (<code>Module</code>): child module to be added to the module.</li></ul><hr><h4 id="apply-self-fn-callable-forwardref-module-nonetype-self-source-1" tabindex="-1"><code>apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#apply-self-fn-callable-forwardref-module-nonetype-self-source-1" aria-label="Permalink to &quot;`apply(self, fn: Callable[[ForwardRef(&#39;Module&#39;)], NoneType]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L990)&quot;">​</a></h4><p>Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self.</p><p>Typical use includes initializing the parameters of a model (see also <code>nn-init-doc</code>).</p><h4 id="args-30" tabindex="-1">Args <a class="header-anchor" href="#args-30" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>fn</strong> (``Module<code> -&gt; None</code>): function to be applied to each submodule</li></ul><h4 id="returns-25" tabindex="-1">Returns <a class="header-anchor" href="#returns-25" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="example-10" tabindex="-1">Example <a class="header-anchor" href="#example-10" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">@tensorplay.no_grad</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> init_weights</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        m.weight.fill_(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(m.weight)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net.apply(init_weights)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">., </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><hr><h4 id="bfloat16-self-self-source-1" tabindex="-1"><code>bfloat16(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#bfloat16-self-self-source-1" aria-label="Permalink to &quot;`bfloat16(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1108)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-26" tabindex="-1">Returns <a class="header-anchor" href="#returns-26" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source-1" tabindex="-1"><code>buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#buffers-self-recurse-bool-true-collections-abc-iterator-tensorplay-c-tensorbase-source-1" aria-label="Permalink to &quot;`buffers(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay._C.TensorBase]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2627)&quot;">​</a></h4><p>Return an iterator over module buffers.</p><h4 id="args-31" tabindex="-1">Args <a class="header-anchor" href="#args-31" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.</li></ul><h4 id="yields-8" tabindex="-1">Yields <a class="header-anchor" href="#yields-8" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>tensorplay.Tensor: module buffer
</code></pre><h4 id="example-11" tabindex="-1">Example <a class="header-anchor" href="#example-11" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.buffers():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf), buf.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="children-self-collections-abc-iterator-module-source-1" tabindex="-1"><code>children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#children-self-collections-abc-iterator-module-source-1" aria-label="Permalink to &quot;`children(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2681)&quot;">​</a></h4><p>Return an iterator over immediate children modules.</p><h4 id="yields-9" tabindex="-1">Yields <a class="header-anchor" href="#yields-9" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a child module</li></ul><hr><h4 id="compile-self-args-kwargs-source-1" tabindex="-1"><code>compile(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#compile-self-args-kwargs-source-1" aria-label="Permalink to &quot;`compile(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2944)&quot;">​</a></h4><p>Compile this Module&#39;s forward using <code>tensorplay.compile</code>.</p><p>This Module&#39;s <code>__call__</code> method is compiled and all arguments are passed as-is to <code>tensorplay.compile</code>.</p><p>See <code>tensorplay.compile</code> for details on the arguments for this function.</p><hr><h4 id="cpu-self-self-source-1" tabindex="-1"><code>cpu(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cpu-self-self-source-1" aria-label="Permalink to &quot;`cpu(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1050)&quot;">​</a></h4><p>Move all model parameters and buffers to the CPU.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-27" tabindex="-1">Returns <a class="header-anchor" href="#returns-27" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source-1" tabindex="-1"><code>cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#cuda-self-device-union-int-tensorplay-device-nonetype-none-self-source-1" aria-label="Permalink to &quot;`cuda(self, device: Union[int, tensorplay.Device, NoneType] = None) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1031)&quot;">​</a></h4><p>Move all model parameters and buffers to the GPU.</p><p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on GPU while being optimized.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-32" tabindex="-1">Args <a class="header-anchor" href="#args-32" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>int, optional</code>): if specified, all parameters will be copied to that device</li></ul><h4 id="returns-28" tabindex="-1">Returns <a class="header-anchor" href="#returns-28" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="double-self-self-source-1" tabindex="-1"><code>double(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#double-self-self-source-1" aria-label="Permalink to &quot;`double(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1086)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-29" tabindex="-1">Returns <a class="header-anchor" href="#returns-29" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="eval-self-self-source-1" tabindex="-1"><code>eval(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#eval-self-self-source-1" aria-label="Permalink to &quot;`eval(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2808)&quot;">​</a></h4><p>Set the module in evaluation mode.</p><p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e. whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, etc.</p><p>This is equivalent with <code>self.train(False) &lt;tensorplay.nn.Module.train&gt;</code>.</p><p>See <code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.</p><h4 id="returns-30" tabindex="-1">Returns <a class="header-anchor" href="#returns-30" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="extra-repr-self-str-source-1" tabindex="-1"><code>extra_repr(self) -&gt; str</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/flatten.py#L165" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#extra-repr-self-str-source-1" aria-label="Permalink to &quot;`extra_repr(self) -&gt; str` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/flatten.py#L165)&quot;">​</a></h4><p>Returns the extra representation of the module.</p><hr><h4 id="float-self-self-source-1" tabindex="-1"><code>float(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#float-self-self-source-1" aria-label="Permalink to &quot;`float(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1075)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-31" tabindex="-1">Returns <a class="header-anchor" href="#returns-31" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="forward-self-input-tensorplay-c-tensorbase-tensorplay-c-tensorbase-source-1" tabindex="-1"><code>forward(self, input: tensorplay._C.TensorBase) -&gt; tensorplay._C.TensorBase</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/flatten.py#L159" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#forward-self-input-tensorplay-c-tensorbase-tensorplay-c-tensorbase-source-1" aria-label="Permalink to &quot;`forward(self, input: tensorplay._C.TensorBase) -&gt; tensorplay._C.TensorBase` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/flatten.py#L159)&quot;">​</a></h4><p>Runs the forward pass.</p><hr><h4 id="get-buffer-self-target-str-tensor-source-1" tabindex="-1"><code>get_buffer(self, target: str) -&gt; &#39;Tensor&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-buffer-self-target-str-tensor-source-1" aria-label="Permalink to &quot;`get_buffer(self, target: str) -&gt; &#39;Tensor&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L881)&quot;">​</a></h4><p>Return the buffer given by <code>target</code> if it exists, otherwise throw an error.</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-33" tabindex="-1">Args <a class="header-anchor" href="#args-33" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the buffer to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-32" tabindex="-1">Returns <a class="header-anchor" href="#returns-32" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.Tensor: The buffer referenced by ``target``
</code></pre><h4 id="raises-4" tabindex="-1">Raises <a class="header-anchor" href="#raises-4" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not a buffer</li></ul><hr><h4 id="get-extra-state-self-any-source-1" tabindex="-1"><code>get_extra_state(self) -&gt; Any</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-extra-state-self-any-source-1" aria-label="Permalink to &quot;`get_extra_state(self) -&gt; Any` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L917)&quot;">​</a></h4><p>Return any extra state to include in the module&#39;s state_dict.</p><p>Implement this and a corresponding <code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module&#39;s <code>state_dict()</code>.</p><p>Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.</p><h4 id="returns-33" tabindex="-1">Returns <a class="header-anchor" href="#returns-33" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>object</strong>: Any extra state to store in the module&#39;s state_dict</li></ul><hr><h4 id="get-parameter-self-target-str-parameter-source-1" tabindex="-1"><code>get_parameter(self, target: str) -&gt; &#39;Parameter&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-parameter-self-target-str-parameter-source-1" aria-label="Permalink to &quot;`get_parameter(self, target: str) -&gt; &#39;Parameter&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L845)&quot;">​</a></h4><p>Return the parameter given by <code>target</code> if it exists, otherwise throw an error.</p><p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method&#39;s functionality as well as how to correctly specify <code>target</code>.</p><h4 id="args-34" tabindex="-1">Args <a class="header-anchor" href="#args-34" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the Parameter to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</li></ul><h4 id="returns-34" tabindex="-1">Returns <a class="header-anchor" href="#returns-34" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Parameter: The Parameter referenced by ``target``
</code></pre><h4 id="raises-5" tabindex="-1">Raises <a class="header-anchor" href="#raises-5" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If the target string references an invalid path or resolves to something that is not an <code>nn.Parameter</code></li></ul><hr><h4 id="get-submodule-self-target-str-module-source-1" tabindex="-1"><code>get_submodule(self, target: str) -&gt; &#39;Module&#39;</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#get-submodule-self-target-str-module-source-1" aria-label="Permalink to &quot;`get_submodule(self, target: str) -&gt; &#39;Module&#39;` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L699)&quot;">​</a></h4><p>Return the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> which has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(&quot;net_b.linear&quot;)</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(&quot;net_b.net_c.conv&quot;)</code>.</p><p>The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.</p><h4 id="args-35" tabindex="-1">Args <a class="header-anchor" href="#args-35" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li></ul><h4 id="returns-35" tabindex="-1">Returns <a class="header-anchor" href="#returns-35" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>tensorplay.nn.Module: The submodule referenced by ``target``
</code></pre><h4 id="raises-6" tabindex="-1">Raises <a class="header-anchor" href="#raises-6" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>AttributeError</strong>: If at any point along the path resulting from the target string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="half-self-self-source-1" tabindex="-1"><code>half(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#half-self-self-source-1" aria-label="Permalink to &quot;`half(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1097)&quot;">​</a></h4><p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="returns-36" tabindex="-1">Returns <a class="header-anchor" href="#returns-36" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source-1" tabindex="-1"><code>load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#load-state-dict-self-state-dict-collections-abc-mapping-str-typing-any-strict-bool-true-assign-bool-false-source-1" aria-label="Permalink to &quot;`load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2436)&quot;">​</a></h4><p>Copy parameters and buffers from <code>state_dict</code> into this module and its descendants.</p><p>If <code>strict</code> is <code>True</code>, then the keys of <code>state_dict</code> must exactly match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function.</p><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>If <code>assign</code> is <code>True</code> the optimizer must be created after the call to <code>load_state_dict</code> unless <code>~tensorplay.__future__.get_swap_module_params_on_conversion</code> is <code>True</code>.</p></div><h4 id="args-36" tabindex="-1">Args <a class="header-anchor" href="#args-36" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state_dict</strong> (<code>dict</code>): a dict containing parameters and persistent buffers.</li><li><strong>strict</strong> (<code>bool, optional</code>): whether to strictly enforce that the keys in <code>state_dict</code> match the keys returned by this module&#39;s <code>~tensorplay.nn.Module.state_dict</code> function. Default: <code>True</code></li><li><strong>assign</strong> (<code>bool, optional</code>): When set to <code>False</code>, the properties of the tensors in the current module are preserved whereas setting it to <code>True</code> preserves properties of the Tensors in the state dict. The only exception is the <code>requires_grad</code> field of <code>~tensorplay.nn.Parameter</code> for which the value from the module is preserved. Default: <code>False</code></li></ul><h4 id="returns-37" tabindex="-1">Returns <a class="header-anchor" href="#returns-37" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:
    * ``missing_keys`` is a list of str containing any keys that are expected
        by this module but missing from the provided ``state_dict``.
    * ``unexpected_keys`` is a list of str containing the keys that are not
        expected by this module but present in the provided ``state_dict``.
</code></pre><h4 id="note-3" tabindex="-1">Note <a class="header-anchor" href="#note-3" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>If a parameter or buffer is registered as ``None`` and its corresponding key
exists in `state_dict`, `load_state_dict` will raise a
``RuntimeError``.
</code></pre><hr><h4 id="modules-self-collections-abc-iterator-module-source-1" tabindex="-1"><code>modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#modules-self-collections-abc-iterator-module-source-1" aria-label="Permalink to &quot;`modules(self) -&gt; collections.abc.Iterator[&#39;Module&#39;]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2710)&quot;">​</a></h4><p>Return an iterator over all modules in the network.</p><h4 id="yields-10" tabindex="-1">Yields <a class="header-anchor" href="#yields-10" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Module</strong>: a module in the network</li></ul><h4 id="note-4" tabindex="-1">Note <a class="header-anchor" href="#note-4" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-12" tabindex="-1">Example <a class="header-anchor" href="#example-12" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source-1" tabindex="-1"><code>named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-buffers-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-c-tensorbase-source-1" aria-label="Permalink to &quot;`named_buffers(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay._C.TensorBase]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2650)&quot;">​</a></h4><p>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p><h4 id="args-37" tabindex="-1">Args <a class="header-anchor" href="#args-37" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all buffer names.</li><li><strong>recurse</strong> (<code>bool, optional</code>): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Defaults to True.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated buffers in the result. Defaults to True.</li></ul><h4 id="yields-11" tabindex="-1">Yields <a class="header-anchor" href="#yields-11" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, tensorplay.Tensor): Tuple containing the name and buffer
</code></pre><h4 id="example-13" tabindex="-1">Example <a class="header-anchor" href="#example-13" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, buf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_buffers():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_var&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(buf.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-children-self-collections-abc-iterator-tuple-str-module-source-1" tabindex="-1"><code>named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-children-self-collections-abc-iterator-tuple-str-module-source-1" aria-label="Permalink to &quot;`named_children(self) -&gt; collections.abc.Iterator[tuple[str, &#39;Module&#39;]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2690)&quot;">​</a></h4><p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p><h4 id="yields-12" tabindex="-1">Yields <a class="header-anchor" href="#yields-12" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple containing a name and child module
</code></pre><h4 id="example-14" tabindex="-1">Example <a class="header-anchor" href="#example-14" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, module </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.named_children():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv4&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;conv5&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(module)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source-1" tabindex="-1"><code>named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-modules-self-memo-optional-set-module-none-prefix-str-remove-duplicate-bool-true-source-1" aria-label="Permalink to &quot;`named_modules(self, memo: Optional[set[&#39;Module&#39;]] = None, prefix: str = &#39;&#39;, remove_duplicate: bool = True)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2737)&quot;">​</a></h4><p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p><h4 id="args-38" tabindex="-1">Args <a class="header-anchor" href="#args-38" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>memo</strong>: a memo to store the set of modules already added to the result</li><li><strong>prefix</strong>: a prefix that will be added to the name of the module</li><li><strong>remove_duplicate</strong>: whether to remove the duplicated module instances in the result or not</li></ul><h4 id="yields-13" tabindex="-1">Yields <a class="header-anchor" href="#yields-13" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Module): Tuple of name and module
</code></pre><h4 id="note-5" tabindex="-1">Note <a class="header-anchor" href="#note-5" aria-label="Permalink to &quot;Note&quot;">​</a></h4><pre><code>Duplicate modules are returned only once. In the following
example, ``l`` will be returned only once.
</code></pre><h4 id="example-15" tabindex="-1">Example <a class="header-anchor" href="#example-15" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">net </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Sequential(l, l)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, m </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(net.named_modules()):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(idx, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;-&gt;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, m)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Sequential(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">): Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;"> -&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;0&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><hr><h4 id="named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source-1" tabindex="-1"><code>named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#named-parameters-self-prefix-str-recurse-bool-true-remove-duplicate-bool-true-collections-abc-iterator-tuple-str-tensorplay-nn-parameter-parameter-source-1" aria-label="Permalink to &quot;`named_parameters(self, prefix: str = &#39;&#39;, recurse: bool = True, remove_duplicate: bool = True) -&gt; collections.abc.Iterator[tuple[str, tensorplay.nn.parameter.Parameter]]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2595)&quot;">​</a></h4><p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p><h4 id="args-39" tabindex="-1">Args <a class="header-anchor" href="#args-39" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>prefix</strong> (<code>str</code>): prefix to prepend to all parameter names.</li><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</li><li><strong>remove_duplicate</strong> (<code>bool, optional</code>): whether to remove the duplicated parameters in the result. Defaults to True.</li></ul><h4 id="yields-14" tabindex="-1">Yields <a class="header-anchor" href="#yields-14" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><pre><code>(str, Parameter): Tuple containing the name and parameter
</code></pre><h4 id="example-16" tabindex="-1">Example <a class="header-anchor" href="#example-16" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name, param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.named_parameters():</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param.size())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><hr><h4 id="parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source-1" tabindex="-1"><code>parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#parameters-self-recurse-bool-true-collections-abc-iterator-tensorplay-nn-parameter-parameter-source-1" aria-label="Permalink to &quot;`parameters(self, recurse: bool = True) -&gt; collections.abc.Iterator[tensorplay.nn.parameter.Parameter]` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2570)&quot;">​</a></h4><p>Return an iterator over module parameters.</p><p>This is typically passed to an optimizer.</p><h4 id="args-40" tabindex="-1">Args <a class="header-anchor" href="#args-40" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>recurse</strong> (<code>bool</code>): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</li></ul><h4 id="yields-15" tabindex="-1">Yields <a class="header-anchor" href="#yields-15" aria-label="Permalink to &quot;Yields&quot;">​</a></h4><ul><li><strong>Parameter</strong>: module parameter</li></ul><h4 id="example-17" tabindex="-1">Example <a class="header-anchor" href="#example-17" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> param </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model.parameters():</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">type</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(param), param.size())</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;class</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;tensorplay.Tensor&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">20</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">L</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><hr><h4 id="register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source-1" tabindex="-1"><code>register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-tensorplay-utils-hooks-removablehandle-source-1" aria-label="Permalink to &quot;`register_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]]) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1336)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>This function is deprecated in favor of <code>~tensorplay.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.</p><h4 id="returns-38" tabindex="-1">Returns <a class="header-anchor" href="#returns-38" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source-1" tabindex="-1"><code>register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-buffer-self-name-str-tensor-optional-tensorplay-c-tensorbase-persistent-bool-true-none-source-1" aria-label="Permalink to &quot;`register_buffer(self, name: str, tensor: Optional[tensorplay._C.TensorBase], persistent: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L553)&quot;">​</a></h4><p>Add a buffer to the module.</p><p>This is typically used to register a buffer that should not be considered a model parameter. For example, BatchNorm&#39;s <code>running_mean</code> is not a parameter, but is part of the module&#39;s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting <code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module&#39;s <code>state_dict</code>.</p><p>Buffers can be accessed as attributes using given names.</p><h4 id="args-41" tabindex="-1">Args <a class="header-anchor" href="#args-41" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the buffer. The buffer can be accessed from this module using the given name</li><li><strong>tensor</strong> (<code>Tensor or None</code>): buffer to be registered. If <code>None</code>, then operations that run on buffers, such as <code>cuda</code>, are ignored. If <code>None</code>, the buffer is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li><li><strong>persistent</strong> (<code>bool</code>): whether the buffer is part of this module&#39;s <code>state_dict</code>.</li></ul><h4 id="example-18" tabindex="-1">Example <a class="header-anchor" href="#example-18" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.register_buffer(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;running_mean&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, tensorplay.zeros(num_features))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><hr><h4 id="register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source-1" tabindex="-1"><code>register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-hook-self-hook-union-callable-t-tuple-any-any-optional-any-callable-t-tuple-any-dict-str-any-any-optional-any-prepend-bool-false-with-kwargs-bool-false-always-call-bool-false-tensorplay-utils-hooks-removablehandle-source-1" aria-label="Permalink to &quot;`register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1592)&quot;">​</a></h4><p>Register a forward hook on the module.</p><p>The hook will be called every time after <code>forward</code> has computed an output.</p><p>If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after <code>forward</code> is called. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs, output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified output</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-42" tabindex="-1">Args <a class="header-anchor" href="#args-42" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If <code>True</code>, the provided <code>hook</code> will be fired before all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward</code> hooks registered with <code>register_module_forward_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If <code>True</code>, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>always_call</strong> (<code>bool</code>): If <code>True</code> the <code>hook</code> will be run regardless of whether an exception is raised while calling the Module.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-39" tabindex="-1">Returns <a class="header-anchor" href="#returns-39" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source-1" tabindex="-1"><code>register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-forward-pre-hook-self-hook-union-callable-t-tuple-any-optional-any-callable-t-tuple-any-dict-str-any-optional-tuple-any-dict-str-any-prepend-bool-false-with-kwargs-bool-false-tensorplay-utils-hooks-removablehandle-source-1" aria-label="Permalink to &quot;`register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1526)&quot;">​</a></h4><p>Register a forward pre-hook on the module.</p><p>The hook will be called every time before <code>forward</code> is invoked.</p><p>If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won&#39;t be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, args, kwargs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> of modified </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">input</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> and</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> kwargs</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="args-43" tabindex="-1">Args <a class="header-anchor" href="#args-43" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>forward_pre</code> hooks registered with <code>register_module_forward_pre_hook</code> will fire before all hooks registered by this method.</li><li><strong>Default</strong>: <code>False</code></li><li><strong>with_kwargs</strong> (<code>bool</code>): If true, the <code>hook</code> will be passed the kwargs given to the forward function.</li><li><strong>Default</strong>: <code>False</code></li></ul><h4 id="returns-40" tabindex="-1">Returns <a class="header-anchor" href="#returns-40" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-1" tabindex="-1"><code>register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-1" aria-label="Permalink to &quot;`register_full_backward_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase], Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1362)&quot;">​</a></h4><p>Register a backward hook on the module.</p><p>The hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:</p><pre><code>1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.
2. If none of the module inputs require gradients, the hook will fire when the gradients are computed
   with respect to module outputs.
3. If none of the module outputs require gradients, then the hooks will not fire.
</code></pre><p>The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_input, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> tuple</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Tensor) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_input</code> and <code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of <code>grad_input</code> in subsequent computations. <code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in <code>grad_input</code> and <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> outputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-44" tabindex="-1">Args <a class="header-anchor" href="#args-44" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward</code> hooks registered with <code>register_module_full_backward_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-41" tabindex="-1">Returns <a class="header-anchor" href="#returns-41" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-1" tabindex="-1"><code>register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-full-backward-pre-hook-self-hook-callable-forwardref-module-union-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-union-nonetype-tuple-tensorplay-c-tensorbase-tensorplay-c-tensorbase-prepend-bool-false-tensorplay-utils-hooks-removablehandle-source-1" aria-label="Permalink to &quot;`register_full_backward_pre_hook(self, hook: Callable[[ForwardRef(&#39;Module&#39;), Union[tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], Union[NoneType, tuple[tensorplay._C.TensorBase, ...], tensorplay._C.TensorBase]], prepend: bool = False) -&gt; tensorplay.utils.hooks.RemovableHandle` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1287)&quot;">​</a></h4><p>Register a backward pre-hook on the module.</p><p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, grad_output) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tuple[Tensor] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">or</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of <code>grad_output</code> in subsequent computations. Entries in <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p><p>For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module&#39;s forward function.</p><p>.. warning</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Modifying inputs inplace </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> allowed when using backward hooks </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">and</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">will </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">raise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> an error.</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="args-45" tabindex="-1">Args <a class="header-anchor" href="#args-45" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): The user-defined hook to be registered.</li><li><strong>prepend</strong> (<code>bool</code>): If true, the provided <code>hook</code> will be fired before all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks on this <code>tensorplay.nn.Module</code>. Note that global <code>backward_pre</code> hooks registered with <code>register_module_full_backward_pre_hook</code> will fire before all hooks registered by this method.</li></ul><h4 id="returns-42" tabindex="-1">Returns <a class="header-anchor" href="#returns-42" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-post-hook-self-hook-source-1" tabindex="-1"><code>register_load_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-post-hook-self-hook-source-1" aria-label="Permalink to &quot;`register_load_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2226)&quot;">​</a></h4><p>Register a post-hook to be run after module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, incompatible_keys) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.</p><p>The given incompatible_keys can be modified inplace if needed.</p><p>Note that the checks performed when calling <code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.</p><h4 id="returns-43" tabindex="-1">Returns <a class="header-anchor" href="#returns-43" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><pre><code>`tensorplay.utils.hooks.RemovableHandle`:
    a handle that can be used to remove the added hook by calling
    ``handle.remove()``
</code></pre><hr><h4 id="register-load-state-dict-pre-hook-self-hook-source-1" tabindex="-1"><code>register_load_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-load-state-dict-pre-hook-self-hook-source-1" aria-label="Permalink to &quot;`register_load_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2214)&quot;">​</a></h4><p>Register a pre-hook to be run before module&#39;s <code>~nn.Module.load_state_dict</code> is called.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # noqa: B950</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="arguments-1" tabindex="-1">Arguments <a class="header-anchor" href="#arguments-1" aria-label="Permalink to &quot;Arguments&quot;">​</a></h4><ul><li><strong>hook</strong> (<code>Callable</code>): Callable hook that will be invoked before loading the state dict.</li></ul><hr><h4 id="register-module-self-name-str-module-optional-forwardref-module-none-source-1" tabindex="-1"><code>register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-module-self-name-str-module-optional-forwardref-module-none-source-1" aria-label="Permalink to &quot;`register_module(self, name: str, module: Optional[ForwardRef(&#39;Module&#39;)]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L695)&quot;">​</a></h4><p>Alias for <code>add_module</code>.</p><hr><h4 id="register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source-1" tabindex="-1"><code>register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-parameter-self-name-str-param-optional-tensorplay-nn-parameter-parameter-none-source-1" aria-label="Permalink to &quot;`register_parameter(self, name: str, param: Optional[tensorplay.nn.parameter.Parameter]) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L617)&quot;">​</a></h4><p>Add a parameter to the module.</p><p>The parameter can be accessed as an attribute using given name.</p><h4 id="args-46" tabindex="-1">Args <a class="header-anchor" href="#args-46" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>name</strong> (<code>str</code>): name of the parameter. The parameter can be accessed from this module using the given name</li><li><strong>param</strong> (<code>Parameter or None</code>): parameter to be added to the module. If <code>None</code>, then operations that run on parameters, such as <code>cuda</code>, are ignored. If <code>None</code>, the parameter is <strong>not</strong> included in the module&#39;s <code>state_dict</code>.</li></ul><hr><h4 id="register-state-dict-post-hook-self-hook-source-1" tabindex="-1"><code>register_state_dict_post_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-post-hook-self-hook-source-1" aria-label="Permalink to &quot;`register_state_dict_post_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2017)&quot;">​</a></h4><p>Register a post-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, state_dict, prefix, local_metadata) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can modify the <code>state_dict</code> inplace.</p><hr><h4 id="register-state-dict-pre-hook-self-hook-source-1" tabindex="-1"><code>register_state_dict_pre_hook(self, hook)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#register-state-dict-pre-hook-self-hook-source-1" aria-label="Permalink to &quot;`register_state_dict_pre_hook(self, hook)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2041)&quot;">​</a></h4><p>Register a pre-hook for the <code>~tensorplay.nn.Module.state_dict</code> method.</p><p>It should have the following signature</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">hook(module, prefix, keep_vars) </span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">-&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p><hr><h4 id="requires-grad-self-requires-grad-bool-true-self-source-1" tabindex="-1"><code>requires_grad_(self, requires_grad: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#requires-grad-self-requires-grad-bool-true-self-source-1" aria-label="Permalink to &quot;`requires_grad_(self, requires_grad: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2826)&quot;">​</a></h4><p>Change if autograd should record operations on parameters in this module.</p><p>This method sets the parameters&#39; <code>requires_grad</code> attributes in-place.</p><p>This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).</p><p>See <code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.</p><h4 id="args-47" tabindex="-1">Args <a class="header-anchor" href="#args-47" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>requires_grad</strong> (<code>bool</code>): whether autograd should record operations on parameters in this module. Default: <code>True</code>.</li></ul><h4 id="returns-44" tabindex="-1">Returns <a class="header-anchor" href="#returns-44" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="set-extra-state-self-state-any-none-source-1" tabindex="-1"><code>set_extra_state(self, state: Any) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-extra-state-self-state-any-none-source-1" aria-label="Permalink to &quot;`set_extra_state(self, state: Any) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L938)&quot;">​</a></h4><p>Set extra state contained in the loaded <code>state_dict</code>.</p><p>This function is called from <code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding <code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.</p><h4 id="args-48" tabindex="-1">Args <a class="header-anchor" href="#args-48" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>state</strong> (<code>dict</code>): Extra state from the <code>state_dict</code></li></ul><hr><h4 id="set-submodule-self-target-str-module-module-strict-bool-false-none-source-1" tabindex="-1"><code>set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#set-submodule-self-target-str-module-module-strict-bool-false-none-source-1" aria-label="Permalink to &quot;`set_submodule(self, target: str, module: &#39;Module&#39;, strict: bool = False) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L764)&quot;">​</a></h4><p>Set the submodule given by <code>target</code> if it exists, otherwise throw an error.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>If <code>strict</code> is set to <code>False</code> (default), the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>strict</code> is set to <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule does not exist.</p></div><p>For example, let&#39;s say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p><p>.. code-block:: text</p><ul><li><strong>A</strong> (<code> (net_b</code>): Module( (net_c): Module( (conv): Conv2d(3, 3, 3) ) (linear): Linear(3, 3) ) )</li></ul><p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p><p>To override the <code>Conv2d</code> with a new submodule <code>Linear</code>, you could call <code>set_submodule(&quot;net_b.net_c.conv&quot;, nn.Linear(1, 1))</code> where <code>strict</code> could be <code>True</code> or <code>False</code></p><p>To add a new submodule <code>Conv2d</code> to the existing <code>net_b</code> module, you would call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1))</code>.</p><p>In the above if you set <code>strict=True</code> and call <code>set_submodule(&quot;net_b.conv&quot;, nn.Conv2d(1, 1, 1), strict=True)</code>, an AttributeError will be raised because <code>net_b</code> does not have a submodule named <code>conv</code>.</p><h4 id="args-49" tabindex="-1">Args <a class="header-anchor" href="#args-49" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>target</strong>: The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</li><li><strong>module</strong>: The module to set the submodule to.</li><li><strong>strict</strong>: If <code>False</code>, the method will replace an existing submodule or create a new submodule if the parent module exists. If <code>True</code>, the method will only attempt to replace an existing submodule and throw an error if the submodule doesn&#39;t already exist.</li></ul><h4 id="raises-7" tabindex="-1">Raises <a class="header-anchor" href="#raises-7" aria-label="Permalink to &quot;Raises&quot;">​</a></h4><ul><li><strong>ValueError</strong>: If the <code>target</code> string is empty or if <code>module</code> is not an instance of <code>nn.Module</code>.</li><li><strong>AttributeError</strong>: If at any point along the path resulting from the <code>target</code> string the (sub)path resolves to a non-existent attribute name or an object that is not an instance of <code>nn.Module</code>.</li></ul><hr><h4 id="share-memory-self-self-source-1" tabindex="-1"><code>share_memory(self) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#share-memory-self-self-source-1" aria-label="Permalink to &quot;`share_memory(self) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2877)&quot;">​</a></h4><p>See <code>tensorplay.Tensor.share_memory_</code>.</p><hr><h4 id="state-dict-self-args-destination-none-prefix-keep-vars-false-source-1" tabindex="-1"><code>state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#state-dict-self-args-destination-none-prefix-keep-vars-false-source-1" aria-label="Permalink to &quot;`state_dict(self, *args, destination=None, prefix=&#39;&#39;, keep_vars=False)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2105)&quot;">​</a></h4><p>Return a dictionary containing references to the whole state of the module.</p><p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>The returned object is a shallow copy. It contains references to the module&#39;s parameters and buffers.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Currently <code>state_dict()</code> also accepts positional arguments for <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However, this is being deprecated and keyword arguments will be enforced in future releases.</p></div><div class="danger custom-block"><p class="custom-block-title">DANGER</p><p>Please avoid the use of argument <code>destination</code> as it is not designed for end-users.</p></div><h4 id="args-50" tabindex="-1">Args <a class="header-anchor" href="#args-50" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>destination</strong> (<code>dict, optional</code>): If provided, the state of module will be updated into the dict and the same object is returned. Otherwise, an <code>OrderedDict</code> will be created and returned.</li><li><strong>Default</strong>: <code>None</code>.</li><li><strong>prefix</strong> (<code>str, optional</code>): a prefix added to parameter and buffer names to compose the keys in state_dict. Default: <code>&#39;&#39;</code>.</li><li><strong>keep_vars</strong> (<code>bool, optional</code>): by default the <code>~tensorplay.Tensor</code> s returned in the state dict are detached from autograd. If it&#39;s set to <code>True</code>, detaching will not be performed.</li><li><strong>Default</strong>: <code>False</code>.</li></ul><h4 id="returns-45" tabindex="-1">Returns <a class="header-anchor" href="#returns-45" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>dict</strong>: a dictionary containing a whole state of the module</li></ul><h4 id="example-19" tabindex="-1">Example <a class="header-anchor" href="#example-19" aria-label="Permalink to &quot;Example&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">module.state_dict().keys()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;bias&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;weight&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><hr><h4 id="to-self-args-kwargs-source-1" tabindex="-1"><code>to(self, *args, **kwargs)</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-self-args-kwargs-source-1" aria-label="Permalink to &quot;`to(self, *args, **kwargs)` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1151)&quot;">​</a></h4><p>Move and/or cast the parameters and buffers.</p><p>This can be called as</p><p>.. function:: to(device=None, dtype=None, non_blocking=False) :noindex:</p><p>.. function:: to(dtype, non_blocking=False) :noindex:</p><p>.. function:: to(tensor, non_blocking=False) :noindex:</p><p>.. function:: to(memory_format=tensorplay.channels_last) :noindex:</p><p>Its signature is similar to <code>tensorplay.Tensor.to</code>, but only accepts floating point or complex <code>dtype</code>\ s. In addition, this method will only cast the floating point or complex parameters and buffers to <code>dtype</code> (if given). The integral parameters and buffers will be moved <code>device</code>, if that is given, but with dtypes unchanged. When <code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.</p><p>See below for examples.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-51" tabindex="-1">Args <a class="header-anchor" href="#args-51" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): the desired device of the parameters and buffers in this module</li><li><strong>dtype</strong> (<code>tensorplay.dtype</code>): the desired floating point or complex dtype of the parameters and buffers in this module</li><li><strong>tensor</strong> (<code>tensorplay.Tensor</code>): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module</li><li><strong>memory_format</strong> (<code>tensorplay.memory_format</code>): the desired memory format for 4D parameters and buffers in this module (keyword only argument)</li></ul><h4 id="returns-46" tabindex="-1">Returns <a class="header-anchor" href="#returns-46" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><h4 id="examples-3" tabindex="-1">Examples <a class="header-anchor" href="#examples-3" aria-label="Permalink to &quot;Examples&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +IGNORE_WANT(&quot;non-deterministic&quot;)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(tensorplay.double)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1913</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5113</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2325</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float64)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># xdoctest: +REQUIRES(env:TENSORPLAY_DOCTEST_CUDA1)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">gpu1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cuda:1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(gpu1, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.half, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">non_blocking</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;cuda:1&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cpu </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> tensorplay.device(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;cpu&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.to(cpu)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Linear(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">in_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">out_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1914</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3420</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5112</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2324</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.float16)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).to(tensorplay.cdouble)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear.weight</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Parameter containing:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3741</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,  </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2382</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [ </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5593</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.4443</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">linear(tensorplay.ones(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.cdouble))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6122</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1150</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">j</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensorplay.complex128)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br></div></div><hr><h4 id="to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source-1" tabindex="-1"><code>to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#to-empty-self-device-union-str-tensorplay-device-int-nonetype-recurse-bool-true-self-source-1" aria-label="Permalink to &quot;`to_empty(self, *, device: Union[str, tensorplay.Device, int, NoneType], recurse: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1119)&quot;">​</a></h4><p>Move the parameters and buffers to the specified device without copying storage.</p><h4 id="args-52" tabindex="-1">Args <a class="header-anchor" href="#args-52" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>device</strong> (<code>tensorplay.device</code>): The desired device of the parameters and buffers in this module.</li><li><strong>recurse</strong> (<code>bool</code>): Whether parameters and buffers of submodules should be recursively moved to the specified device.</li></ul><h4 id="returns-47" tabindex="-1">Returns <a class="header-anchor" href="#returns-47" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="train-self-mode-bool-true-self-source-1" tabindex="-1"><code>train(self, mode: bool = True) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#train-self-mode-bool-true-self-source-1" aria-label="Permalink to &quot;`train(self, mode: bool = True) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2786)&quot;">​</a></h4><p>Set the module in training mode.</p><p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e., whether they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, etc.</p><h4 id="args-53" tabindex="-1">Args <a class="header-anchor" href="#args-53" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>mode</strong> (<code>bool</code>): whether to set training mode (<code>True</code>) or evaluation mode (<code>False</code>). Default: <code>True</code>.</li></ul><h4 id="returns-48" tabindex="-1">Returns <a class="header-anchor" href="#returns-48" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="type-self-dst-type-union-tensorplay-dtype-str-self-source-1" tabindex="-1"><code>type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#type-self-dst-type-union-tensorplay-dtype-str-self-source-1" aria-label="Permalink to &quot;`type(self, dst_type: Union[tensorplay.DType, str]) -&gt; Self` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L1061)&quot;">​</a></h4><p>Casts all parameters and buffers to <code>dst_type</code>.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>This method modifies the module in-place.</p></div><h4 id="args-54" tabindex="-1">Args <a class="header-anchor" href="#args-54" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>dst_type</strong> (<code>type or string</code>): the desired type</li></ul><h4 id="returns-49" tabindex="-1">Returns <a class="header-anchor" href="#returns-49" aria-label="Permalink to &quot;Returns&quot;">​</a></h4><ul><li><strong>Module</strong>: self</li></ul><hr><h4 id="zero-grad-self-set-to-none-bool-true-none-source-1" tabindex="-1"><code>zero_grad(self, set_to_none: bool = True) -&gt; None</code> <a href="https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849" target="_blank" rel="noreferrer">[source]</a> <a class="header-anchor" href="#zero-grad-self-set-to-none-bool-true-none-source-1" aria-label="Permalink to &quot;`zero_grad(self, set_to_none: bool = True) -&gt; None` [[source]](https://github.com/bluemoon-o2/tensorplay/blob/main/tensorplay/nn/modules/module.py#L2849)&quot;">​</a></h4><p>Reset gradients of all model parameters.</p><p>See similar function under <code>tensorplay.optim.Optimizer</code> for more context.</p><h4 id="args-55" tabindex="-1">Args <a class="header-anchor" href="#args-55" aria-label="Permalink to &quot;Args&quot;">​</a></h4><ul><li><strong>set_to_none</strong> (<code>bool</code>): instead of setting to zero, set the grads to None. See <code>tensorplay.optim.Optimizer.zero_grad</code> for details.</li></ul><hr></details></div></div></main><footer class="VPDocFooter" data-v-39a288b8 data-v-e257564d><!--[--><!--]--><!----><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-e257564d><span class="visually-hidden" id="doc-footer-aria-label" data-v-e257564d>Pager</span><div class="pager" data-v-e257564d><!----></div><div class="pager" data-v-e257564d><a class="VPLink link pager-link next" href="/api/" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>Next page</span><span class="title" data-v-e257564d>Overview</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-5d98c3a5 data-v-e315a0ad><div class="container" data-v-e315a0ad><p class="message" data-v-e315a0ad>Released under the Apache 2.0 License.</p><p class="copyright" data-v-e315a0ad>Copyright © 2025 zlx. All rights reserved.</p></div></footer><!--[--><a href="https://deepwiki.com/bluemoon-o2/TensorPlay" target="_blank" rel="noopener noreferrer" class="deepwiki-floating-badge" title="View on DeepWiki" data-v-5bf2a798><div class="badge-content" data-v-5bf2a798><span class="badge-icon" data-v-5bf2a798>📚</span><span class="badge-text" data-v-5bf2a798>DeepWiki</span></div></a><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"about_index.md\":\"D9dXnE2q\",\"api__c.md\":\"Qv-Y0wjX\",\"api__reduction.md\":\"rW4mwTRL\",\"api_activation.md\":\"DBJWtw-_\",\"api_adagrad.md\":\"7fEiNcol\",\"api_adam.md\":\"DgIdmxbk\",\"api_adamw.md\":\"S2cbrQb2\",\"api_alexnet.md\":\"xfyhbAAv\",\"api_amp.md\":\"C1Rj-pkI\",\"api_audio.md\":\"CAYtsDhl\",\"api_autocast_mode.md\":\"Hp418vAU\",\"api_autograd.md\":\"C37rD2mB\",\"api_backend.md\":\"CKBPOse6\",\"api_backends.md\":\"CBEPI80Z\",\"api_batchnorm.md\":\"CsfUu6Z4\",\"api_collate.md\":\"VS3YRrxy\",\"api_common_types.md\":\"B_86CbjF\",\"api_comparison.md\":\"rUN-CnTf\",\"api_container.md\":\"Cbof7ev1\",\"api_conv.md\":\"CIzymlZe\",\"api_cpp.md\":\"-u6g5kTd\",\"api_cpu.md\":\"DsGhUd54\",\"api_cuda.md\":\"DBdOhQWo\",\"api_data.md\":\"BOUwdm3B\",\"api_dataloader.md\":\"D2ehS173\",\"api_dataset.md\":\"BiSF7wC4\",\"api_datasets.md\":\"DRjHfmsZ\",\"api_dropout.md\":\"D2jx3blg\",\"api_flatten.md\":\"kF2VdI-B\",\"api_folder.md\":\"DfdTacwP\",\"api_function.md\":\"5rAE_oC6\",\"api_functional.md\":\"BzPu4r9-\",\"api_grad_mode.md\":\"DezgUw3X\",\"api_grad_scaler.md\":\"6-RhoFRG\",\"api_hooks.md\":\"hmrR_JFD\",\"api_hub.md\":\"B2G66UMX\",\"api_index.md\":\"bVIfhRqa\",\"api_init.md\":\"Cj8numYV\",\"api_instancenorm.md\":\"g1Nob3MB\",\"api_io.md\":\"CGoKD34V\",\"api_lazy.md\":\"CtdTYN7T\",\"api_linear.md\":\"BFksnXVV\",\"api_loss.md\":\"D6D496Nc\",\"api_lr_scheduler.md\":\"DMd8S7P9\",\"api_mkl.md\":\"KhDXHcio\",\"api_mkldnn.md\":\"OSo2TWhL\",\"api_mnist.md\":\"H73sp2Vx\",\"api_models.md\":\"CZ5yo_yb\",\"api_module.md\":\"wX6_Ikao\",\"api_modules.md\":\"49CYcx2L\",\"api_multiprocessing.md\":\"Bd55nk9B\",\"api_nn.md\":\"usSUfArh\",\"api_normalization.md\":\"B7JudeAG\",\"api_onnx.md\":\"bPexZ2kf\",\"api_openmp.md\":\"0DZ1wZuu\",\"api_ops.md\":\"Cx9M0vLQ\",\"api_optim.md\":\"BOjgiHyt\",\"api_optimizer.md\":\"DR7JhJE4\",\"api_parameter.md\":\"DhBlWvMo\",\"api_pooling.md\":\"DQs7B4a8\",\"api_primitives.md\":\"BR4vfoSM\",\"api_rmsprop.md\":\"CIbJMV6n\",\"api_sampler.md\":\"i75Z7DWN\",\"api_serialization.md\":\"B6wu7cB-\",\"api_sgd.md\":\"BV8ROtS0\",\"api_sparse.md\":\"0uz7O3D9\",\"api_stax.md\":\"C3cBQ6GW\",\"api_tensorplay.md\":\"CQCL757z\",\"api_transforms.md\":\"BNml18Hv\",\"api_triton.md\":\"kCIU5EE5\",\"api_types.md\":\"DEthfquR\",\"api_utils.md\":\"C52JWJfN\",\"api_vision.md\":\"Dsfwg8F4\",\"api_viz.md\":\"CoC58de7\",\"api_worker.md\":\"CJ7Xru23\",\"blog_index.md\":\"DWx6QLbZ\",\"blog_posts_deep-dive-p10-dispatcher.md\":\"JfuylyH0\",\"blog_posts_tensorplay-architecture-design.md\":\"DZ7AhEGV\",\"blog_posts_tpx-autograd-decoupling.md\":\"y63udokG\",\"changelog.md\":\"BjgyldP3\",\"community_index.md\":\"CDLlGWNR\",\"contributing.md\":\"_wut4jFO\",\"ecosystem_index.md\":\"CsS7Iq3X\",\"guide_architecture.md\":\"DN9lvA1S\",\"guide_getting-started.md\":\"DUjZDmki\",\"guide_install.md\":\"BP5Ti7wA\",\"guide_quickstart.md\":\"3nrEFmq3\",\"guide_resources.md\":\"BOifstC4\",\"guide_tutorials.md\":\"C3HGReL3\",\"guide_tutorials_cnn-classification.md\":\"3MlqEGfJ\",\"guide_tutorials_custom-autograd.md\":\"BA2l9XEV\",\"guide_tutorials_linear-regression.md\":\"CAjgMIxs\",\"guide_what-is-tensorplay.md\":\"Bus4zqCU\",\"index.md\":\"BJoikFZB\",\"join_index.md\":\"DyN1heDn\",\"privacy_index.md\":\"Dc18AF-S\",\"subscribe_index.md\":\"C1N0ypk3\",\"zh_about_index.md\":\"CfkbfFQO\",\"zh_api__c.md\":\"QeicHXDe\",\"zh_api__reduction.md\":\"DKP80wgV\",\"zh_api_activation.md\":\"CUSYvzmq\",\"zh_api_adagrad.md\":\"DsSFdYof\",\"zh_api_adam.md\":\"aYFvwb_r\",\"zh_api_adamw.md\":\"CuuMUrqp\",\"zh_api_alexnet.md\":\"lUwYgt4s\",\"zh_api_amp.md\":\"Bx-qbrS1\",\"zh_api_audio.md\":\"DZ7q1f0A\",\"zh_api_autocast_mode.md\":\"uTU9KiPt\",\"zh_api_autograd.md\":\"CPhHsUFS\",\"zh_api_backend.md\":\"fRwsnBsB\",\"zh_api_backends.md\":\"BdFrLHXF\",\"zh_api_batchnorm.md\":\"CxUNeLTO\",\"zh_api_collate.md\":\"Cws0sRGA\",\"zh_api_common_types.md\":\"BEuCGHON\",\"zh_api_comparison.md\":\"-2Uetodq\",\"zh_api_container.md\":\"D37_QN0R\",\"zh_api_conv.md\":\"DJ8R3UbX\",\"zh_api_cpp.md\":\"CBkBC3zp\",\"zh_api_cuda.md\":\"DLkcv7Ir\",\"zh_api_data.md\":\"Bm-dEaQ8\",\"zh_api_dataloader.md\":\"lDF5DeKn\",\"zh_api_dataset.md\":\"BY_BnnRD\",\"zh_api_datasets.md\":\"CGpwicJU\",\"zh_api_dropout.md\":\"BMfQ_6Jp\",\"zh_api_flatten.md\":\"Cz8OGJr_\",\"zh_api_folder.md\":\"Dkku3kqH\",\"zh_api_function.md\":\"Hl6Wc3if\",\"zh_api_functional.md\":\"BBH6gxp5\",\"zh_api_grad_mode.md\":\"B-aIV7qE\",\"zh_api_grad_scaler.md\":\"KHiz-nuo\",\"zh_api_hooks.md\":\"BcBD7LCX\",\"zh_api_hub.md\":\"BarfC3-I\",\"zh_api_index.md\":\"BkC_r94Y\",\"zh_api_init.md\":\"idSMB-ST\",\"zh_api_instancenorm.md\":\"BYOE56AO\",\"zh_api_io.md\":\"DqQyHES8\",\"zh_api_lazy.md\":\"CTqJ9B6Q\",\"zh_api_linear.md\":\"Bw2KJByh\",\"zh_api_loss.md\":\"CD_2mlj8\",\"zh_api_lr_scheduler.md\":\"CJQqZqOW\",\"zh_api_mkl.md\":\"DNl7FjP1\",\"zh_api_mkldnn.md\":\"C1sB8LAM\",\"zh_api_mnist.md\":\"CiAk-CTL\",\"zh_api_models.md\":\"DA2vw96r\",\"zh_api_module.md\":\"C-JmapjX\",\"zh_api_modules.md\":\"DdXcls2D\",\"zh_api_multiprocessing.md\":\"BIFNJIFd\",\"zh_api_nn.md\":\"CLe0rS6O\",\"zh_api_normalization.md\":\"D4kXog4C\",\"zh_api_onnx.md\":\"C3CyIUd2\",\"zh_api_openmp.md\":\"BrkaPwMO\",\"zh_api_ops.md\":\"DLDjaRJF\",\"zh_api_optim.md\":\"H0ppJfCx\",\"zh_api_optimizer.md\":\"B23Nsf3p\",\"zh_api_parameter.md\":\"CI9-b1SL\",\"zh_api_pooling.md\":\"D-QWUN-u\",\"zh_api_primitives.md\":\"CQMCnO1g\",\"zh_api_rmsprop.md\":\"DcIfxc-y\",\"zh_api_sampler.md\":\"1BHbZ0ce\",\"zh_api_serialization.md\":\"BK3y5eRp\",\"zh_api_sgd.md\":\"D4bm7-9m\",\"zh_api_sparse.md\":\"Fk1PMEC7\",\"zh_api_stax.md\":\"rEV1TX_h\",\"zh_api_tensorplay.md\":\"Bcr-Gbkh\",\"zh_api_transforms.md\":\"B9PVNQ_u\",\"zh_api_types.md\":\"B19GW1aQ\",\"zh_api_utils.md\":\"C4y9IfsU\",\"zh_api_vision.md\":\"DQiyNVBw\",\"zh_api_viz.md\":\"DsbB1XZt\",\"zh_api_worker.md\":\"D391YJXa\",\"zh_blog_index.md\":\"BYN71ob1\",\"zh_blog_posts_deep-dive-p10-dispatcher.md\":\"Y3GqFZb4\",\"zh_blog_posts_tensorplay-architecture-design.md\":\"QEfEv62m\",\"zh_blog_posts_tpx-autograd-decoupling.md\":\"DBPxqNCw\",\"zh_changelog.md\":\"CIInNQ38\",\"zh_community_index.md\":\"D-eqgfTx\",\"zh_contributing.md\":\"BIqwhrVB\",\"zh_ecosystem_index.md\":\"BZxgrPi-\",\"zh_guide_architecture.md\":\"B71RCcWJ\",\"zh_guide_getting-started.md\":\"C5phlr4x\",\"zh_guide_install.md\":\"B8k6r5zl\",\"zh_guide_quickstart.md\":\"DW3vVox8\",\"zh_guide_resources.md\":\"Ckj5cZ7z\",\"zh_guide_tutorials.md\":\"DWXrgGXv\",\"zh_guide_tutorials_cnn-classification.md\":\"DiJZgTxg\",\"zh_guide_tutorials_custom-autograd.md\":\"CV4tgvS7\",\"zh_guide_tutorials_linear-regression.md\":\"BzPRWTLi\",\"zh_guide_what-is-tensorplay.md\":\"siipjvez\",\"zh_index.md\":\"BSTTcJkG\",\"zh_join_index.md\":\"D-DdnxPn\",\"zh_privacy_index.md\":\"CM-jjhAY\",\"zh_subscribe_index.md\":\"B-hqBebQ\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"TensorPlay\",\"description\":\"A transparent, educational, and PyTorch-compatible deep learning framework.\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"logo\":\"/images/logo-0.png\",\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/bluemoon-o2/tensorplay\"}],\"search\":{\"provider\":\"local\",\"options\":{\"locales\":{\"zh\":{\"translations\":{\"button\":{\"buttonText\":\"搜索文档\",\"buttonAriaLabel\":\"搜索文档\"},\"modal\":{\"noResultsText\":\"无法找到相关结果\",\"resetButtonTitle\":\"清除查询条件\",\"footer\":{\"selectText\":\"选择\",\"navigateText\":\"切换\",\"closeText\":\"关闭\"}}}}}}}},\"locales\":{\"root\":{\"label\":\"English\",\"lang\":\"en\",\"description\":\"A simple deep learning framework designed for educational purposes and small-scale experiments.\",\"themeConfig\":{\"nav\":[{\"text\":\"Learn\",\"items\":[{\"text\":\"Getting Started\",\"link\":\"/guide/getting-started\"},{\"text\":\"Tutorials\",\"link\":\"/guide/tutorials\"}]},{\"text\":\"Community\",\"link\":\"/community/\"},{\"text\":\"Projects\",\"link\":\"/ecosystem/\"},{\"text\":\"Docs\",\"link\":\"/api/\"},{\"text\":\"Blog & News\",\"link\":\"/blog/\"},{\"text\":\"About\",\"link\":\"/about/\"},{\"text\":\"JOIN\",\"link\":\"/join/\"}],\"sidebar\":{\"/guide/\":[{\"text\":\"Introduction\",\"items\":[{\"text\":\"What is TensorPlay?\",\"link\":\"/guide/what-is-tensorplay\"},{\"text\":\"Getting Started\",\"link\":\"/guide/getting-started\"},{\"text\":\"Installation\",\"link\":\"/guide/install\"},{\"text\":\"Quickstart\",\"link\":\"/guide/quickstart\"},{\"text\":\"Architecture\",\"link\":\"/guide/architecture\"}]},{\"text\":\"Guides & Resources\",\"items\":[{\"text\":\"Tutorials\",\"link\":\"/guide/tutorials\",\"items\":[{\"text\":\"Image Classification\",\"link\":\"/guide/tutorials/cnn-classification\"},{\"text\":\"Linear Regression\",\"link\":\"/guide/tutorials/linear-regression\"},{\"text\":\"Custom Autograd\",\"link\":\"/guide/tutorials/custom-autograd\"}]},{\"text\":\"Resources\",\"link\":\"/guide/resources\"},{\"text\":\"API Reference\",\"link\":\"/api/\"}]}],\"/api/\":[{\"text\":\"Core API\",\"items\":[{\"text\":\"Overview\",\"link\":\"/api/\"},{\"text\":\"tensorplay\",\"link\":\"/api/tensorplay\"},{\"text\":\"autograd\",\"link\":\"/api/autograd\"},{\"text\":\"functional\",\"link\":\"/api/functional\"},{\"text\":\"optim\",\"link\":\"/api/optim\"},{\"text\":\"cuda\",\"link\":\"/api/cuda\"}]},{\"text\":\"Neural Networks (nn)\",\"collapsed\":false,\"items\":[{\"text\":\"nn\",\"link\":\"/api/nn\"},{\"text\":\"Modules\",\"link\":\"/api/modules\"},{\"text\":\"Linear Layers\",\"link\":\"/api/linear\"},{\"text\":\"Convolution Layers\",\"link\":\"/api/conv\"},{\"text\":\"Pooling Layers\",\"link\":\"/api/pooling\"},{\"text\":\"Activation Functions\",\"link\":\"/api/activation\"},{\"text\":\"Normalization Layers\",\"link\":\"/api/normalization\"},{\"text\":\"Loss Functions\",\"link\":\"/api/loss\"},{\"text\":\"Dropout Layers\",\"link\":\"/api/dropout\"},{\"text\":\"Container\",\"link\":\"/api/container\"}]}]},\"footer\":{\"message\":\"Released under the Apache 2.0 License.\",\"copyright\":\"Copyright © 2025 zlx. All rights reserved.\"}}},\"zh\":{\"label\":\"简体中文\",\"lang\":\"zh\",\"link\":\"/zh/\",\"description\":\"一个适合学习者、兼容 PyTorch 的深度学习框架。\",\"themeConfig\":{\"nav\":[{\"text\":\"学习\",\"items\":[{\"text\":\"快速开始\",\"link\":\"/zh/guide/getting-started\"},{\"text\":\"教程\",\"link\":\"/zh/guide/tutorials\"}]},{\"text\":\"社区\",\"link\":\"/zh/community/\"},{\"text\":\"项目\",\"link\":\"/zh/ecosystem/\"},{\"text\":\"文档\",\"link\":\"/zh/api/\"},{\"text\":\"博客与新闻\",\"link\":\"/zh/blog/\"},{\"text\":\"关于\",\"link\":\"/zh/about/\"},{\"text\":\"加入我们\",\"link\":\"/zh/join/\"}],\"sidebar\":{\"/zh/guide/\":[{\"text\":\"介绍\",\"items\":[{\"text\":\"什么是 TensorPlay?\",\"link\":\"/zh/guide/what-is-tensorplay\"},{\"text\":\"快速开始\",\"link\":\"/zh/guide/getting-started\"},{\"text\":\"安装\",\"link\":\"/zh/guide/install\"},{\"text\":\"快速入门\",\"link\":\"/zh/guide/quickstart\"},{\"text\":\"架构设计\",\"link\":\"/zh/guide/architecture\"}]},{\"text\":\"指南与资源\",\"items\":[{\"text\":\"教程\",\"link\":\"/zh/guide/tutorials\",\"items\":[{\"text\":\"图像分类\",\"link\":\"/zh/guide/tutorials/cnn-classification\"},{\"text\":\"线性回归\",\"link\":\"/zh/guide/tutorials/linear-regression\"},{\"text\":\"自定义自动微分\",\"link\":\"/zh/guide/tutorials/custom-autograd\"}]},{\"text\":\"资源\",\"link\":\"/zh/guide/resources\"},{\"text\":\"API 参考\",\"link\":\"/zh/api/\"}]}],\"/zh/api/\":[{\"text\":\"核心 API\",\"items\":[{\"text\":\"概览\",\"link\":\"/zh/api/\"},{\"text\":\"tensorplay\",\"link\":\"/zh/api/tensorplay\"},{\"text\":\"autograd\",\"link\":\"/zh/api/autograd\"},{\"text\":\"functional\",\"link\":\"/zh/api/functional\"},{\"text\":\"optim\",\"link\":\"/zh/api/optim\"},{\"text\":\"cuda\",\"link\":\"/zh/api/cuda\"}]},{\"text\":\"神经网络 (nn)\",\"collapsed\":false,\"items\":[{\"text\":\"nn\",\"link\":\"/zh/api/nn\"},{\"text\":\"Modules\",\"link\":\"/zh/api/modules\"},{\"text\":\"Linear Layers\",\"link\":\"/zh/api/linear\"},{\"text\":\"Convolution Layers\",\"link\":\"/zh/api/conv\"},{\"text\":\"Pooling Layers\",\"link\":\"/zh/api/pooling\"},{\"text\":\"Activation Functions\",\"link\":\"/zh/api/activation\"},{\"text\":\"Normalization Layers\",\"link\":\"/zh/api/normalization\"},{\"text\":\"Loss Functions\",\"link\":\"/zh/api/loss\"},{\"text\":\"Dropout Layers\",\"link\":\"/zh/api/dropout\"},{\"text\":\"Container\",\"link\":\"/zh/api/container\"}]}]},\"footer\":{\"message\":\"基于 Apache 2.0 许可发布。\",\"copyright\":\"版权所有 © 2025 zlx。保留所有权利。\"}}}},\"scrollOffset\":134,\"cleanUrls\":true}");</script>
    
  </body>
</html>